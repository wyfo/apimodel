{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#apischema","title":"apischema","text":"<p>JSON (de)serialization, GraphQL and JSON schema generation using Python typing.</p> <p>apischema makes your life easier when dealing with API data.</p>"},{"location":"#install","title":"Install","text":"<p><pre><code>pip install apischema\n</code></pre> It requires only Python 3.8+. PyPy3 is also fully supported.</p>"},{"location":"#why-another-library","title":"Why another library?","text":"<p>This library fulfills the following goals:</p> <ul> <li>stay as close as possible to the standard library (dataclasses, typing, etc.) \u2014 as a consequence we do not need plugins for editors/linters/etc.;</li> <li>avoid object-oriented limitations \u2014 do not require a base class \u2014 thus handle easily every type (<code>Foo</code>, <code>list[Bar]</code>, <code>NewType(Id, int)</code>, etc.) the same way.</li> <li>be adaptable, provide tools to support any types (ORM, etc.);</li> <li>avoid dynamic things like using raw strings for attributes name - play nicely with your IDE.</li> </ul> <p>No known alternative achieves all of this, and apischema is also (a lot) faster than all of them.</p> <p>On top of that, because APIs are not only JSON, apischema is also a complete GraphQL library</p> <p>Note</p> <p>Actually, apischema is even adaptable enough to enable support of competitor libraries in a few dozens of line of code (pydantic support example using conversions feature)</p>"},{"location":"#example","title":"Example","text":"<p><pre><code>from collections.abc import Collection\nfrom dataclasses import dataclass, field\nfrom uuid import UUID, uuid4\n\nimport pytest\nfrom graphql import print_schema\n\nfrom apischema import ValidationError, deserialize, serialize\nfrom apischema.graphql import graphql_schema\nfrom apischema.json_schema import deserialization_schema\n\n\n# Define a schema with standard dataclasses\n@dataclass\nclass Resource:\n    id: UUID\n    name: str\n    tags: set[str] = field(default_factory=set)\n\n\n# Get some data\nuuid = uuid4()\ndata = {\"id\": str(uuid), \"name\": \"wyfo\", \"tags\": [\"some_tag\"]}\n# Deserialize data\nresource = deserialize(Resource, data)\nassert resource == Resource(uuid, \"wyfo\", {\"some_tag\"})\n# Serialize objects\nassert serialize(Resource, resource) == data\n# Validate during deserialization\nwith pytest.raises(ValidationError) as err:  # pytest checks exception is raised\n    deserialize(Resource, {\"id\": \"42\", \"name\": \"wyfo\"})\nassert err.value.errors == [\n    {\"loc\": [\"id\"], \"err\": \"badly formed hexadecimal UUID string\"}\n]\n# Generate JSON Schema\nassert deserialization_schema(Resource) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"id\": {\"type\": \"string\", \"format\": \"uuid\"},\n        \"name\": {\"type\": \"string\"},\n        \"tags\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"},\n            \"uniqueItems\": True,\n            \"default\": [],\n        },\n    },\n    \"required\": [\"id\", \"name\"],\n    \"additionalProperties\": False,\n}\n\n\n# Define GraphQL operations\ndef resources(tags: Collection[str] | None = None) -&gt; Collection[Resource] | None:\n    ...\n\n\n# Generate GraphQL schema\nschema = graphql_schema(query=[resources], id_types={UUID})\nschema_str = \"\"\"\\\ntype Query {\n  resources(tags: [String!]): [Resource!]\n}\n\ntype Resource {\n  id: ID!\n  name: String!\n  tags: [String!]!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre> apischema works out of the box with your data model.</p> <p>Note</p> <p>This example and further ones are using pytest API because they are in fact run as tests in the library CI</p>"},{"location":"#run-the-documentation-examples","title":"Run the documentation examples","text":"<p>All documentation examples are written using the last Python minor version \u2014 currently 3.10 \u2014 in order to provide up-to-date documentation. Because Python 3.10 specificities (like PEP 585) are used, this version is \"mandatory\" to execute the examples as-is.</p> <p>In addition to pytest, some examples use third-party libraries like SQLAlchemy or attrs. All of this dependencies can be downloaded using the <code>examples</code> extra with <pre><code>pip install apischema[examples]\n</code></pre></p> <p>Once dependencies are installed, you can simply copy-paste examples and execute them, using the proper Python version. </p>"},{"location":"#faq","title":"FAQ","text":""},{"location":"#what-is-the-difference-between-apischema-and-pydantic","title":"What is the difference between apischema and pydantic?","text":"<p>See the dedicated section \u2014 there are many differences.</p>"},{"location":"#i-already-have-my-data-model-with-my-sqlalchemyorm-tables-will-i-have-to-duplicate-my-code-making-one-dataclass-per-table","title":"I already have my data model with my SQLAlchemy/ORM tables, will I have to duplicate my code, making one dataclass per table?","text":"<p>No, <code>apischema</code> works with user-defined types as well as types from foreign libraries. Using the conversion feature, you can add default serialization for all your tables, or register a different serializer that you can select according to your API endpoint, or both.</p>"},{"location":"#i-need-more-accurate-validation-than-ensure-this-is-an-integer-and-not-a-string-can-i-do-that","title":"I need more accurate validation than \"ensure this is an integer and not a string \", can I do that?","text":"<p>See the validation section. You can use standard JSON schema validation (<code>maxItems</code>, <code>pattern</code>, etc.) that will be embedded in your schema or add custom Python validators for each class/fields/<code>NewType</code> you want.</p>"},{"location":"conversions/","title":"Conversions \u2013 (de)serialization customization","text":"<p>apischema covers the majority of standard data types, but of course that's not enough, which is why it enables you to add support for all your classes and the libraries you use.</p> <p>Actually, apischema itself uses this conversion feature to provide a basic support for standard library data types like UUID/datetime/etc. (see std_types.py)</p> <p>ORM support can easily be achieved with this feature (see SQLAlchemy example).</p> <p>In fact, you can even add support for competitor libraries like Pydantic (see Pydantic compatibility example)</p>"},{"location":"conversions/#principle-apischema-conversions","title":"Principle - apischema conversions","text":"<p>An apischema conversion is composed of a source type, let's call it <code>Source</code>, a target type <code>Target</code> and a converter function with signature <code>(Source) -&gt; Target</code>.</p> <p>When a class (actually, a non-builtin class, so not <code>int</code>/<code>list</code>/etc.) is deserialized, apischema will check if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then the converter will be applied to get an object of the expected type. Serialization works the same way but inverted: look for a conversion with type as source, apply then converter, and get the target type.</p> <p>Conversions are also handled in schema generation: for a deserialization schema, source schema is merged to target schema, while target schema is merged to source schema for a serialization schema.</p>"},{"location":"conversions/#register-a-conversion","title":"Register a conversion","text":"<p>Conversion is registered using <code>apischema.deserializer</code>/<code>apischema.serializer</code> for deserialization/serialization respectively.</p> <p>When used as function decorator, the <code>Source</code>/<code>Target</code> types are directly extracted from the conversion function signature. </p> <p><code>serializer</code> can be called on methods/properties, in which case <code>Source</code> type is inferred to be the owning type.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import deserialize, schema, serialize\nfrom apischema.conversions import deserializer, serializer\nfrom apischema.json_schema import deserialization_schema, serialization_schema\n\n\n@schema(pattern=r\"^#[0-9a-fA-F]{6}$\")\n@dataclass\nclass RGB:\n    red: int\n    green: int\n    blue: int\n\n    @serializer\n    @property\n    def hexa(self) -&gt; str:\n        return f\"#{self.red:02x}{self.green:02x}{self.blue:02x}\"\n\n\n# serializer can also be called with methods/properties outside of the class\n# For example, `serializer(RGB.hexa)` would have the same effect as the decorator above\n\n\n@deserializer\ndef from_hexa(hexa: str) -&gt; RGB:\n    return RGB(int(hexa[1:3], 16), int(hexa[3:5], 16), int(hexa[5:7], 16))\n\n\nassert deserialize(RGB, \"#000000\") == RGB(0, 0, 0)\nassert serialize(RGB, RGB(0, 0, 42)) == \"#00002a\"\nassert (\n    deserialization_schema(RGB)\n    == serialization_schema(RGB)\n    == {\n        \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n        \"type\": \"string\",\n        \"pattern\": \"^#[0-9a-fA-F]{6}$\",\n    }\n)\n</code></pre> <p>Warning</p> <p>(De)serializer methods cannot be used with <code>typing.NamedTuple</code>; in fact, apischema uses the <code>__set_name__</code> magic method but it is not called on <code>NamedTuple</code> subclass fields. </p>"},{"location":"conversions/#multiple-deserializers","title":"Multiple deserializers","text":"<p>Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with a <code>Union</code> param, it's not very practical. That's why apischema make it possible to register several deserializers for the same type. They will be handled with a <code>Union</code> source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import deserialize, deserializer\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Expression:\n    value: int\n\n\n@deserializer\ndef evaluate_expression(expr: str) -&gt; Expression:\n    return Expression(int(eval(expr)))\n\n\n# Could be shorten into deserializer(Expression), because class is callable too\n@deserializer\ndef expression_from_value(value: int) -&gt; Expression:\n    return Expression(value)\n\n\nassert deserialization_schema(Expression) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": [\"string\", \"integer\"],\n}\nassert deserialize(Expression, 0) == deserialize(Expression, \"1 - 1\") == Expression(0)\n</code></pre> <p>On the other hand, serializer registration overwrites the previous registration if any. </p> <p><code>apischema.conversions.reset_deserializers</code>/<code>apischema.conversions.reset_serializers</code> can be used to reset (de)serializers (even those of the standard types embedded in apischema)</p>"},{"location":"conversions/#inheritance","title":"Inheritance","text":"<p>All serializers are naturally inherited. In fact, with a conversion function <code>(Source) -&gt; Target</code>, you can always pass a subtype of <code>Source</code> and get a <code>Target</code> in return.</p> <p>Moreover, when serializer is a method/property, overriding this method/property in a subclass will override the inherited serializer.</p> <pre><code>from apischema import serialize, serializer\n\n\nclass Foo:\n    pass\n\n\n@serializer\ndef serialize_foo(foo: Foo) -&gt; int:\n    return 0\n\n\nclass Foo2(Foo):\n    pass\n\n\n# Deserializer is inherited\nassert serialize(Foo, Foo()) == serialize(Foo2, Foo2()) == 0\n\n\nclass Bar:\n    @serializer\n    def serialize(self) -&gt; int:\n        return 0\n\n\nclass Bar2(Bar):\n    def serialize(self) -&gt; int:\n        return 1\n\n\n# Deserializer is inherited and overridden\nassert serialize(Bar, Bar()) == 0 != serialize(Bar2, Bar2()) == 1\n</code></pre> <p>Note</p> <p>Inheritance can also be toggled off in specific cases, like in the Class as union of its subclasses example</p> <p>On the other hand, deserializers cannot be inherited, because the same <code>Source</code> passed to a conversion function <code>(Source) -&gt; Target</code> will always give the same <code>Target</code> (not ensured to be the desired subtype).</p> <p>Note</p> <p>Pseudo-inheritance could be achieved by registering a conversion (using for example a <code>classmethod</code>) for each subclass in <code>__init_subclass__</code> method (or a metaclass), or by using <code>__subclasses__</code>; see example</p>"},{"location":"conversions/#generic-conversions","title":"Generic conversions","text":"<p><code>Generic</code> conversions are supported out of the box.</p> <pre><code>from typing import Generic, TypeVar\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, serialize\nfrom apischema.conversions import deserializer, serializer\nfrom apischema.json_schema import deserialization_schema, serialization_schema\n\nT = TypeVar(\"T\")\n\n\nclass Wrapper(Generic[T]):\n    def __init__(self, wrapped: T):\n        self.wrapped = wrapped\n\n    @serializer\n    def unwrap(self) -&gt; T:\n        return self.wrapped\n\n\n# Wrapper constructor can be used as a function too (so deserializer could work as decorator)\ndeserializer(Wrapper)\n\n\nassert deserialize(Wrapper[list[int]], [0, 1]).wrapped == [0, 1]\nwith pytest.raises(ValidationError):\n    deserialize(Wrapper[int], \"wrapped\")\nassert serialize(Wrapper[str], Wrapper(\"wrapped\")) == \"wrapped\"\nassert (\n    deserialization_schema(Wrapper[int])\n    == {\"$schema\": \"http://json-schema.org/draft/2020-12/schema#\", \"type\": \"integer\"}\n    == serialization_schema(Wrapper[int])\n)\n</code></pre> <p>However, you're not allowed to register a conversion of a specialized generic type, like <code>Foo[int]</code>.</p>"},{"location":"conversions/#conversion-object","title":"Conversion object","text":"<p>In the previous example, conversions were registered using only converter functions. However, it can also be done by passing a <code>apischema.conversions.Conversion</code> instance. It allows specifying additional metadata to conversion (see next sections for examples) and precise converter source/target when annotations are not available.</p> <pre><code>from base64 import b64decode\n\nfrom apischema import deserialize, deserializer\nfrom apischema.conversions import Conversion\n\ndeserializer(Conversion(b64decode, source=str, target=bytes))\n# Roughly equivalent to:\n# def decode_bytes(source: str) -&gt; bytes:\n#     return b64decode(source)\n# but saving a function call\n\nassert deserialize(bytes, \"Zm9v\") == b\"foo\"\n</code></pre>"},{"location":"conversions/#dynamic-conversions-select-conversions-at-runtime","title":"Dynamic conversions \u2014 select conversions at runtime","text":"<p>Whether or not a conversion is registered for a given type, conversions can also be provided at runtime, using the <code>conversion</code> parameter of <code>deserialize</code>/<code>serialize</code>/<code>deserialization_schema</code>/<code>serialization_schema</code>.</p> <pre><code>import os\nimport time\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Annotated\n\nfrom apischema import deserialize, serialize\nfrom apischema.metadata import conversion\n\n# Set UTC timezone for example\nos.environ[\"TZ\"] = \"UTC\"\ntime.tzset()\n\n\ndef datetime_from_timestamp(timestamp: int) -&gt; datetime:\n    return datetime.fromtimestamp(timestamp)\n\n\ndate = datetime(2017, 9, 2)\nassert deserialize(datetime, 1504310400, conversion=datetime_from_timestamp) == date\n\n\n@dataclass\nclass Foo:\n    bar: int\n    baz: int\n\n    def sum(self) -&gt; int:\n        return self.bar + self.baz\n\n    @property\n    def diff(self) -&gt; int:\n        return int(self.bar - self.baz)\n\n\nassert serialize(Foo, Foo(0, 1)) == {\"bar\": 0, \"baz\": 1}\nassert serialize(Foo, Foo(0, 1), conversion=Foo.sum) == 1\nassert serialize(Foo, Foo(0, 1), conversion=Foo.diff) == -1\n# conversions can be specified using Annotated\nassert serialize(Annotated[Foo, conversion(serialization=Foo.sum)], Foo(0, 1)) == 1\n</code></pre> <p>Note</p> <p>For <code>definitions_schema</code>, conversions can be added with types by using a tuple instead, for example <code>definitions_schema(serializations=[(list[Foo], foo_to_bar)])</code>. </p> <p>The <code>conversion</code> parameter can also take a tuple of conversions, when you have a <code>Union</code>, a <code>tuple</code> or when you want to have several deserializations for the same type.</p>"},{"location":"conversions/#dynamic-conversions-are-local","title":"Dynamic conversions are local","text":"<p>Dynamic conversions are discarded after having been applied (or after class without conversion having been encountered). For example, you can't apply directly a dynamic conversion to a dataclass field when calling <code>serialize</code> on an instance of this dataclass. Reasons for this design are detailed in the FAQ. </p> <pre><code>import os\nimport time\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\nfrom apischema import serialize\n\n# Set UTC timezone for example\nos.environ[\"TZ\"] = \"UTC\"\ntime.tzset()\n\n\ndef to_timestamp(d: datetime) -&gt; int:\n    return int(d.timestamp())\n\n\n@dataclass\nclass Foo:\n    bar: datetime\n\n\n# timestamp conversion is not applied on Foo field because it's discarded\n# when encountering Foo\nassert serialize(Foo, Foo(datetime(2019, 10, 13)), conversion=to_timestamp) == {\n    \"bar\": \"2019-10-13T00:00:00\"\n}\n\n# timestamp conversion is applied on every member of list\nassert serialize(list[datetime], [datetime(1970, 1, 1)], conversion=to_timestamp) == [0]\n</code></pre> <p>Note</p> <p>Dynamic conversion is not discarded when the encountered type is a container (<code>list</code>, <code>dict</code>, <code>Collection</code>, etc. or <code>Union</code>) or a registered conversion from/to a container; the dynamic conversion can then apply to the container elements</p>"},{"location":"conversions/#dynamic-conversions-interact-with-type_name","title":"Dynamic conversions interact with <code>type_name</code>","text":"<p>Dynamic conversions are applied before looking for a ref registered with <code>type_name</code></p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import type_name\nfrom apischema.json_schema import serialization_schema\n\n\n@dataclass\nclass Foo:\n    pass\n\n\n@dataclass\nclass Bar:\n    pass\n\n\ndef foo_to_bar(_: Foo) -&gt; Bar:\n    return Bar()\n\n\ntype_name(\"Bars\")(list[Bar])\n\nassert serialization_schema(list[Foo], conversion=foo_to_bar, all_refs=True) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$ref\": \"#/$defs/Bars\",\n    \"$defs\": {\n        # Bars is present because `list[Foo]` is dynamically converted to `list[Bar]`\n        \"Bars\": {\"type\": \"array\", \"items\": {\"$ref\": \"#/$defs/Bar\"}},\n        \"Bar\": {\"type\": \"object\", \"additionalProperties\": False},\n    },\n}\n</code></pre>"},{"location":"conversions/#bypass-registered-conversion","title":"Bypass registered conversion","text":"<p>Using <code>apischema.identity</code> as a dynamic conversion allows you to bypass a registered conversion, i.e. to (de)serialize the given type as it would be without conversion registered.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import identity, serialize, serializer\nfrom apischema.conversions import Conversion\n\n\n@dataclass\nclass RGB:\n    red: int\n    green: int\n    blue: int\n\n    @serializer\n    @property\n    def hexa(self) -&gt; str:\n        return f\"#{self.red:02x}{self.green:02x}{self.blue:02x}\"\n\n\nassert serialize(RGB, RGB(0, 0, 0)) == \"#000000\"\n# dynamic conversion used to bypass the registered one\nassert serialize(RGB, RGB(0, 0, 0), conversion=identity) == {\n    \"red\": 0,\n    \"green\": 0,\n    \"blue\": 0,\n}\n# Expended bypass form\nassert serialize(\n    RGB, RGB(0, 0, 0), conversion=Conversion(identity, source=RGB, target=RGB)\n) == {\"red\": 0, \"green\": 0, \"blue\": 0}\n</code></pre> <p>Note</p> <p>For a more precise selection of bypassed conversion, for <code>tuple</code> or <code>Union</code> member for example, it's possible to pass the concerned class as the source and the target of conversion with <code>identity</code> converter, as shown in the example. </p>"},{"location":"conversions/#liskov-substitution-principle","title":"Liskov substitution principle","text":"<p>LSP is taken into account when applying dynamic conversion: the serializer source can be a subclass of the actual class and the deserializer target can be a superclass of the actual class.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import deserialize, serialize\n\n\n@dataclass\nclass Foo:\n    field: int\n\n\n@dataclass\nclass Bar(Foo):\n    other: str\n\n\ndef foo_to_int(foo: Foo) -&gt; int:\n    return foo.field\n\n\ndef bar_from_int(i: int) -&gt; Bar:\n    return Bar(i, str(i))\n\n\nassert serialize(Bar, Bar(0, \"\"), conversion=foo_to_int) == 0\nassert deserialize(Foo, 0, conversion=bar_from_int) == Bar(0, \"0\")\n</code></pre>"},{"location":"conversions/#generic-dynamic-conversions","title":"Generic dynamic conversions","text":"<p><code>Generic</code> dynamic conversions are supported out of the box. Also, contrary to registered conversions, partially specialized generics are allowed. </p> <pre><code>from collections.abc import Mapping, Sequence\nfrom operator import itemgetter\nfrom typing import TypeVar\n\nfrom apischema import serialize\nfrom apischema.json_schema import serialization_schema\n\nT = TypeVar(\"T\")\nPriority = int\n\n\ndef sort_by_priority(values_with_priority: Mapping[T, Priority]) -&gt; Sequence[T]:\n    return [k for k, _ in sorted(values_with_priority.items(), key=itemgetter(1))]\n\n\nassert serialize(\n    dict[str, Priority], {\"a\": 1, \"b\": 0}, conversion=sort_by_priority\n) == [\"b\", \"a\"]\nassert serialization_schema(dict[str, Priority], conversion=sort_by_priority) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"array\",\n    \"items\": {\"type\": \"string\"},\n}\n</code></pre>"},{"location":"conversions/#field-conversions","title":"Field conversions","text":"<p>It is possible to register a conversion for a particular dataclass field using <code>conversion</code> metadata.</p> <pre><code>import os\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\nfrom apischema import deserialize, serialize\nfrom apischema.conversions import Conversion\nfrom apischema.metadata import conversion\n\n# Set UTC timezone for example\nos.environ[\"TZ\"] = \"UTC\"\ntime.tzset()\n\nfrom_timestamp = Conversion(datetime.fromtimestamp, source=int, target=datetime)\n\n\ndef to_timestamp(d: datetime) -&gt; int:\n    return int(d.timestamp())\n\n\n@dataclass\nclass Foo:\n    some_date: datetime = field(metadata=conversion(from_timestamp, to_timestamp))\n    other_date: datetime\n\n\nassert deserialize(Foo, {\"some_date\": 0, \"other_date\": \"2019-10-13\"}) == Foo(\n    datetime(1970, 1, 1), datetime(2019, 10, 13)\n)\nassert serialize(Foo, Foo(datetime(1970, 1, 1), datetime(2019, 10, 13))) == {\n    \"some_date\": 0,\n    \"other_date\": \"2019-10-13T00:00:00\",\n}\n</code></pre> <p>Note</p> <p>It's possible to pass a conversion only for deserialization or only for serialization</p>"},{"location":"conversions/#serialized-method-conversions","title":"Serialized method conversions","text":"<p>Serialized methods can also have dedicated conversions for their return</p> <pre><code>import os\nimport time\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\nfrom apischema import serialize, serialized\n\n# Set UTC timezone for example\nos.environ[\"TZ\"] = \"UTC\"\ntime.tzset()\n\n\ndef to_timestamp(d: datetime) -&gt; int:\n    return int(d.timestamp())\n\n\n@dataclass\nclass Foo:\n    @serialized(conversion=to_timestamp)\n    def some_date(self) -&gt; datetime:\n        return datetime(1970, 1, 1)\n\n\nassert serialize(Foo, Foo()) == {\"some_date\": 0}\n</code></pre>"},{"location":"conversions/#default-conversions","title":"Default conversions","text":"<p>As with almost every default behavior in apischema, default conversions can be configured using <code>apischema.settings.deserialization.default_conversion</code>/<code>apischema.settings.serialization.default_conversion</code>. The initial value of these settings are the function which retrieved conversions registered with <code>deserializer</code>/<code>serializer</code>.</p> <p>You can for example support attrs classes with this feature:</p> <pre><code>from typing import Sequence\n\nimport attrs\n\nfrom apischema import deserialize, serialize, settings\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.objects import ObjectField\n\nprev_default_object_fields = settings.default_object_fields\n\n\ndef attrs_fields(cls: type) -&gt; Sequence[ObjectField] | None:\n    if hasattr(cls, \"__attrs_attrs__\"):\n        return [\n            ObjectField(\n                a.name, a.type, required=a.default == attrs.NOTHING, default=a.default\n            )\n            for a in getattr(cls, \"__attrs_attrs__\")\n        ]\n    else:\n        return prev_default_object_fields(cls)\n\n\nsettings.default_object_fields = attrs_fields\n\n\n@attrs.define\nclass Foo:\n    bar: int\n\n\nassert deserialize(Foo, {\"bar\": 0}) == Foo(0)\nassert serialize(Foo, Foo(0)) == {\"bar\": 0}\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {\"type\": \"integer\"}},\n    \"required\": [\"bar\"],\n    \"additionalProperties\": False,\n}\n</code></pre> <p>apischema functions (<code>deserialize</code>/<code>serialize</code>/<code>deserialization_schema</code>/<code>serialization_schema</code>/<code>definitions_schema</code>) also have a <code>default_conversion</code> parameter to dynamically modify default conversions. See FAQ for the difference between <code>conversion</code> and <code>default_conversion</code> parameters.</p>"},{"location":"conversions/#sub-conversions","title":"Sub-conversions","text":"<p>Sub-conversions are dynamic conversions applied on the result of a conversion.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Generic, TypeVar\n\nfrom apischema.conversions import Conversion\nfrom apischema.json_schema import serialization_schema\n\nT = TypeVar(\"T\")\n\n\nclass Query(Generic[T]):\n    ...\n\n\ndef query_to_list(q: Query[T]) -&gt; list[T]:\n    ...\n\n\ndef query_to_scalar(q: Query[T]) -&gt; T | None:\n    ...\n\n\n@dataclass\nclass FooModel:\n    bar: int\n\n\nclass Foo:\n    def serialize(self) -&gt; FooModel:\n        ...\n\n\nassert serialization_schema(\n    Query[Foo], conversion=Conversion(query_to_list, sub_conversion=Foo.serialize)\n) == {\n    # We get an array of Foo\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\"bar\": {\"type\": \"integer\"}},\n        \"required\": [\"bar\"],\n        \"additionalProperties\": False,\n    },\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n}\n</code></pre> <p>Sub-conversions can also be used to bypass registered conversions or to define recursive conversions.</p>"},{"location":"conversions/#lazyrecursive-conversions","title":"Lazy/recursive conversions","text":"<p>Conversions can be defined lazily, i.e. using a function returning <code>Conversion</code> (single, or a tuple of it); this function must be wrapped into a <code>apischema.conversions.LazyConversion</code> instance.</p> <p>It allows creating recursive conversions or using a conversion object which can be modified after its definition (for example a conversion for a base class modified by <code>__init_subclass__</code>)</p> <p>It is used by apischema itself for the generated JSON schema. It is indeed a recursive data, and the different versions are handled by a conversion with a lazy recursive sub-conversion.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import serialize\nfrom apischema.conversions import Conversion, LazyConversion\n\n\n@dataclass\nclass Foo:\n    elements: list[\"int | Foo\"]\n\n\ndef foo_elements(foo: Foo) -&gt; list[int | Foo]:\n    return foo.elements\n\n\n# Recursive conversion pattern\ntmp = None\nconversion = Conversion(foo_elements, sub_conversion=LazyConversion(lambda: tmp))\ntmp = conversion\n\nassert serialize(Foo, Foo([0, Foo([1])]), conversion=conversion) == [0, [1]]\n# Without the recursive sub-conversion, it would have been:\nassert serialize(Foo, Foo([0, Foo([1])]), conversion=foo_elements) == [\n    0,\n    {\"elements\": [1]},\n]\n</code></pre>"},{"location":"conversions/#lazy-registered-conversions","title":"Lazy registered conversions","text":"<p>Lazy conversions can also be registered, but the deserialization target/serialization source has to be passed too.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import deserialize, deserializer, serialize, serializer\nfrom apischema.conversions import Conversion\n\n\n@dataclass\nclass Foo:\n    bar: int\n\n\ndeserializer(\n    lazy=lambda: Conversion(lambda bar: Foo(bar), source=int, target=Foo), target=Foo\n)\nserializer(\n    lazy=lambda: Conversion(lambda foo: foo.bar, source=Foo, target=int), source=Foo\n)\n\nassert deserialize(Foo, 0) == Foo(0)\nassert serialize(Foo, Foo(0)) == 0\n</code></pre>"},{"location":"conversions/#conversion-helpers","title":"Conversion helpers","text":""},{"location":"conversions/#string-conversions","title":"String conversions","text":"<p>A common pattern of conversion concerns classes that have a string constructor and a <code>__str__</code> method, for example standard types <code>uuid.UUID</code>, <code>pathlib.Path</code>, or <code>ipaddress.IPv4Address</code>. Using <code>apischema.conversions.as_str</code> will register a string-deserializer from the constructor and a string-serializer from the <code>__str__</code> method. <code>ValueError</code> raised by the constructor is caught and converted to <code>ValidationError</code>.</p> <pre><code>import bson\nimport pytest\n\nfrom apischema import Unsupported, deserialize, serialize\nfrom apischema.conversions import as_str\n\nwith pytest.raises(Unsupported):\n    deserialize(bson.ObjectId, \"0123456789ab0123456789ab\")\nwith pytest.raises(Unsupported):\n    serialize(bson.ObjectId, bson.ObjectId(\"0123456789ab0123456789ab\"))\n\nas_str(bson.ObjectId)\n\nassert deserialize(bson.ObjectId, \"0123456789ab0123456789ab\") == bson.ObjectId(\n    \"0123456789ab0123456789ab\"\n)\nassert (\n    serialize(bson.ObjectId, bson.ObjectId(\"0123456789ab0123456789ab\"))\n    == \"0123456789ab0123456789ab\"\n)\n</code></pre> <p>Note</p> <p>Previously mentioned standard types are handled by apischema using <code>as_str</code>.</p>"},{"location":"conversions/#valueerrorcatching","title":"ValueErrorCatching","text":"<p>Converters can be wrapped with <code>apischema.conversions.catch_value_error</code> in order to catch <code>ValueError</code> and reraise it as a <code>ValidationError</code>. It's notably used but <code>as_str</code> and other standard types.</p> <p>Note</p> <p>This wrapper is in fact inlined in deserialization, so it has better performance than writing the try-catch in the code.</p>"},{"location":"conversions/#use-enum-names","title":"Use <code>Enum</code> names","text":"<p><code>Enum</code> subclasses are (de)serialized using values. However, you may want to use enumeration names instead, that's why apischema provides <code>apischema.conversion.as_names</code> to decorate <code>Enum</code> subclasses.</p> <pre><code>from enum import Enum\n\nfrom apischema import deserialize, serialize\nfrom apischema.conversions import as_names\nfrom apischema.json_schema import deserialization_schema, serialization_schema\n\n\n@as_names\nclass MyEnum(Enum):\n    FOO = object()\n    BAR = object()\n\n\nassert deserialize(MyEnum, \"FOO\") == MyEnum.FOO\nassert serialize(MyEnum, MyEnum.FOO) == \"FOO\"\nassert (\n    deserialization_schema(MyEnum)\n    == serialization_schema(MyEnum)\n    == {\n        \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n        \"type\": \"string\",\n        \"enum\": [\"FOO\", \"BAR\"],\n    }\n)\n</code></pre>"},{"location":"conversions/#class-as-union-of-its-subclasses","title":"Class as union of its subclasses","text":""},{"location":"conversions/#object-deserialization-transform-function-into-a-dataclass-deserializer","title":"Object deserialization \u2014 transform function into a dataclass deserializer","text":"<p><code>apischema.objects.object_deserialization</code> can convert a function into a new function taking a unique parameter, a dataclass whose fields are mapped from the original function parameters.</p> <p>It can be used for example to build a deserialization conversion from an alternative constructor.</p> <pre><code>from apischema import deserialize, deserializer, type_name\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.objects import object_deserialization\n\n\ndef create_range(start: int, stop: int, step: int = 1) -&gt; range:\n    return range(start, stop, step)\n\n\nrange_conv = object_deserialization(create_range, type_name(\"Range\"))\n# Conversion can be registered\ndeserializer(range_conv)\nassert deserialize(range, {\"start\": 0, \"stop\": 10}) == range(0, 10)\nassert deserialization_schema(range) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"start\": {\"type\": \"integer\"},\n        \"stop\": {\"type\": \"integer\"},\n        \"step\": {\"type\": \"integer\", \"default\": 1},\n    },\n    \"required\": [\"start\", \"stop\"],\n    \"additionalProperties\": False,\n}\n</code></pre> <p>Note</p> <p>Parameters metadata can be specified using <code>typing.Annotated</code>, or be passed with <code>parameters_metadata</code> parameter, which is a mapping of parameter names as key and mapped metadata as value.</p>"},{"location":"conversions/#object-serialization-select-only-a-subset-of-fields","title":"Object serialization \u2014 select only a subset of fields","text":"<p><code>apischema.objects.object_serialization</code> can be used to serialize only a subset of an object fields and methods.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Any\n\nfrom apischema import alias, serialize, type_name\nfrom apischema.json_schema import JsonSchemaVersion, definitions_schema\nfrom apischema.objects import get_field, object_serialization\n\n\n@dataclass\nclass Data:\n    id: int\n    content: str\n\n    @property\n    def size(self) -&gt; int:\n        return len(self.content)\n\n    def get_details(self) -&gt; Any:\n        ...\n\n\n# Serialization fields can be a str/field or a function/method/property\nsize_only = object_serialization(\n    Data, [get_field(Data).id, Data.size], type_name(\"DataSize\")\n)\n# [\"id\", Data.size] would also work\n\n\ndef complete_data():\n    return [\n        ...,  # shortcut to include all the fields\n        Data.size,\n        (Data.get_details, alias(\"details\")),  # add/override metadata using tuple\n    ]\n\n\n# Serialization fields computation can be deferred in a function\n# The serialization name will then be defaulted to the function name\ncomplete = object_serialization(Data, complete_data)\n\ndata = Data(0, \"data\")\nassert serialize(Data, data, conversion=size_only) == {\"id\": 0, \"size\": 4}\nassert serialize(Data, data, conversion=complete) == {\n    \"id\": 0,\n    \"content\": \"data\",\n    \"size\": 4,\n    \"details\": None,  # because get_details return None in this example\n}\n\n\nassert definitions_schema(\n    serialization=[(Data, size_only), (Data, complete)],\n    version=JsonSchemaVersion.OPEN_API_3_0,\n) == {\n    \"DataSize\": {\n        \"type\": \"object\",\n        \"properties\": {\"id\": {\"type\": \"integer\"}, \"size\": {\"type\": \"integer\"}},\n        \"required\": [\"id\", \"size\"],\n        \"additionalProperties\": False,\n    },\n    \"CompleteData\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"integer\"},\n            \"content\": {\"type\": \"string\"},\n            \"size\": {\"type\": \"integer\"},\n            \"details\": {},\n        },\n        \"required\": [\"id\", \"content\", \"size\", \"details\"],\n        \"additionalProperties\": False,\n    },\n}\n</code></pre>"},{"location":"conversions/#faq","title":"FAQ","text":""},{"location":"conversions/#whats-the-difference-between-conversion-and-default_conversion-parameters","title":"What's the difference between <code>conversion</code> and <code>default_conversion</code> parameters?","text":"<p>Dynamic conversions (<code>conversion</code> parameter) exists to ensure consistency and reuse of subschemas referenced (with a <code>$ref</code>) in the JSON/OpenAPI schema. </p> <p>In fact, different global conversions (<code>default_conversion</code> parameter) could lead to having a field with different schemas depending on global conversions, so a class would not be able to be referenced consistently. Because dynamic conversions are local, they cannot mess with an object field schema.</p> <p>Schema generation uses the same default conversions for all definitions (which can have associated dynamic conversion).</p> <p><code>default_conversion</code> parameter allows having different (de)serialization contexts, for example to map date to string between frontend and backend, and to timestamp between backend services.</p>"},{"location":"data_model/","title":"Data model","text":"<p>apischema handles every class/type you need.</p> <p>By the way, it's done in an additive way, meaning that it doesn't affect your types.</p>"},{"location":"data_model/#pep-585","title":"PEP 585","text":"<p>With Python 3.9 and PEP 585, typing is substantially shaken up; all container types of <code>typing</code> module are now deprecated.</p> <p>apischema fully support 3.9 and PEP 585, as shown in the different examples. However, <code>typing</code> containers can still be used, especially/necessarily when using an older version.  </p>"},{"location":"data_model/#dataclasses","title":"Dataclasses","text":"<p>Because the library aims to bring the minimum boilerplate, it's built on the top of standard library. Dataclasses are thus the core structure of the data model.</p> <p>Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of <code>dataclasses.field</code>, customization is done with the <code>metadata</code> parameter; metadata can also be passed using PEP 593 <code>typing.Annotated</code>.</p> <p>With some teasing of features presented later:</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Annotated\n\nfrom apischema import alias, schema\nfrom apischema.metadata import required\n\n\n@dataclass\nclass Foo:\n    bar: int = field(\n        default=0,\n        metadata=alias(\"foo_bar\") | schema(title=\"foo! bar!\", min=0, max=42) | required,\n    )\n    baz: Annotated[\n        int, alias(\"foo_baz\"), schema(title=\"foo! baz!\", min=0, max=32), required\n    ] = 0\n    # pipe `|` operator can also be used in Annotated\n</code></pre> <p>Note</p> <p>Field's metadata are just an ordinary <code>dict</code>; apischema provides some functions to enrich these metadata with its own keys (<code>alias(\"foo_bar\")</code> is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to apischema and other keys can be added.</p> <p>Because PEP 584 is painfully missing before Python 3.9, apischema metadata use their own subclass of <code>dict</code> just to add <code>|</code> operator for convenience in all Python versions.</p> <p>Dataclasses <code>__post_init__</code> and <code>field(init=False)</code> are fully supported. Implications of this feature usage are documented in the relative sections.</p> <p>Warning</p> <p>Before 3.8, <code>InitVar</code> is doing type erasure, which is why it's not possible for apischema to retrieve type information of init variables. To fix this behavior, a field metadata <code>init_var</code> can be used to put back the type of the field (<code>init_var</code> also accepts stringified type annotations).</p> <p>Dataclass-like types (attrs/SQLAlchemy/etc.) can also be supported with a few lines of code, see next section</p>"},{"location":"data_model/#standard-library-types","title":"Standard library types","text":"<p>apischema natively handles most of the types provided by the standard library. They are sorted in the following categories:</p>"},{"location":"data_model/#primitive","title":"Primitive","text":"<p><code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>None</code>, subclasses of them</p> <p>They correspond to JSON primitive types.</p>"},{"location":"data_model/#collection","title":"Collection","text":"<ul> <li><code>collection.abc.Collection</code> (<code>typing.Collection</code>)</li> <li><code>collection.abc.Sequence</code> (<code>typing.Sequence</code>)</li> <li><code>tuple</code> (<code>typing.Tuple</code>)</li> <li><code>collection.abc.MutableSequence</code> (<code>typing.MutableSequence</code>)</li> <li><code>list</code> (<code>typing.List</code>)</li> <li><code>collection.abc.Set</code> (<code>typing.AbstractSet</code>)</li> <li><code>collection.abc.MutableSet</code> (<code>typing.MutableSet</code>)</li> <li><code>frozenset</code> (<code>typing.FrozenSet</code>)</li> <li><code>set</code> (<code>typing.Set</code>)</li> </ul> <p>They correspond to JSON array and are serialized to <code>list</code>.</p>"},{"location":"data_model/#mapping","title":"Mapping","text":"<ul> <li><code>collection.abc.Mapping</code> (<code>typing.Mapping</code>)</li> <li><code>collection.abc.MutableMapping</code> (<code>typing.MutableMapping</code>)</li> <li><code>dict</code> (<code>typing.Dict</code>)</li> </ul> <p>They correpond to JSON object and are serialized to <code>dict</code>.</p>"},{"location":"data_model/#enumeration","title":"Enumeration","text":"<p><code>enum.Enum</code> subclasses, <code>typing.Literal</code></p> <p>Warning</p> <p><code>Enum</code> subclasses are (de)serialized using values, not names. apischema also provides a conversion to use names instead.</p>"},{"location":"data_model/#typing-facilities","title":"Typing facilities","text":"<ul> <li><code>typing.Optional</code>/<code>typing.Union</code> (<code>Optional[T]</code> is strictly equivalent to <code>Union[T, None]</code>)</li> </ul> <p>: Deserialization select the first matching alternative; unsupported alternatives are ignored</p> <ul> <li><code>tuple</code> (<code>typing.Tuple</code>)</li> </ul> <p>: Can be used as collection as well as true tuple, like <code>tuple[str, int]</code></p> <ul> <li><code>typing.NewType</code></li> </ul> <p>: Serialized according to its base type</p> <ul> <li><code>typing.NamedTuple</code></li> </ul> <p>: Handled as an object type, roughly like a dataclass; fields metadata can be passed using <code>Annotated</code></p> <ul> <li><code>typing.TypedDict</code></li> </ul> <p>: Handled as an object type, but with a dictionary shape; fields metadata can be passed using <code>Annotated</code></p> <ul> <li><code>typing.Any</code></li> </ul> <p>: Untouched by deserialization, serialized according to the object runtime class</p> <ul> <li><code>typing.LiteralString</code></li> </ul> <p>: Handled as <code>str</code></p>"},{"location":"data_model/#other-standard-library-types","title":"Other standard library types","text":"<ul> <li><code>bytes</code></li> </ul> <p>: with <code>str</code> (de)serialization using base64 encoding</p> <ul> <li><code>datetime.datetime</code></li> <li><code>datetime.date</code></li> <li><code>datetime.time</code></li> </ul> <p>: Supported only in 3.7+ with <code>fromisoformat</code>/<code>isoformat</code></p> <ul> <li><code>Decimal</code></li> </ul> <p>: With <code>float</code> (de)serialization</p> <ul> <li><code>ipaddress.IPv4Address</code> </li> <li><code>ipaddress.IPv4Interface</code></li> <li><code>ipaddress.IPv4Network</code></li> <li><code>ipaddress.IPv6Address</code> </li> <li><code>ipaddress.IPv6Interface</code></li> <li><code>ipaddress.IPv6Network</code></li> <li><code>pathlib.Path</code></li> <li><code>re.Pattern</code> (<code>typing.Pattern</code>)</li> <li><code>uuid.UUID</code></li> </ul> <p>: With <code>str</code> (de)serialization</p>"},{"location":"data_model/#generic","title":"Generic","text":"<p><code>typing.Generic</code> can be used out of the box like in the following example:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Generic, TypeVar\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize\n\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass Box(Generic[T]):\n    content: T\n\n\nassert deserialize(Box[str], {\"content\": \"void\"}) == Box(\"void\")\nwith pytest.raises(ValidationError):\n    deserialize(Box[str], {\"content\": 42})\n</code></pre> <p>Warning</p> <p>Generic types don't have default type name (used in JSON/GraphQL schema) \u2014 should <code>Group[Foo]</code> be named <code>GroupFoo</code>/<code>FooGroup</code>/something else? \u2014 so they require by-class or default <code>type_name</code> assignment.</p>"},{"location":"data_model/#recursive-types-string-annotations-and-pep-563","title":"Recursive types, string annotations and PEP 563","text":"<p>Recursive classes can be typed as they usually do, with or without PEP 563. Here with string annotations: <pre><code>from dataclasses import dataclass\nfrom typing import Optional\n\nfrom apischema import deserialize\n\n\n@dataclass\nclass Node:\n    value: int\n    child: Optional[\"Node\"] = None\n\n\nassert deserialize(Node, {\"value\": 0, \"child\": {\"value\": 1}}) == Node(0, Node(1))\n</code></pre> Here with PEP 563 (requires 3.7+) <pre><code>from __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom apischema import deserialize\n\n\n@dataclass\nclass Node:\n    value: int\n    child: Node | None = None\n\n\nassert deserialize(Node, {\"value\": 0, \"child\": {\"value\": 1}}) == Node(0, Node(1))\n</code></pre></p> <p>Warning</p> <p>To resolve annotations, apischema uses <code>typing.get_type_hints</code>; this doesn't work really well when used on objects defined outside of global scope.</p> <p>Warning (minor)</p> <p>Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370</p>"},{"location":"data_model/#null-vs-undefined","title":"<code>null</code> vs. <code>undefined</code>","text":"<p>Contrary to Javascript, Python doesn't have an <code>undefined</code> equivalent (if we consider <code>None</code> to be the equivalent of <code>null</code>). But it can be useful to distinguish (especially when thinking about HTTP <code>PATCH</code> method) between a <code>null</code> field and an <code>undefined</code>/absent field.</p> <p>That's why apischema provides an <code>Undefined</code> constant (a single instance of <code>UndefinedType</code> class) which can be used as a default value everywhere where this distinction is needed. In fact, default values are used when field are absent, thus a default <code>Undefined</code> will mark the field as absent. </p> <p>Dataclass/<code>NamedTuple</code> fields are ignored by serialization when <code>Undefined</code>. </p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import Undefined, UndefinedType, deserialize, serialize\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Foo:\n    bar: int | UndefinedType = Undefined\n    baz: int | UndefinedType | None = Undefined\n\n\nassert deserialize(Foo, {\"bar\": 0, \"baz\": None}) == Foo(0, None)\nassert deserialize(Foo, {}) == Foo(Undefined, Undefined)\nassert serialize(Foo, Foo(Undefined, 42)) == {\"baz\": 42}\n# Foo.bar and Foo.baz are not required\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {\"type\": \"integer\"}, \"baz\": {\"type\": [\"integer\", \"null\"]}},\n    \"additionalProperties\": False,\n}\n</code></pre> <p>Note</p> <p><code>UndefinedType</code> must only be used inside an <code>Union</code>, as it has no sense as a standalone type. By the way, no suitable name was found to shorten <code>Union[T, UndefinedType]</code> but propositions are welcomed.</p> <p>Note</p> <p><code>Undefined</code> is a falsy constant, i.e. <code>bool(Undefined) is False</code>.</p>"},{"location":"data_model/#use-none-as-if-it-was-undefined","title":"Use <code>None</code> as if it was <code>Undefined</code>","text":"<p>Using <code>None</code> can be more convenient than <code>Undefined</code> as a placeholder for missing value, but <code>Optional</code> types are translated to nullable fields.</p> <p>That's why apischema provides <code>none_as_undefined</code> metadata, allowing <code>None</code> to be handled as if it was <code>Undefined</code>: type will not be nullable and field not serialized if its value is <code>None</code>.</p> <pre><code>from dataclasses import dataclass, field\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, serialize\nfrom apischema.json_schema import deserialization_schema, serialization_schema\nfrom apischema.metadata import none_as_undefined\n\n\n@dataclass\nclass Foo:\n    bar: str | None = field(default=None, metadata=none_as_undefined)\n\n\nassert (\n    deserialization_schema(Foo)\n    == serialization_schema(Foo)\n    == {\n        \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n        \"type\": \"object\",\n        \"properties\": {\"bar\": {\"type\": \"string\"}},\n        \"additionalProperties\": False,\n    }\n)\nwith pytest.raises(ValidationError):\n    deserialize(Foo, {\"bar\": None})\nassert serialize(Foo, Foo(None)) == {}\n</code></pre>"},{"location":"data_model/#annotated-pep-593","title":"Annotated - PEP 593","text":"<p>PEP 593 is fully supported; annotations stranger to apischema are simply ignored.</p>"},{"location":"data_model/#custom-types","title":"Custom types","text":"<p>apischema can support almost all of your custom types in a few lines of code, using the conversion feature. However, it also provides a simple and direct way to support dataclass-like types, as presented below.</p> <p>Otherwise, when apischema encounters a type that it doesn't support, <code>apischema.Unsupported</code> exception will be raised.</p> <p>Note</p> <p>In the rare case when a union member should be ignored by apischema, it's possible to use mark it as unsupported using <code>Union[Foo, Annotated[Bar, Unsupported]]</code>.</p>"},{"location":"data_model/#dataclass-like-types-aka-object-types","title":"Dataclass-like types, aka object types","text":"<p>Internally, apischema handle standard object types \u2014 dataclasses, named tuple and typed dictionary \u2014 the same way by mapping them to a set of <code>apischema.objects.ObjectField</code>, which has the following definition:</p> <pre><code>@dataclass(frozen=True)\nclass ObjectField:\n    name: str  # field's name\n    type: Any  # field's type\n    required: bool = True  # if the field is required\n    metadata: Mapping[str, Any] = field(default_factory=dict)  # field's metadata \n    default: InitVar[Any] = ...  # field's default value\n    default_factory: Optional[Callable[[], Any]] = None  # field's default factory\n    kind: FieldKind = FieldKind.NORMAL  # NORMAL/READ_ONLY/WRITE_ONLY\n</code></pre> <p>Thus, support of dataclass-like types (attrs, SQLAlchemy traditional mappers, etc.) can be achieved by mapping the concerned class to its own list of <code>ObjectField</code>s; this is done using <code>apischema.objects.set_object_fields</code>.</p> <pre><code>from apischema import deserialize, serialize\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.objects import ObjectField, set_object_fields\n\n\nclass Foo:\n    def __init__(self, bar):\n        self.bar = bar\n\n\nset_object_fields(Foo, [ObjectField(\"bar\", int)])\n# Fields can also be passed in a factory\nset_object_fields(Foo, lambda: [ObjectField(\"bar\", int)])\n\nfoo = deserialize(Foo, {\"bar\": 0})\nassert isinstance(foo, Foo) and foo.bar == 0\nassert serialize(Foo, Foo(0)) == {\"bar\": 0}\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {\"type\": \"integer\"}},\n    \"required\": [\"bar\"],\n    \"additionalProperties\": False,\n}\n</code></pre> <p>Another way to set object fields is to directly modify apischema default behavior, using <code>apischema.settings.default_object_fields</code>.</p> <p>Note</p> <p><code>set_object_fields</code>/<code>settings.default_object_fields</code> can be used to override existing fields. Current fields can be retrieved using <code>apischema.objects.object_fields</code>.</p> <pre><code>from collections.abc import Sequence\nfrom typing import Optional\nfrom apischema import settings\nfrom apischema.objects import ObjectField\n\nprevious_default_object_fields = settings.default_object_field\n\n\ndef default_object_fields(cls) -&gt; Optional[Sequence[ObjectField]]:\n    return [...] if ... else previous_default_object_fields(cls)\n\n\nsettings.default_object_fields = default_object_fields\n</code></pre> <p>Note</p> <p>Almost every default behavior of apischema can be customized using <code>apischema.settings</code>.</p> <p>Examples of SQLAlchemy support and attrs support illustrate both methods (which could also be combined).</p>"},{"location":"data_model/#skip-field","title":"Skip field","text":"<p>Dataclass fields can be excluded from apischema processing by using <code>apischema.metadata.skip</code> in the field metadata. It can be parametrized with <code>deserialization</code>/<code>serialization</code> boolean parameters to skip a field only for the given operations.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\nfrom apischema.json_schema import deserialization_schema, serialization_schema\nfrom apischema.metadata import skip\n\n\n@dataclass\nclass Foo:\n    bar: Any\n    deserialization_only: Any = field(metadata=skip(serialization=True))\n    serialization_only: Any = field(default=None, metadata=skip(deserialization=True))\n    baz: Any = field(default=None, metadata=skip)\n\n\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {}, \"deserialization_only\": {}},\n    \"required\": [\"bar\", \"deserialization_only\"],\n    \"additionalProperties\": False,\n}\nassert serialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {}, \"serialization_only\": {}},\n    \"required\": [\"bar\", \"serialization_only\"],\n    \"additionalProperties\": False,\n}   \n</code></pre> <p>Note</p> <p>Fields skipped in deserialization should have a default value if deserialized, because deserialization of the class could raise otherwise.</p>"},{"location":"data_model/#skip-field-serialization-depending-on-condition","title":"Skip field serialization depending on condition","text":"<p>Field can also be skipped when serializing, depending on the condition given by <code>serialization_if</code>, or when the field value is equal to its default value with <code>serialization_default=True</code>.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\nfrom apischema import serialize\nfrom apischema.metadata import skip\n\n\n@dataclass\nclass Foo:\n    bar: Any = field(metadata=skip(serialization_if=lambda x: not x))\n    baz: Any = field(default_factory=list, metadata=skip(serialization_default=True))\n\n\nassert serialize(Foo(False, [])) == {}   \n</code></pre>"},{"location":"data_model/#composition-over-inheritance-composed-dataclasses-flattening","title":"Composition over inheritance - composed dataclasses flattening","text":"<p>Dataclass fields which are themselves dataclass can be \"flattened\" into the owning one by using <code>flatten</code> metadata. Then, when the class is (de)serialized, \"flattened\" fields will be (de)serialized at the same level as the owning class.</p> <pre><code>from dataclasses import dataclass, field\n\nfrom apischema import Undefined, UndefinedType, alias, deserialize, serialize\nfrom apischema.fields import with_fields_set\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.metadata import flatten\n\n\n@dataclass\nclass JsonSchema:\n    title: str | UndefinedType = Undefined\n    description: str | UndefinedType = Undefined\n    format: str | UndefinedType = Undefined\n    ...\n\n\n@with_fields_set\n@dataclass\nclass RootJsonSchema:\n    schema: str | UndefinedType = field(default=Undefined, metadata=alias(\"$schema\"))\n    defs: list[JsonSchema] = field(default_factory=list, metadata=alias(\"$defs\"))\n    # This field schema is flattened inside the owning one\n    json_schema: JsonSchema = field(default_factory=JsonSchema, metadata=flatten)\n\n\ndata = {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"title\": \"flattened example\",\n}\nroot_schema = RootJsonSchema(\n    schema=\"http://json-schema.org/draft/2020-12/schema#\",\n    json_schema=JsonSchema(title=\"flattened example\"),\n)\nassert deserialize(RootJsonSchema, data) == root_schema\nassert serialize(RootJsonSchema, root_schema) == data\nassert deserialization_schema(RootJsonSchema) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$defs\": {\n        \"JsonSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"title\": {\"type\": \"string\"},\n                \"description\": {\"type\": \"string\"},\n                \"format\": {\"type\": \"string\"},\n            },\n            \"additionalProperties\": False,\n        }\n    },\n    # It results in allOf + unevaluatedProperties=False\n    \"allOf\": [\n        # RootJsonSchema (without JsonSchema)\n        {\n            \"type\": \"object\",\n            \"properties\": {\n                \"$schema\": {\"type\": \"string\"},\n                \"$defs\": {\n                    \"type\": \"array\",\n                    \"items\": {\"$ref\": \"#/$defs/JsonSchema\"},\n                    \"default\": [],\n                },\n            },\n            \"additionalProperties\": False,\n        },\n        # JonsSchema\n        {\"$ref\": \"#/$defs/JsonSchema\"},\n    ],\n    \"unevaluatedProperties\": False,\n}\n</code></pre> <p>Note</p> <p>Generated JSON schema use <code>unevaluatedProperties</code> keyword.</p> <p>This feature is very convenient for building model by composing smaller components. If some kind of reuse could also be achieved with inheritance, it can be less practical when it comes to use it in code, because there is no easy way to build an inherited class when you have an instance of the super class; you have to copy all the fields by hand. On the other hand, using composition (of flattened fields), it's easy to instantiate the class when the smaller component is just a field of it.</p>"},{"location":"data_model/#faq","title":"FAQ","text":""},{"location":"data_model/#why-isnt-iterable-handled-with-other-collection-types","title":"Why isn't <code>Iterable</code> handled with other collection types?","text":"<p>Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterables are computation objects, they can be infinite, etc. They don't correspond to a serialized data; <code>Collection</code> is way more appropriate in this context.</p>"},{"location":"data_model/#what-happens-if-i-override-dataclass-__init__","title":"What happens if I override dataclass <code>__init__</code>?","text":"<p>apischema always assumes that dataclass <code>__init__</code> can be called with all its fields as kwargs parameters. If that's no longer the case after a modification of <code>__init__</code> (what means if an exception is thrown when the constructor is called because of bad parameters), apischema treats then the class as not supported.</p>"},{"location":"de_serialization/","title":"(De)serialization","text":"<p>apischema aims to help with deserialization/serialization of API data, mostly JSON.</p> <p>Let's start again with the overview example <pre><code>from collections.abc import Collection\nfrom dataclasses import dataclass, field\nfrom uuid import UUID, uuid4\n\nimport pytest\nfrom graphql import print_schema\n\nfrom apischema import ValidationError, deserialize, serialize\nfrom apischema.graphql import graphql_schema\nfrom apischema.json_schema import deserialization_schema\n\n\n# Define a schema with standard dataclasses\n@dataclass\nclass Resource:\n    id: UUID\n    name: str\n    tags: set[str] = field(default_factory=set)\n\n\n# Get some data\nuuid = uuid4()\ndata = {\"id\": str(uuid), \"name\": \"wyfo\", \"tags\": [\"some_tag\"]}\n# Deserialize data\nresource = deserialize(Resource, data)\nassert resource == Resource(uuid, \"wyfo\", {\"some_tag\"})\n# Serialize objects\nassert serialize(Resource, resource) == data\n# Validate during deserialization\nwith pytest.raises(ValidationError) as err:  # pytest checks exception is raised\n    deserialize(Resource, {\"id\": \"42\", \"name\": \"wyfo\"})\nassert err.value.errors == [\n    {\"loc\": [\"id\"], \"err\": \"badly formed hexadecimal UUID string\"}\n]\n# Generate JSON Schema\nassert deserialization_schema(Resource) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"id\": {\"type\": \"string\", \"format\": \"uuid\"},\n        \"name\": {\"type\": \"string\"},\n        \"tags\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"},\n            \"uniqueItems\": True,\n            \"default\": [],\n        },\n    },\n    \"required\": [\"id\", \"name\"],\n    \"additionalProperties\": False,\n}\n\n\n# Define GraphQL operations\ndef resources(tags: Collection[str] | None = None) -&gt; Collection[Resource] | None:\n    ...\n\n\n# Generate GraphQL schema\nschema = graphql_schema(query=[resources], id_types={UUID})\nschema_str = \"\"\"\\\ntype Query {\n  resources(tags: [String!]): [Resource!]\n}\n\ntype Resource {\n  id: ID!\n  name: String!\n  tags: [String!]!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre></p>"},{"location":"de_serialization/#deserialization_1","title":"Deserialization","text":"<p><code>apischema.deserialize</code> deserializes Python types from JSON-like data: <code>dict</code>/<code>list</code>/<code>str</code>/<code>int</code>/<code>float</code>/<code>bool</code>/<code>None</code> \u2014 in short, what you get when you execute <code>json.loads</code>. Types can be dataclasses as well as <code>list[int]</code>, <code>NewType</code>s, or whatever you want (see conversions to extend deserialization support to every type you want).</p> <pre><code>from collections.abc import Collection, Mapping\nfrom dataclasses import dataclass\nfrom typing import NewType\n\nfrom apischema import deserialize\n\n\n@dataclass\nclass Foo:\n    bar: str\n\n\nMyInt = NewType(\"MyInt\", int)\n\nassert deserialize(Foo, {\"bar\": \"bar\"}) == Foo(\"bar\")\nassert deserialize(MyInt, 0) == MyInt(0) == 0\nassert deserialize(Mapping[str, Collection[Foo]], {\"key\": [{\"bar\": \"42\"}]}) == {\n    \"key\": [Foo(\"42\")]\n}\n</code></pre> <p>Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation).</p>"},{"location":"de_serialization/#deserialization-passthrough","title":"Deserialization passthrough","text":"<p>In some case, e.g. MessagePack loading with raw bytes inside, some data will have other type than JSON primitive ones. These types can be allowed using <code>pass_through</code> parameter; it must be collection of classes, or a predicate. Behavior can also be set globally using <code>apischema.settings.deserialization.pass_through</code>.</p> <p>Only non JSON primitive classes can be allowed, because apischema relies on a type check with <code>isinstance</code> to skip deserialization. That exclude <code>NewType</code> but also <code>TypeDict</code>. </p> <pre><code>from datetime import datetime, timedelta\n\nfrom apischema import deserialize\n\nstart, end = datetime.now(), datetime.now() + timedelta(1)\nassert deserialize(\n    tuple[datetime, datetime], [start, end], pass_through={datetime}\n) == (start, end)\n# Passing through types can also be deserialized normally from JSON types\nassert deserialize(\n    tuple[datetime, datetime],\n    [start.isoformat(), end.isoformat()],\n    pass_through={datetime},\n) == (start, end)\n</code></pre> <p>Note</p> <p>Equivalent serialization feature is presented in optimizations documentation.</p>"},{"location":"de_serialization/#strictness","title":"Strictness","text":""},{"location":"de_serialization/#coercion","title":"Coercion","text":"<p>apischema is strict by default. You ask for an integer, you have to receive an integer. </p> <p>However, in some cases, data has to be be coerced, for example when parsing a configuration file. That can be done using <code>coerce</code> parameter; when set to <code>True</code>, all primitive types will be coerced to the expected type of the data model like the following:</p> <pre><code>import pytest\n\nfrom apischema import ValidationError, deserialize\n\nwith pytest.raises(ValidationError):\n    deserialize(bool, \"ok\")\nassert deserialize(bool, \"ok\", coerce=True)\n</code></pre> <p><code>bool</code> can be coerced from <code>str</code> with the following case-insensitive mapping:</p> False True 0 1 f t n y no yes false true off on ko ok <p>The <code>coerce</code> parameter can also receive a coercion function which will then be used instead of default one.</p> <pre><code>from typing import TypeVar, cast\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize\n\nT = TypeVar(\"T\")\n\n\ndef coerce(cls: type[T], data) -&gt; T:\n    \"\"\"Only coerce int to bool\"\"\"\n    if cls is bool and isinstance(data, int):\n        return cast(T, bool(data))\n    else:\n        return data\n\n\nwith pytest.raises(ValidationError):\n    deserialize(bool, 0)\nwith pytest.raises(ValidationError):\n    assert deserialize(bool, \"ok\", coerce=coerce)\nassert deserialize(bool, 1, coerce=coerce)\n</code></pre> <p>Note</p> <p>If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message</p> <p>Warning</p> <p>Coercer first argument is a primitive json type <code>str</code>/<code>bool</code>/<code>int</code>/<code>float</code>/<code>list</code>/<code>dict</code>/<code>type(None)</code>; it can be <code>type(None)</code>, so returning <code>cls(data)</code> will fail in this case.</p>"},{"location":"de_serialization/#additional-properties","title":"Additional properties","text":"<p>apischema is strict too about the number of fields received for an object. In JSON schema terms, apischema put <code>\"additionalProperties\": false</code> by default (this can be configured by class with properties field). </p> <p>This behavior can be controlled by <code>additional_properties</code> parameter. When set to <code>True</code>, it prevents the rejection of unexpected properties. </p> <pre><code>from dataclasses import dataclass\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize\n\n\n@dataclass\nclass Foo:\n    bar: str\n\n\ndata = {\"bar\": \"bar\", \"other\": 42}\nwith pytest.raises(ValidationError):\n    deserialize(Foo, data)\nassert deserialize(Foo, data, additional_properties=True) == Foo(\"bar\")\n</code></pre>"},{"location":"de_serialization/#fall-back-on-default","title":"Fall back on default","text":"<p>Validation errors can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fall back on this default; this is enabled by <code>fall_back_on_default</code> parameter. This behavior can also be configured for each field using metadata. </p> <pre><code>from dataclasses import dataclass, field\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize\nfrom apischema.metadata import fall_back_on_default\n\n\n@dataclass\nclass Foo:\n    bar: str = \"bar\"\n    baz: str = field(default=\"baz\", metadata=fall_back_on_default)\n\n\nwith pytest.raises(ValidationError):\n    deserialize(Foo, {\"bar\": 0})\nassert deserialize(Foo, {\"bar\": 0}, fall_back_on_default=True) == Foo()\nassert deserialize(Foo, {\"baz\": 0}) == Foo()\n</code></pre>"},{"location":"de_serialization/#strictness-configuration","title":"Strictness configuration","text":"<p>apischema global configuration is managed through <code>apischema.settings</code> object. It has, among other, three global variables <code>settings.additional_properties</code>, <code>settings.deserialization.coerce</code> and <code>settings.deserialization.fall_back_on_default</code> whose values are used as default parameter values for the <code>deserialize</code>; by default, <code>additional_properties=False</code>, <code>coerce=False</code> and <code>fall_back_on_default=False</code>.</p> <p>Note</p> <p><code>additional_properties</code> settings is in <code>settings.deserialization</code> because it's also used in serialization.</p> <p>Global coercion function can be set with <code>settings.coercer</code> following this example:</p> <pre><code>import json\nfrom apischema import ValidationError, settings\n\nprev_coercer = settings.coercer\n\ndef coercer(cls, data):\n    \"\"\"In case of coercion failures, try to deserialize json data\"\"\"\n    try:\n        return prev_coercer(cls, data)\n    except ValidationError as err:\n        if not isinstance(data, str):\n            raise\n        try:\n            return json.loads(data)\n        except json.JSONDecodeError:\n            raise err\n\nsettings.coercer = coercer\n</code></pre>"},{"location":"de_serialization/#fields-set","title":"Fields set","text":"<p>Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section)</p> <p>Because apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. apischema provides a simple API to get/set this metadata.  </p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import deserialize\nfrom apischema.fields import (\n    fields_set,\n    is_set,\n    set_fields,\n    unset_fields,\n    with_fields_set,\n)\n\n\n# This decorator enable the feature\n@with_fields_set\n@dataclass\nclass Foo:\n    bar: int\n    baz: str | None = None\n\n\n# Retrieve fields set\nfoo1 = Foo(0, None)\nassert fields_set(foo1) == {\"bar\", \"baz\"}\nfoo2 = Foo(0)\nassert fields_set(foo2) == {\"bar\"}\n# Test fields individually (with autocompletion and refactoring)\nassert is_set(foo1).baz\nassert not is_set(foo2).baz\n# Mark fields as set/unset\nset_fields(foo2, \"baz\")\nassert fields_set(foo2) == {\"bar\", \"baz\"}\nunset_fields(foo2, \"baz\")\nassert fields_set(foo2) == {\"bar\"}\nset_fields(foo2, \"baz\", overwrite=True)\nassert fields_set(foo2) == {\"baz\"}\n# Fields modification are taken in account\nfoo2.bar = 0\nassert fields_set(foo2) == {\"bar\", \"baz\"}\n# Because deserialization use normal constructor, it works with the feature\nfoo3 = deserialize(Foo, {\"bar\": 0})\nassert fields_set(foo3) == {\"bar\"}\n</code></pre> <p>Warning</p> <p>The <code>with_fields_set</code> decorator MUST be put above <code>dataclass</code> one. This is because both of them modify <code>__init__</code> method, but only the first is built to take the second in account.</p> <p>Warning</p> <p><code>dataclasses.replace</code> works by setting all the fields of the replaced object. Because of this issue, apischema provides a little wrapper <code>apischema.dataclasses.replace</code>.</p>"},{"location":"de_serialization/#serialization","title":"Serialization","text":"<p><code>apischema.serialize</code> is used to serialize Python objects to JSON-like data. Contrary to <code>apischema.deserialize</code>, Python type can be omitted; in this case, the object will be serialized with an <code>typing.Any</code> type, i.e. the class of the serialized object will be used.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Any\n\nfrom apischema import serialize\n\n\n@dataclass\nclass Foo:\n    bar: str\n\n\nassert serialize(Foo, Foo(\"baz\")) == {\"bar\": \"baz\"}\nassert serialize(tuple[int, int], (0, 1)) == [0, 1]\nassert (\n    serialize(Any, {\"key\": (\"value\", 42)})\n    == serialize({\"key\": (\"value\", 42)})\n    == {\"key\": [\"value\", 42]}\n)\nassert serialize(Foo(\"baz\")) == {\"bar\": \"baz\"}\n</code></pre> <p>Note</p> <p>Omitting type with <code>serialize</code> can have unwanted side effects, as it makes loose any type annotations of the serialized object. In fact, generic specialization as well as PEP 593 annotations cannot be retrieved from an object instance; conversions can also be impacted</p> <p>That's why it's advisable to pass the type when it is available.</p>"},{"location":"de_serialization/#type-checking","title":"Type checking","text":"<p>Serialization can be configured using <code>check_type</code> (default to <code>False</code>) and <code>fall_back_on_any</code> (default to <code>False</code>) parameters. If <code>check_type</code> is <code>True</code>, the serialized object type will be checked to match the serialized type. If it doesn't, <code>fall_back_on_any</code> allows bypassing the serialized type to use <code>typing.Any</code> instead, i.e. to use the serialized object class.</p> <p>The default values of these parameters can be modified through <code>apischema.settings.serialization.check_type</code> and <code>apischema.settings.serialization.fall_back_on_any</code>.</p> <p>Note</p> <p>apischema relies on typing annotations, and assumes that the code is well statically type-checked. That's why it doesn't add the overhead of type checking by default (it's more than 10% performance impact).</p>"},{"location":"de_serialization/#serialized-methodsproperties","title":"Serialized methods/properties","text":"<p>apischema can execute methods/properties during serialization and add the computed values with the other fields values; just put <code>apischema.serialized</code> decorator on top of methods/properties you want to be serialized.</p> <p>The function name is used unless an alias is given in decorator argument.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import serialize, serialized\nfrom apischema.json_schema import serialization_schema\n\n\n@dataclass\nclass Foo:\n    @serialized\n    @property\n    def bar(self) -&gt; int:\n        return 0\n\n    # Serialized method can have default argument\n    @serialized\n    def baz(self, some_arg_with_default: int = 1) -&gt; int:\n        return some_arg_with_default\n\n    @serialized(\"aliased\")\n    @property\n    def with_alias(self) -&gt; int:\n        return 2\n\n\n# Serialized method can also be defined outside class,\n# but first parameter must be annotated\n@serialized\ndef function(foo: Foo) -&gt; int:\n    return 3\n\n\nassert serialize(Foo, Foo()) == {\"bar\": 0, \"baz\": 1, \"aliased\": 2, \"function\": 3}\nassert serialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"aliased\": {\"type\": \"integer\"},\n        \"bar\": {\"type\": \"integer\"},\n        \"baz\": {\"type\": \"integer\"},\n        \"function\": {\"type\": \"integer\"},\n    },\n    \"required\": [\"bar\", \"baz\", \"aliased\", \"function\"],\n    \"additionalProperties\": False,\n}\n</code></pre> <p>Note</p> <p>Serialized methods must not have parameters without default, as apischema needs to execute them without arguments</p> <p>Note</p> <p>Overriding of a serialized method in a subclass will also override the serialization of the subclass.</p>"},{"location":"de_serialization/#error-handling","title":"Error handling","text":"<p>Errors occurring in serialized methods can be caught in a dedicated error handler registered with <code>error_handler</code> parameter. This function takes in parameters the exception, the object and the alias of the serialized method; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization.</p> <p>The resulting serialization type will be a <code>Union</code> of the normal type and the error handling type; if the error handler always raises, use <code>typing.NoReturn</code> annotation.</p> <p><code>error_handler=None</code> correspond to a default handler which only return <code>None</code> \u2014 exception is thus discarded and serialization type becomes <code>Optional</code>.</p> <p>The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code.</p> <pre><code>from dataclasses import dataclass\nfrom logging import getLogger\nfrom typing import Any\n\nfrom apischema import serialize, serialized\nfrom apischema.json_schema import serialization_schema\n\nlogger = getLogger(__name__)\n\n\ndef log_error(error: Exception, obj: Any, alias: str) -&gt; None:\n    logger.error(\n        \"Serialization error in %s.%s\", type(obj).__name__, alias, exc_info=error\n    )\n    return None\n\n\n@dataclass\nclass Foo:\n    @serialized(error_handler=log_error)\n    def bar(self) -&gt; int:\n        raise RuntimeError(\"Some error\")\n\n\nassert serialize(Foo, Foo()) == {\"bar\": None}  # Logs \"Serialization error in Foo.bar\"\nassert serialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {\"type\": [\"integer\", \"null\"]}},\n    \"required\": [\"bar\"],\n    \"additionalProperties\": False,\n}\n</code></pre>"},{"location":"de_serialization/#non-required-serialized-methods","title":"Non-required serialized methods","text":"<p>Serialized methods (or their error handler) can return <code>apischema.Undefined</code>, in which case the property will not be included into the serialization; accordingly, the property loses the required qualification in the JSON schema.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import Undefined, UndefinedType, serialize, serialized\nfrom apischema.json_schema import serialization_schema\n\n\n@dataclass\nclass Foo:\n    @serialized\n    def bar(self) -&gt; int | UndefinedType:\n        return Undefined\n\n\nassert serialize(Foo, Foo()) == {}\nassert serialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {\"type\": \"integer\"}},\n    \"additionalProperties\": False,\n}\n</code></pre>"},{"location":"de_serialization/#generic-serialized-methods","title":"Generic serialized methods","text":"<p>Serialized methods of generic classes get the right type when their owning class is specialized.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Generic, TypeVar\n\nfrom apischema import serialized\nfrom apischema.json_schema import serialization_schema\n\nT = TypeVar(\"T\")\nU = TypeVar(\"U\")\n\n\n@dataclass\nclass Foo(Generic[T]):\n    @serialized\n    def bar(self) -&gt; T:\n        ...\n\n\n@serialized\ndef baz(foo: Foo[U]) -&gt; U:\n    ...\n\n\n@dataclass\nclass FooInt(Foo[int]):\n    ...\n\n\nassert (\n    serialization_schema(Foo[int])\n    == serialization_schema(FooInt)\n    == {\n        \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n        \"type\": \"object\",\n        \"properties\": {\"bar\": {\"type\": \"integer\"}, \"baz\": {\"type\": \"integer\"}},\n        \"required\": [\"bar\", \"baz\"],\n        \"additionalProperties\": False,\n    }\n)\n</code></pre>"},{"location":"de_serialization/#exclude-unset-fields","title":"Exclude unset fields","text":"<p>When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking, <code>serialize</code> can exclude unset fields using its <code>exclude_unset</code> parameter or <code>settings.serialization.exclude_unset</code> (default is <code>True</code>).</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import serialize\nfrom apischema.fields import with_fields_set\n\n\n# Decorator needed to benefit from the feature\n@with_fields_set\n@dataclass\nclass Foo:\n    bar: int\n    baz: str | None = None\n\n\nassert serialize(Foo, Foo(0)) == {\"bar\": 0}\nassert serialize(Foo, Foo(0), exclude_unset=False) == {\"bar\": 0, \"baz\": None}\n</code></pre> <p>Note</p> <p>As written in comment in the example, <code>with_fields_set</code> is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect.</p> <p>Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, a field will be marked as set even if its default value is used at initialization.</p> <pre><code>from dataclasses import dataclass, field\n\nfrom apischema import serialize\nfrom apischema.fields import with_fields_set\nfrom apischema.metadata import default_as_set\n\n\n# Decorator needed to benefit from the feature\n@with_fields_set\n@dataclass\nclass Foo:\n    bar: int | None = field(default=None, metadata=default_as_set)\n\n\nassert serialize(Foo, Foo()) == {\"bar\": None}\nassert serialize(Foo, Foo(0)) == {\"bar\": 0}\n</code></pre> <p>Note</p> <p>This metadata has effect only in combination with <code>with_fields_set</code> decorator.</p>"},{"location":"de_serialization/#exclude-fields-with-default-value-or-none","title":"Exclude fields with default value or <code>None</code>","text":"<p>Fields metadata <code>apischema.skip</code> already allows skipping fields serialization depending on a condition, for example if the field is <code>None</code> or equal to its default value. However, it must be added on each concerned fields, and that can be tedious when you want to set that behavior globally.</p> <p>That's why apischema provides the two following settings:</p> <ul> <li><code>settings.serialization.exclude_defaults</code>: whether fields which are equal to their default values should be excluded from serialization; default <code>False</code></li> <li><code>settings.serialization.exclude_none</code>: whether fields which are equal to <code>None</code> should be excluded from serialization; default <code>False</code></li> </ul> <p>These settings can also be set directly using <code>serialize</code> parameters, like in the following example:</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import serialize\n\n\n@dataclass\nclass Foo:\n    bar: int = 0\n    baz: str | None = None\n\n\nassert serialize(Foo, Foo(), exclude_defaults=True) == {}\nassert serialize(Foo, Foo(), exclude_none=True) == {\"bar\": 0}\n</code></pre>"},{"location":"de_serialization/#field-ordering","title":"Field ordering","text":"<p>Usually, JSON object properties are unordered, but sometimes, order does matter. By default, fields, are ordered according to their declaration; serialized methods are appended after the fields.</p> <p>However, it's possible to change the ordering using <code>apischema.order</code>.</p>"},{"location":"de_serialization/#class-level-ordering","title":"Class-level ordering","text":"<p><code>order</code> can be used to decorate a class with the field ordered as expected:</p> <pre><code>import json\nfrom dataclasses import dataclass\n\nfrom apischema import order, serialize\n\n\n@order([\"baz\", \"bar\", \"biz\"])\n@dataclass\nclass Foo:\n    bar: int\n    baz: int\n    biz: str\n\n\nassert json.dumps(serialize(Foo, Foo(0, 0, \"\"))) == '{\"baz\": 0, \"bar\": 0, \"biz\": \"\"}'\n</code></pre>"},{"location":"de_serialization/#field-level-ordering","title":"Field-level ordering","text":"<p>Each field has an order \"value\" (0 by default), and ordering is done by sorting fields using this value; if several fields have the same order value, they are sorted by their declaration order. For instance, assigning <code>-1</code> to a field will put it before every other fields, and <code>999</code> will surely put it at the end.</p> <p>This order value is set using <code>order</code>, this time as a field metadata (or passed to <code>order</code> argument of serialized methods/properties). It has the following overloaded signature:</p> <ul> <li><code>order(value: int, /)</code>: set the order value of the field</li> <li><code>order(*, after)</code>: ignore the order value and put the field after the given field/method/property</li> <li><code>order(*, before)</code>: ignore the order value and put the field before the given field/method/property</li> </ul> <p>Note</p> <p><code>after</code> and <code>before</code> can be raw strings, but also dataclass fields, methods or properties.</p> <p>Also, <code>order</code> can again be used as class decorator to override ordering metadata, by passing this time a mapping of field with their overridden order.</p> <pre><code>import json\nfrom dataclasses import dataclass, field\nfrom datetime import date\n\nfrom apischema import order, serialize, serialized\n\n\n@order({\"trigram\": order(-1)})\n@dataclass\nclass User:\n    firstname: str\n    lastname: str\n    address: str = field(metadata=order(after=\"birthdate\"))\n    birthdate: date = field()\n\n    @serialized\n    @property\n    def trigram(self) -&gt; str:\n        return (self.firstname[0] + self.lastname[0] + self.lastname[-1]).lower()\n\n    @serialized(order=order(before=birthdate))\n    @property\n    def age(self) -&gt; int:\n        age = date.today().year - self.birthdate.year\n        if age &gt; 0 and (date.today().month, date.today().day) &lt; (\n            self.birthdate.month,\n            self.birthdate.day,\n        ):\n            age -= 1\n        return age\n\n\nuser = User(\"Harry\", \"Potter\", \"London\", date(1980, 7, 31))\ndump = f\"\"\"{{\n    \"trigram\": \"hpr\",\n    \"firstname\": \"Harry\",\n    \"lastname\": \"Potter\",\n    \"age\": {user.age},\n    \"birthdate\": \"1980-07-31\",\n    \"address\": \"London\"\n}}\"\"\"\nassert json.dumps(serialize(User, user), indent=4) == dump\n</code></pre>"},{"location":"de_serialization/#typeddict-additional-properties","title":"TypedDict additional properties","text":"<p><code>TypedDict</code> can contain additional keys, which are not serialized by default. Setting <code>additional_properties</code> parameter to <code>True</code> (or <code>apischema.settings.additional_properties</code>) will toggle on their serialization (without aliasing).</p>"},{"location":"de_serialization/#faq","title":"FAQ","text":""},{"location":"de_serialization/#why-isnt-coercion-the-default-behavior","title":"Why isn't coercion the default behavior?","text":"<p>Because ill-formed data can be symptomatic of deeper issues, it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable.</p>"},{"location":"de_serialization/#why-isnt-with_fields_set-enabled-by-default","title":"Why isn't <code>with_fields_set</code> enabled by default?","text":"<p>It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)</p>"},{"location":"de_serialization/#why-isnt-serialization-type-checking-enabled-by-default","title":"Why isn't serialization type checking enabled by default?","text":"<p>Type checking has a runtime cost, which means poorer performance. Moreover, as explained in performances section, it prevents \"passthrough\" optimization. At last, code is supposed to be statically verified, and thus types already checked. (If some silly things are done and leads to have unsupported types passed to the JSON library, an error will be raised anyway).</p> <p>Runtime type checking is more a development feature, which could for example be with <code>apischema.settings.serialization.check_type = __debug__</code>.</p>"},{"location":"de_serialization/#why-not-use-json-library-default-fallback-parameter-for-serialization","title":"Why not use json library <code>default</code> fallback parameter for serialization?","text":"<p>Some apischema features like conversions can simply not be implemented with <code>default</code> fallback. By the way, apischema can perform surprisingly better than using <code>default</code>.</p> <p>However, <code>default</code> can be used in combination with passthrough optimization when needed to improve performance.  </p>"},{"location":"difference_with_pydantic/","title":"Difference with pydantic","text":"<p>As the question is often asked, it is answered in a dedicated section. Here are some the key differences between apischema and pydantic:</p>"},{"location":"difference_with_pydantic/#apischema-is-a-lot-faster","title":"apischema is (a lot) faster","text":"<p>According to benchmark, apischema is a lot faster than pydantic, especially for serialization. Both use Cython to optimize the code, but even without compilation (running only Python modules), apischema is still faster than Cythonized pydantic.</p> <p>Better performance, but not at the cost of fewer functionalities; that's rather the opposite: dynamic aliasing, conversions, flattened fields, etc.</p>"},{"location":"difference_with_pydantic/#apischema-can-generate-graphql-schema-from-your-resolvers","title":"apischema can generate GraphQL schema from your resolvers","text":"<p>Not just a simple printable schema but a complete <code>graphql.GraphQLSchema</code> (using graphql-core library) which can be used to execute your queries/mutations/subscriptions through your resolvers, powered by apischema (de)serialization and conversions features.</p> <p>Types and resolvers can be used both in traditional JSON-oriented API and GraphQL API.</p>"},{"location":"difference_with_pydantic/#apischema-uses-standard-dataclasses-and-types","title":"apischema uses standard dataclasses and types","text":"<p>pydantic uses its own <code>BaseModel</code> class, or its own pseudo-<code>dataclass</code>, so you are forced to tie all your code to the library, and you cannot easily reuse code written in a more standard way or in external libraries.</p> <p>By the way, Pydantic use expressions in typing annotations (<code>conint</code>, etc.), while it's not recommended and treated as an error by tools like Mypy</p>"},{"location":"difference_with_pydantic/#apischema-doesnt-require-external-plugins-for-editors-linters-etc","title":"apischema doesn't require external plugins for editors, linters, etc.","text":"<p>pydantic requires a plugin to allow Mypy to type check <code>BaseModel</code> and other pydantic singularities (and to not raise errors on it); plugins are also needed for editors.</p>"},{"location":"difference_with_pydantic/#apischema-doesnt-mix-up-deserialization-with-your-code","title":"apischema doesn't mix up (de)serialization with your code","text":"<p>While pydantic mixes up model constructor with deserializer, apischema uses dedicated functions for its features, meaning your dataclasses are instantiated normally with type checking. In your code, you manipulate objects; (de)serialization is for input/output.</p> <p>apischema also doesn't mix up validation of external data with your statically checked code; there is no runtime validation in constructors.</p>"},{"location":"difference_with_pydantic/#apischema-truly-works-out-of-the-box-with-forward-type-references-especially-for-recursive-model","title":"apischema truly works out-of-the-box with forward type references (especially for recursive model)","text":"<p>pydantic requires calling <code>update_forward_refs</code> method on recursive types, while apischema \"just works\".</p>"},{"location":"difference_with_pydantic/#apischema-supports-generic-without-requiring-additional-stuff","title":"apischema supports <code>Generic</code> without requiring additional stuff","text":"<p>pydantic <code>BaseModel</code> cannot be used with generic model, you have to use <code>GenericModel</code>.</p> <p>With apischema, you just write your generic classes normally. </p>"},{"location":"difference_with_pydantic/#apischema-conversions-feature-allows-to-support-any-type-defined-in-your-code-but-also-in-external-libraries","title":"apischema conversions feature allows to support any type defined in your code, but also in external libraries","text":"<p>pydantic doesn't make it easy to support external types, like <code>bson.ObjectId</code>; see this issue on the subject. You could dynamically add a <code>__get_validators__</code> method to foreign classes, but that doesn't work with builtin types like <code>collection.deque</code> and other types written in C.</p> <p>Serialization customization is harder, with definition of encoding function by model; it cannot be done at the same place as deserialization. There is also no correlation done between (de)serialization customization and model JSON schema; you could have to overwrite the generated schema if you don't want to get an inconsistency.</p> <p>apischema only requires a few lines of code to support any type you want, from <code>bson.ObjectId</code> to SQLAlchemy models by way of builtin and generic like <code>collection.deque</code>, and even pydantic. Conversions are also integrated in JSON schema this one is generated according to the source/target of the conversion</p> <p>Here is a comparison of a custom type support:</p> <pre><code>import re\nfrom typing import NamedTuple, NewType\n\nimport pydantic.validators\n\nimport apischema\n\n\n# Serialization can only be customized into the enclosing models\nclass RGB(NamedTuple):\n    red: int\n    green: int\n    blue: int\n\n    # If you don't put this method, RGB schema will be:\n    # {'title': 'Rgb', 'type': 'array', 'items': {}}\n    @classmethod\n    def __modify_schema__(cls, field_schema) -&gt; None:\n        field_schema.update({\"type\": \"string\", \"pattern\": r\"#[0-9A-Fa-f]{6}\"})\n        field_schema.pop(\"items\", ...)\n\n    @classmethod\n    def __get_validators__(cls):\n        yield pydantic.validators.str_validator\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value) -&gt; \"RGB\":\n        if (\n            not isinstance(value, str)\n            or re.fullmatch(r\"#[0-9A-Fa-f]{6}\", value) is None\n        ):\n            raise ValueError(\"Invalid RGB\")\n        return RGB(\n            red=int(value[1:3], 16), green=int(value[3:5], 16), blue=int(value[5:7], 16)\n        )\n\n\n# Simpler with apischema\n\n\nclass RGB(NamedTuple):\n    red: int\n    green: int\n    blue: int\n\n\n# NewType can be used to add schema to conversion source/target\n# but Annotated[str, apischema.schema(pattern=r\"#[0-9A-Fa-f]{6}\")] would have worked too\nHexaRGB = NewType(\"HexaRGB\", str)\n# pattern is used in JSON schema and in deserialization validation\napischema.schema(pattern=r\"#[0-9A-Fa-f]{6}\")(HexaRGB)\n\n\n@apischema.deserializer  # could be declared as a staticmethod of RGB class\ndef from_hexa(hexa: HexaRGB) -&gt; RGB:\n    return RGB(int(hexa[1:3], 16), int(hexa[3:5], 16), int(hexa[5:7], 16))\n\n\n@apischema.serializer  # could be declared as a method/property of RGB class\ndef to_hexa(rgb: RGB) -&gt; HexaRGB:\n    return HexaRGB(f\"#{rgb.red:02x}{rgb.green:02x}{rgb.blue:02x}\")\n\n\nassert (  # schema is inherited from deserialized type\n    apischema.json_schema.deserialization_schema(RGB)\n    == apischema.json_schema.deserialization_schema(HexaRGB)\n    == {\n        \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n        \"type\": \"string\",\n        \"pattern\": \"#[0-9A-Fa-f]{6}\",\n    }\n)\n</code></pre>"},{"location":"difference_with_pydantic/#apischema-can-also-customize-serialization-with-computed-fields","title":"apischema can also customize serialization with computed fields","text":"<p>Serialized methods/properties are regular methods/properties which are included in serialization effortlessly.</p>"},{"location":"difference_with_pydantic/#apischema-allows-you-to-use-composition-over-inheritance","title":"apischema allows you to use composition over inheritance","text":"<p>Flattened fields is a distinctive apischema feature that is very handy to build complex model from smaller fragments; you don't have to merge the fields of your fragments in a complex class with a lot of fields yourself, apischema deal with it for you, and your code is kept simple.</p>"},{"location":"difference_with_pydantic/#apischema-has-a-functional-approach-pydantic-has-an-object-one","title":"apischema has a functional approach, pydantic has an object one","text":"<p>pydantic features are based on <code>BaseModel</code> methods. You have to have a <code>BaseModel</code> instance to do anything, even if you manipulate only an integer. Complex pydantic stuff like <code>__root__</code> model or deserialization customization come from this approach.</p> <p>apischema is functional, it doesn't use method but simple functions, which works with all types. You can also register conversions for any types similarly you would implement a type class in a functional language. And your class namespace don't mix up with a mandatory base class' one.</p>"},{"location":"difference_with_pydantic/#apischema-can-use-both-camelcase-and-snake_case-with-the-same-types","title":"apischema can use both camelCase and snake_case with the same types","text":"<p>While pydantic field aliases are fixed at model creation, apischema lets you choose which aliasing you want at (de)serialization time. </p> <p>It can be convenient if you need to juggle with cases for the same models between frontends and other backend services for example.</p>"},{"location":"difference_with_pydantic/#apischema-doesnt-coerce-by-default","title":"apischema doesn't coerce by default","text":"<p>Your API respects its schema. </p> <p>It can also coerce, for example to parse configuration file, and coercion can be adjusted (for example coercing list from comma-separated string). </p>"},{"location":"difference_with_pydantic/#apischema-has-a-better-integration-of-json-schemaopenapi","title":"apischema has a better integration of JSON schema/OpenAPI","text":"<p>With pydantic, if you want to have a <code>nullable</code> field in the generated schema, you have to put <code>nullable</code> into schema extra keywords.</p> <p>apischema is bound to the last JSON schema version but offers conversion to other version like OpenAPI 3.0 and <code>nullable</code> is added for <code>Optional</code> types.</p> <p>apischema also supports more advanced features like <code>dependentRequired</code> or <code>unevaluatedProperties</code>. Reference handling is also more flexible</p>"},{"location":"difference_with_pydantic/#apischema-can-add-validators-and-json-schema-to-newtype","title":"apischema can add validators and JSON schema to <code>NewType</code>","text":"<p>So it will be used in deserialization validation. You can use <code>NewType</code> everywhere, to gain a better type checking, self-documented code.</p>"},{"location":"difference_with_pydantic/#apischema-validators-are-regular-methods-with-automatic-dependencies-management","title":"apischema validators are regular methods with automatic dependencies management","text":"<p>Using regular methods allows benefiting of type checking of fields, where pydantic validators use dynamic stuff (name of the fields as strings) and are not type-checked or have to get redundant type annotations.</p> <p>apischema validators also have automatic dependency management. And apischema directly supports JSON schema property dependencies.</p> <p>Comparison is simple with an example (validator is taken from pydantic documentation:</p> <pre><code>from dataclasses import dataclass\n\nimport pydantic\n\nimport apischema\n\n\nclass UserModel(pydantic.BaseModel):\n    username: str\n    password1: str\n    password2: str\n\n    @pydantic.root_validator\n    def check_passwords_match(cls, values):\n        # This is a classmethod (it needs a plugin to not raise a warning in your IDE)\n        # What is the type of of values? of values['password1']?\n        # If you rename password1 field, validator will hardly be updated\n        # You also have to test yourself that values are provided\n        pw1, pw2 = values.get(\"password1\"), values.get(\"password2\")\n        if pw1 is not None and pw2 is not None and pw1 != pw2:\n            raise ValueError(\"passwords do not match\")\n        return values\n\n\n@dataclass\nclass LoginForm:\n    username: str\n    password1: str\n    password2: str\n\n    @apischema.validator\n    def check_password_match(self):\n        # Typed checked, simpler, and not executed if password1 or password2\n        # are missing/invalid\n        if self.password1 != self.password2:\n            raise ValueError(\"passwords do not match\")\n</code></pre>"},{"location":"difference_with_pydantic/#apischema-supports-pydantic","title":"apischema supports pydantic","text":"<p>It's not a feature, is just the result of 30 lines of code.</p>"},{"location":"json_schema/","title":"JSON schema","text":""},{"location":"json_schema/#json-schema-generation","title":"JSON schema generation","text":"<p>JSON schema can be generated from data model. However, because of all possible customizations, the schema can differ between deserilialization and serialization. In common cases, <code>deserialization_schema</code> and <code>serialization_schema</code> will give the same result.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema.json_schema import deserialization_schema, serialization_schema\n\n\n@dataclass\nclass Foo:\n    bar: str\n\n\nassert deserialization_schema(Foo) == serialization_schema(Foo)\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"additionalProperties\": False,\n    \"properties\": {\"bar\": {\"type\": \"string\"}},\n    \"required\": [\"bar\"],\n    \"type\": \"object\",\n}\n</code></pre>"},{"location":"json_schema/#field-alias","title":"Field alias","text":"<p>Sometimes dataclass field names can clash with a language keyword, sometimes the property name is not convenient. Hopefully, field can define an <code>alias</code> which will be used in schema and  deserialization/serialization.</p> <pre><code>from dataclasses import dataclass, field\n\nfrom apischema import alias, deserialize, serialize\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Foo:\n    class_: str = field(metadata=alias(\"class\"))\n\n\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"additionalProperties\": False,\n    \"properties\": {\"class\": {\"type\": \"string\"}},\n    \"required\": [\"class\"],\n    \"type\": \"object\",\n}\nassert deserialize(Foo, {\"class\": \"bar\"}) == Foo(\"bar\")\nassert serialize(Foo, Foo(\"bar\")) == {\"class\": \"bar\"}\n</code></pre>"},{"location":"json_schema/#alias-all-fields","title":"Alias all fields","text":"<p>Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if <code>override=False</code> is specified.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\nfrom apischema import alias\nfrom apischema.json_schema import deserialization_schema\n\n\n@alias(lambda s: f\"foo_{s}\")\n@dataclass\nclass Foo:\n    field1: Any\n    field2: Any = field(metadata=alias(override=False))\n    field3: Any = field(metadata=alias(\"field03\"))\n    field4: Any = field(metadata=alias(\"field04\", override=False))\n\n\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"additionalProperties\": False,\n    \"properties\": {\"foo_field1\": {}, \"field2\": {}, \"foo_field03\": {}, \"field04\": {}},\n    \"required\": [\"foo_field1\", \"field2\", \"foo_field03\", \"field04\"],\n    \"type\": \"object\",\n}\n</code></pre> <p>Class-level aliasing can be used to define a camelCase API.</p>"},{"location":"json_schema/#dynamic-aliasing-and-default-aliaser","title":"Dynamic aliasing and default aliaser","text":"<p>apischema operations <code>deserialize</code>/<code>serialize</code>/<code>deserialization_schema</code>/<code>serialization_schema</code> provide an <code>aliaser</code> parameter which will be applied on every fields being processed in this operation.</p> <p>Similar to <code>strictness configuration</code>, this parameter has a default value controlled by <code>apischema.settings.aliaser</code>.</p> <p>It can be used for example to make all an application use camelCase. Actually, there is a shortcut for that:</p> <p>Otherwise, it's used the same way than <code>settings.coercer</code>.</p> <pre><code>from apischema import settings\n\nsettings.camel_case = True\n</code></pre> <p>Note</p> <p>Dynamic aliaser ignores <code>override=False</code></p>"},{"location":"json_schema/#schema-annotations","title":"Schema annotations","text":"<p>Type annotations are not enough to express a complete schema, but apischema has a function for that; <code>schema</code> can be used both as type decorator or field metadata.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import NewType\n\nfrom apischema import schema\nfrom apischema.json_schema import deserialization_schema\n\nTag = NewType(\"Tag\", str)\nschema(min_len=3, pattern=r\"^\\w*$\", examples=[\"available\", \"EMEA\"])(Tag)\n\n\n@dataclass\nclass Resource:\n    id: int\n    tags: list[Tag] = field(\n        default_factory=list,\n        metadata=schema(\n            description=\"regroup multiple resources\", max_items=3, unique=True\n        ),\n    )\n\n\nassert deserialization_schema(Resource) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"additionalProperties\": False,\n    \"properties\": {\n        \"id\": {\"type\": \"integer\"},\n        \"tags\": {\n            \"description\": \"regroup multiple resources\",\n            \"items\": {\n                \"examples\": [\"available\", \"EMEA\"],\n                \"minLength\": 3,\n                \"pattern\": \"^\\\\w*$\",\n                \"type\": \"string\",\n            },\n            \"maxItems\": 3,\n            \"type\": \"array\",\n            \"uniqueItems\": True,\n            \"default\": [],\n        },\n    },\n    \"required\": [\"id\"],\n    \"type\": \"object\",\n}\n</code></pre> <p>Note</p> <p>Schema are particularly useful with <code>NewType</code>. For example, if you use prefixed ids, you can use a <code>NewType</code> with a <code>pattern</code> schema to validate them, and benefit of more precise type checking.</p> <p>The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case):</p> Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum <code>int</code> max maximum <code>int</code> exc_min exclusiveMinimum <code>int</code> exc_max exclusiveMaximum <code>int</code> mult_of multipleOf <code>int</code> format / <code>str</code> media_type contentMediaType <code>str</code> encoding contentEncoding <code>str</code> min_len minLength <code>str</code> max_len maxLength <code>str</code> pattern / <code>str</code> min_items minItems <code>list</code> max_items maxItems <code>list</code> unique / <code>list</code> min_props minProperties <code>dict</code> max_props maxProperties <code>dict</code> <p>Note</p> <p>In case of field schema, field default value will be serialized (if possible) to add <code>default</code> keyword to the schema.</p>"},{"location":"json_schema/#constraints-validation","title":"Constraints validation","text":"<p>JSON schema constrains the data deserialized; these constraints are naturally used for validation.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import NewType\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, schema\n\nTag = NewType(\"Tag\", str)\nschema(min_len=3, pattern=r\"^\\w*$\", examples=[\"available\", \"EMEA\"])(Tag)\n\n\n@dataclass\nclass Resource:\n    id: int\n    tags: list[Tag] = field(\n        default_factory=list,\n        metadata=schema(\n            description=\"regroup multiple resources\", max_items=3, unique=True\n        ),\n    )\n\n\nwith pytest.raises(ValidationError) as err:  # pytest check exception is raised\n    deserialize(\n        Resource, {\"id\": 42, \"tags\": [\"tag\", \"duplicate\", \"duplicate\", \"bad&amp;\", \"_\"]}\n    )\nassert err.value.errors == [\n    {\"loc\": [\"tags\"], \"err\": \"item count greater than 3 (maxItems)\"},\n    {\"loc\": [\"tags\"], \"err\": \"duplicate items (uniqueItems)\"},\n    {\"loc\": [\"tags\", 3], \"err\": \"not matching pattern ^\\\\w*$ (pattern)\"},\n    {\"loc\": [\"tags\", 4], \"err\": \"string length lower than 3 (minLength)\"},\n]\n</code></pre> <p>Note</p> <p>Error message are fully customizable</p>"},{"location":"json_schema/#extra-schema","title":"Extra schema","text":"<p><code>schema</code> has two other arguments: <code>extra</code> and <code>override</code>, which give a finer control of the JSON schema generated: <code>extra</code> and <code>override</code>. It can be used for example to build \"strict\" unions (using <code>oneOf</code> instead of <code>anyOf</code>)</p> <pre><code>from dataclasses import dataclass\nfrom typing import Annotated, Any\n\nfrom apischema import schema\nfrom apischema.json_schema import deserialization_schema\n\n\n# schema extra can be callable to modify the schema in place\ndef to_one_of(schema: dict[str, Any]):\n    if \"anyOf\" in schema:\n        schema[\"oneOf\"] = schema.pop(\"anyOf\")\n\n\nOneOf = schema(extra=to_one_of)\n\n\n# or extra can be a dictionary which will update the schema\n@schema(\n    extra={\"$ref\": \"http://some-domain.org/path/to/schema.json#/$defs/Foo\"},\n    override=True,  # override apischema generated schema, using only extra\n)\n@dataclass\nclass Foo:\n    bar: int\n\n\n# Use Annotated with OneOf to make a \"strict\" Union\nassert deserialization_schema(Annotated[Foo | int, OneOf]) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"oneOf\": [  # oneOf instead of anyOf\n        {\"$ref\": \"http://some-domain.org/path/to/schema.json#/$defs/Foo\"},\n        {\"type\": \"integer\"},\n    ],\n}\n</code></pre>"},{"location":"json_schema/#base-schema","title":"Base <code>schema</code>","text":"<p><code>apischema.settings.base_schema</code> can be used to define \"base schema\" for the different kind of objects: types, object fields or (serialized) methods.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any, Callable, get_origin\n\nimport docstring_parser\n\nfrom apischema import schema, serialized, settings\nfrom apischema.json_schema import serialization_schema\nfrom apischema.schemas import Schema\nfrom apischema.type_names import get_type_name\n\n\n@dataclass\nclass Foo:\n    \"\"\"Foo class\n\n    :var bar: bar attribute\"\"\"\n\n    bar: str = field(metadata=schema(max_len=10))\n\n    @serialized\n    @property\n    def baz(self) -&gt; int:\n        \"\"\"baz method\"\"\"\n        ...\n\n\ndef type_base_schema(tp: Any) -&gt; Schema | None:\n    if not hasattr(tp, \"__doc__\"):\n        return None\n    return schema(\n        title=get_type_name(tp).json_schema,\n        description=docstring_parser.parse(tp.__doc__).short_description,\n    )\n\n\ndef field_base_schema(tp: Any, name: str, alias: str) -&gt; Schema | None:\n    title = alias.replace(\"_\", \" \").capitalize()\n    tp = get_origin(tp) or tp  # tp can be generic\n    for meta in docstring_parser.parse(tp.__doc__).meta:\n        if meta.args == [\"var\", name]:\n            return schema(title=title, description=meta.description)\n    return schema(title=title)\n\n\ndef method_base_schema(tp: Any, method: Callable, alias: str) -&gt; Schema | None:\n    return schema(\n        title=alias.replace(\"_\", \" \").capitalize(),\n        description=docstring_parser.parse(method.__doc__).short_description,\n    )\n\n\nsettings.base_schema.type = type_base_schema\nsettings.base_schema.field = field_base_schema\nsettings.base_schema.method = method_base_schema\n\nassert serialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"additionalProperties\": False,\n    \"title\": \"Foo\",\n    \"description\": \"Foo class\",\n    \"properties\": {\n        \"bar\": {\n            \"description\": \"bar attribute\",\n            \"title\": \"Bar\",\n            \"type\": \"string\",\n            \"maxLength\": 10,\n        },\n        \"baz\": {\"description\": \"baz method\", \"title\": \"Baz\", \"type\": \"integer\"},\n    },\n    \"required\": [\"bar\", \"baz\"],\n    \"type\": \"object\",\n}\n</code></pre> <p>Base schema will be merged with <code>schema</code> defined at type/field/method level.</p>"},{"location":"json_schema/#required-field-with-default-value","title":"Required field with default value","text":"<p>By default, a dataclass/namedtuple field will be tagged <code>required</code> if it doesn't have a default value.</p> <p>However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with <code>\"jsonrpc\": \"2.0\"</code>. That's done with field metadata <code>required</code>.</p> <pre><code>from dataclasses import dataclass, field\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize\nfrom apischema.metadata import required\n\n\n@dataclass\nclass Foo:\n    bar: int | None = field(default=None, metadata=required)\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(Foo, {})\nassert err.value.errors == [{\"loc\": [\"bar\"], \"err\": \"missing property\"}]\n</code></pre>"},{"location":"json_schema/#additional-properties-pattern-properties","title":"Additional properties / pattern properties","text":""},{"location":"json_schema/#with-mapping","title":"With <code>Mapping</code>","text":"<p>Schema of a <code>Mapping</code>/<code>dict</code> type is naturally translated to <code>\"additionalProperties\": &lt;schema of the value type&gt;</code>.</p> <p>However when the schema of the key has a <code>pattern</code>, it will give a <code>\"patternProperties\": {&lt;key pattern&gt;: &lt;schema of the value type&gt;}</code> </p>"},{"location":"json_schema/#with-dataclass","title":"With dataclass","text":"<p><code>additionalProperties</code>/<code>patternProperties</code> can be added to dataclasses by using  fields annotated with <code>properties</code> metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a <code>Mapping</code> type, or be deserializable from a <code>Mapping</code>, because they are instantiated with a mapping.</p> <pre><code>from collections.abc import Mapping\nfrom dataclasses import dataclass, field\nfrom typing import Annotated\n\nfrom apischema import deserialize, properties, schema\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Config:\n    active: bool = True\n    server_options: Mapping[str, bool] = field(\n        default_factory=dict, metadata=properties(pattern=r\"^server_\")\n    )\n    client_options: Mapping[\n        Annotated[str, schema(pattern=r\"^client_\")], bool  # noqa: F722\n    ] = field(default_factory=dict, metadata=properties(...))\n    options: Mapping[str, bool] = field(default_factory=dict, metadata=properties)\n\n\nassert deserialize(\n    Config,\n    {\"use_lightsaber\": True, \"server_auto_restart\": False, \"client_timeout\": False},\n) == Config(\n    True,\n    {\"server_auto_restart\": False},\n    {\"client_timeout\": False},\n    {\"use_lightsaber\": True},\n)\nassert deserialization_schema(Config) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"active\": {\"type\": \"boolean\", \"default\": True}},\n    \"additionalProperties\": {\"type\": \"boolean\"},\n    \"patternProperties\": {\n        \"^server_\": {\"type\": \"boolean\"},\n        \"^client_\": {\"type\": \"boolean\"},\n    },\n}\n</code></pre> <p>Note</p> <p>Of course, a dataclass can only have a single <code>properties</code> field without pattern, because it makes no sens to have several <code>additionalProperties</code>.</p>"},{"location":"json_schema/#property-dependencies","title":"Property dependencies","text":"<p>apischema supports property dependencies for dataclass through a class member. Dependencies are also used in validation.</p> <pre><code>from dataclasses import dataclass, field\n\nimport pytest\n\nfrom apischema import (\n    Undefined,\n    UndefinedType,\n    ValidationError,\n    dependent_required,\n    deserialize,\n)\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Billing:\n    name: str\n    # Fields used in dependencies MUST be declared with `field`\n    credit_card: int | UndefinedType = field(default=Undefined)\n    billing_address: str | UndefinedType = field(default=Undefined)\n\n    dependencies = dependent_required({credit_card: [billing_address]})\n\n\n# it can also be done outside the class with\n# dependent_required({\"credit_card\": [\"billing_address\"]}, owner=Billing)\n\n\nassert deserialization_schema(Billing) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"additionalProperties\": False,\n    \"dependentRequired\": {\"credit_card\": [\"billing_address\"]},\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"credit_card\": {\"type\": \"integer\"},\n        \"billing_address\": {\"type\": \"string\"},\n    },\n    \"required\": [\"name\"],\n    \"type\": \"object\",\n}\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(Billing, {\"name\": \"Anonymous\", \"credit_card\": 1234_5678_9012_3456})\nassert err.value.errors == [\n    {\n        \"loc\": [\"billing_address\"],\n        \"err\": \"missing property (required by ['credit_card'])\",\n    }\n]\n</code></pre> <p>Because bidirectional dependencies are a common idiom, apischema provides a shortcut notation; it's indeed possible to write <code>dependent_required([credit_card, billing_adress])</code>.</p>"},{"location":"json_schema/#json-schema-reference","title":"JSON schema reference","text":"<p>For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them \u2014 it's even mandatory for recursive types; JSON schema use JSON pointers \"$ref\" to refer to the definitions. apischema handles this feature natively.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional\n\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Node:\n    value: int\n    child: Optional[\"Node\"] = None\n\n\nassert deserialization_schema(Node) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$ref\": \"#/$defs/Node\",\n    \"$defs\": {\n        \"Node\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"value\": {\"type\": \"integer\"},\n                \"child\": {\n                    \"anyOf\": [{\"$ref\": \"#/$defs/Node\"}, {\"type\": \"null\"}],\n                    \"default\": None,\n                },\n            },\n            \"required\": [\"value\"],\n            \"additionalProperties\": False,\n        }\n    },\n}\n</code></pre>"},{"location":"json_schema/#use-reference-only-for-reused-types","title":"Use reference only for reused types","text":"<p>apischema can control the reference use through the boolean <code>all_ref</code> parameter of <code>deserialization_schema</code>/<code>serialization_schema</code>: </p> <ul> <li><code>all_refs=True</code> -&gt; all types with a reference will be put in the definitions and referenced with <code>$ref</code>;</li> <li><code>all_refs=False</code> -&gt; only types which are reused in the schema are put in definitions</li> </ul> <p><code>all_refs</code> default value depends on the JSON schema version: it's <code>False</code> for JSON schema drafts but <code>True</code> for OpenAPI.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Bar:\n    baz: str\n\n\n@dataclass\nclass Foo:\n    bar1: Bar\n    bar2: Bar\n\n\nassert deserialization_schema(Foo, all_refs=False) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$defs\": {\n        \"Bar\": {\n            \"additionalProperties\": False,\n            \"properties\": {\"baz\": {\"type\": \"string\"}},\n            \"required\": [\"baz\"],\n            \"type\": \"object\",\n        }\n    },\n    \"additionalProperties\": False,\n    \"properties\": {\"bar1\": {\"$ref\": \"#/$defs/Bar\"}, \"bar2\": {\"$ref\": \"#/$defs/Bar\"}},\n    \"required\": [\"bar1\", \"bar2\"],\n    \"type\": \"object\",\n}\nassert deserialization_schema(Foo, all_refs=True) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$defs\": {\n        \"Bar\": {\n            \"additionalProperties\": False,\n            \"properties\": {\"baz\": {\"type\": \"string\"}},\n            \"required\": [\"baz\"],\n            \"type\": \"object\",\n        },\n        \"Foo\": {\n            \"additionalProperties\": False,\n            \"properties\": {\n                \"bar1\": {\"$ref\": \"#/$defs/Bar\"},\n                \"bar2\": {\"$ref\": \"#/$defs/Bar\"},\n            },\n            \"required\": [\"bar1\", \"bar2\"],\n            \"type\": \"object\",\n        },\n    },\n    \"$ref\": \"#/$defs/Foo\",\n}\n</code></pre>"},{"location":"json_schema/#set-reference-name","title":"Set reference name","text":"<p>In the previous examples, types were referenced using their name. This is indeed the default behavior for every classes/<code>NewType</code>s (except primitive <code>int</code>/<code>str</code>/<code>bool</code>/<code>float</code>).</p> <p>It's possible to override the default reference name using <code>apischema.type_name</code>; passing <code>None</code> instead of a string will remove the reference, making the type unable to be referenced as a separate definition in the schema.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Annotated\n\nfrom apischema import type_name\nfrom apischema.json_schema import deserialization_schema\n\n\n# Type name can be added as a decorator\n@type_name(\"Resource\")\n@dataclass\nclass BaseResource:\n    id: int\n    # or using typing.Annotated\n    tags: Annotated[set[str], type_name(\"ResourceTags\")]\n\n\nassert deserialization_schema(BaseResource, all_refs=True) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$defs\": {\n        \"Resource\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"id\": {\"type\": \"integer\"},\n                \"tags\": {\"$ref\": \"#/$defs/ResourceTags\"},\n            },\n            \"required\": [\"id\", \"tags\"],\n            \"additionalProperties\": False,\n        },\n        \"ResourceTags\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"},\n            \"uniqueItems\": True,\n        },\n    },\n    \"$ref\": \"#/$defs/Resource\",\n}\n</code></pre> <p>Note</p> <p>Builtin collections are interchangeable when a type_name is registered. For example, if a name is registered for <code>list[Foo]</code>, this name will also be used for <code>Sequence[Foo]</code> or <code>Collection[Foo]</code>.</p> <p>Generic aliases can have a type name, but they need to be specialized; <code>Foo[T, int]</code> cannot have a type name but <code>Foo[str, int]</code> can. However, generic classes can get a dynamic type name depending on their generic argument, passing a name factory to <code>type_name</code>:</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Generic, TypeVar\n\nfrom apischema import type_name\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.metadata import flatten\n\nT = TypeVar(\"T\")\n\n\n# Type name factory takes the type and its arguments as (positional) parameters\n@type_name(lambda tp, arg: f\"{arg.__name__}Resource\")\n@dataclass\nclass Resource(Generic[T]):\n    id: int\n    content: T = field(metadata=flatten)\n    ...\n\n\n@dataclass\nclass Foo:\n    bar: str\n\n\nassert deserialization_schema(Resource[Foo], all_refs=True) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$ref\": \"#/$defs/FooResource\",\n    \"$defs\": {\n        \"FooResource\": {\n            \"allOf\": [\n                {\n                    \"type\": \"object\",\n                    \"properties\": {\"id\": {\"type\": \"integer\"}},\n                    \"required\": [\"id\"],\n                    \"additionalProperties\": False,\n                },\n                {\"$ref\": \"#/$defs/Foo\"},\n            ],\n            \"unevaluatedProperties\": False,\n        },\n        \"Foo\": {\n            \"type\": \"object\",\n            \"properties\": {\"bar\": {\"type\": \"string\"}},\n            \"required\": [\"bar\"],\n            \"additionalProperties\": False,\n        },\n    },\n}\n</code></pre> <p>The default behavior can also be customized using <code>apischema.settings.default_type_name</code>:</p>"},{"location":"json_schema/#reference-factory","title":"Reference factory","text":"<p>In JSON schema, <code>$ref</code> looks like <code>#/$defs/Foo</code>, not just <code>Foo</code>. In fact, schema generation use the ref given by <code>type_name</code>/<code>default_type_name</code> and pass it to a <code>ref_factory</code> function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default <code>ref_factory</code>, for draft 2020-12, it prefixes the ref with <code>#/$defs/</code>, while it prefixes with <code>#/components/schema</code> in case of OpenAPI.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Foo:\n    bar: int\n\n\ndef ref_factory(ref: str) -&gt; str:\n    return f\"http://some-domain.org/path/to/{ref}.json#\"\n\n\nassert deserialization_schema(Foo, all_refs=True, ref_factory=ref_factory) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$ref\": \"http://some-domain.org/path/to/Foo.json#\",\n}\n</code></pre> <p>Note</p> <p>When <code>ref_factory</code> is passed in arguments, definitions are not added to the generated schema. That's because <code>ref_factory</code> would surely change definitions location, so there would be no interest to add them with a wrong location. These definitions can of course be generated separately with <code>definitions_schema</code>.</p>"},{"location":"json_schema/#definitions-schema","title":"Definitions schema","text":"<p>Definitions schemas can also be extracted using <code>apischema.json_schema.definitions_schema</code>. It takes two lists <code>deserialization</code>/<code>serialization</code> of types (or tuple of type + dynamic conversion) and returns a dictionary of all referenced schemas.</p> <p>Note</p> <p>This is especially useful when it comes to OpenAPI schema to generate the components section.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema.json_schema import definitions_schema\n\n\n@dataclass\nclass Bar:\n    baz: int = 0\n\n\n@dataclass\nclass Foo:\n    bar: Bar\n\n\nassert definitions_schema(deserialization=[list[Foo]], all_refs=True) == {\n    \"Foo\": {\n        \"type\": \"object\",\n        \"properties\": {\"bar\": {\"$ref\": \"#/$defs/Bar\"}},\n        \"required\": [\"bar\"],\n        \"additionalProperties\": False,\n    },\n    \"Bar\": {\n        \"type\": \"object\",\n        \"properties\": {\"baz\": {\"type\": \"integer\", \"default\": 0}},\n        \"additionalProperties\": False,\n    },\n}\n</code></pre>"},{"location":"json_schema/#json-schema-openapi-version","title":"JSON schema / OpenAPI version","text":"<p>JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If apischema natively use the last one: draft 2020-12, it is possible to specify a schema version which will be used for the generation.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Literal\n\nfrom apischema.json_schema import (\n    JsonSchemaVersion,\n    definitions_schema,\n    deserialization_schema,\n)\n\n\n@dataclass\nclass Bar:\n    baz: int | None\n    constant: Literal[0] = 0\n\n\n@dataclass\nclass Foo:\n    bar: Bar\n\n\nassert deserialization_schema(Foo, all_refs=True) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"$ref\": \"#/$defs/Foo\",\n    \"$defs\": {\n        \"Foo\": {\n            \"type\": \"object\",\n            \"properties\": {\"bar\": {\"$ref\": \"#/$defs/Bar\"}},\n            \"required\": [\"bar\"],\n            \"additionalProperties\": False,\n        },\n        \"Bar\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"baz\": {\"type\": [\"integer\", \"null\"]},\n                \"constant\": {\"type\": \"integer\", \"const\": 0, \"default\": 0},\n            },\n            \"required\": [\"baz\"],\n            \"additionalProperties\": False,\n        },\n    },\n}\nassert deserialization_schema(\n    Foo, all_refs=True, version=JsonSchemaVersion.DRAFT_7\n) == {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    # $ref is isolated in allOf + draft 7 prefix\n    \"allOf\": [{\"$ref\": \"#/definitions/Foo\"}],\n    \"definitions\": {  # not \"$defs\"\n        \"Foo\": {\n            \"type\": \"object\",\n            \"properties\": {\"bar\": {\"$ref\": \"#/definitions/Bar\"}},\n            \"required\": [\"bar\"],\n            \"additionalProperties\": False,\n        },\n        \"Bar\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"baz\": {\"type\": [\"integer\", \"null\"]},\n                \"constant\": {\"type\": \"integer\", \"const\": 0, \"default\": 0},\n            },\n            \"required\": [\"baz\"],\n            \"additionalProperties\": False,\n        },\n    },\n}\nassert deserialization_schema(Foo, version=JsonSchemaVersion.OPEN_API_3_1) == {\n    # No definitions for OpenAPI, use definitions_schema for it\n    \"$ref\": \"#/components/schemas/Foo\"  # OpenAPI prefix\n}\nassert definitions_schema(\n    deserialization=[Foo], version=JsonSchemaVersion.OPEN_API_3_1\n) == {\n    \"Foo\": {\n        \"type\": \"object\",\n        \"properties\": {\"bar\": {\"$ref\": \"#/components/schemas/Bar\"}},\n        \"required\": [\"bar\"],\n        \"additionalProperties\": False,\n    },\n    \"Bar\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"baz\": {\"type\": [\"integer\", \"null\"]},\n            \"constant\": {\"type\": \"integer\", \"const\": 0, \"default\": 0},\n        },\n        \"required\": [\"baz\"],\n        \"additionalProperties\": False,\n    },\n}\nassert definitions_schema(\n    deserialization=[Foo], version=JsonSchemaVersion.OPEN_API_3_0\n) == {\n    \"Foo\": {\n        \"type\": \"object\",\n        \"properties\": {\"bar\": {\"$ref\": \"#/components/schemas/Bar\"}},\n        \"required\": [\"bar\"],\n        \"additionalProperties\": False,\n    },\n    \"Bar\": {\n        \"type\": \"object\",\n        # \"nullable\" instead of \"type\": \"null\"\n        \"properties\": {\n            \"baz\": {\"type\": \"integer\", \"nullable\": True},\n            \"constant\": {\"type\": \"integer\", \"enum\": [0], \"default\": 0},\n        },\n        \"required\": [\"baz\"],\n        \"additionalProperties\": False,\n    },\n}\n</code></pre>"},{"location":"json_schema/#openapi-discriminator","title":"OpenAPI Discriminator","text":"<p>OpenAPI defines a discriminator object which can be used to shortcut deserialization of union of object types.</p> <p>apischema provides two different ways to declare a discriminator:</p> <ul> <li> <p>as an <code>Annotated</code> metadata of a union ; <pre><code>from dataclasses import dataclass\nfrom typing import Annotated\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, discriminator, serialize\nfrom apischema.json_schema import deserialization_schema\n\n\n@dataclass\nclass Cat:\n    pass\n\n\n@dataclass\nclass Dog:\n    pass\n\n\n@dataclass\nclass Lizard:\n    pass\n\n\nPet = Annotated[Cat | Dog | Lizard, discriminator(\"type\", {\"dog\": Dog})]\n\nassert deserialize(Pet, {\"type\": \"dog\"}) == Dog()\nassert deserialize(Pet, {\"type\": \"Cat\"}) == Cat()\nassert serialize(Pet, Dog()) == {\"type\": \"dog\"}\nwith pytest.raises(ValidationError) as err:\n    assert deserialize(Pet, {\"type\": \"not a pet\"})\nassert err.value.errors == [\n    {\"loc\": [\"type\"], \"err\": \"not one of ['dog', 'Cat', 'Lizard'] (oneOf)\"}\n]\n\nassert deserialization_schema(Pet) == {\n    \"oneOf\": [\n        {\"$ref\": \"#/$defs/Cat\"},\n        {\"$ref\": \"#/$defs/Dog\"},\n        {\"$ref\": \"#/$defs/Lizard\"},\n    ],\n    \"discriminator\": {\"propertyName\": \"type\", \"mapping\": {\"dog\": \"#/$defs/Dog\"}},\n    \"$defs\": {\n        \"Dog\": {\"type\": \"object\", \"additionalProperties\": False},\n        \"Cat\": {\"type\": \"object\", \"additionalProperties\": False},\n        \"Lizard\": {\"type\": \"object\", \"additionalProperties\": False},\n    },\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n}\n</code></pre></p> </li> <li> <p>as a decorator of base class. <pre><code>from dataclasses import dataclass\n\nfrom apischema import deserialize, discriminator, serialize\nfrom apischema.json_schema import deserialization_schema\n\n\n@discriminator(\"type\")\nclass Pet:\n    pass\n\n\n@dataclass\nclass Cat(Pet):\n    pass\n\n\n@dataclass\nclass Dog(Pet):\n    pass\n\n\ndata = {\"type\": \"Dog\"}\nassert deserialize(Pet, data) == deserialize(Cat | Dog, data) == Dog()\nassert serialize(Pet, Dog()), serialize(Cat | Dog, Dog()) == data\n\nassert (\n    deserialization_schema(Pet)\n    == deserialization_schema(Cat | Dog)\n    == {\n        \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n        \"oneOf\": [{\"$ref\": \"#/$defs/Cat\"}, {\"$ref\": \"#/$defs/Dog\"}],\n        \"$defs\": {\n            \"Pet\": {\n                \"type\": \"object\",\n                \"required\": [\"type\"],\n                \"properties\": {\"type\": {\"type\": \"string\"}},\n                \"discriminator\": {\"propertyName\": \"type\"},\n            },\n            \"Cat\": {\"allOf\": [{\"$ref\": \"#/$defs/Pet\"}, {\"type\": \"object\"}]},\n            \"Dog\": {\"allOf\": [{\"$ref\": \"#/$defs/Pet\"}, {\"type\": \"object\"}]},\n        },\n    }\n)\n</code></pre></p> </li> </ul> <p>Note</p> <p>Using discriminator doesn't require to have a dedicated field (except for <code>TypedDict</code>)</p> <p>Performance of union deserialization can be improved using discriminator.</p>"},{"location":"json_schema/#readonly-writeonly","title":"<code>readOnly</code> / <code>writeOnly</code>","text":"<p>Dataclasses <code>InitVar</code> and <code>field(init=False)</code> fields will be flagged respectively with <code>\"writeOnly\": true</code> and <code>\"readOnly\": true</code> in the generated schema.</p> <p>In definitions schema, if a type appears both in deserialization and serialization, properties are merged and the resulting schema contains then <code>readOnly</code> and <code>writeOnly</code> properties. By the way, the <code>required</code> is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization <code>required</code> is kept because it's more important as it can be used in validation (OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported)</p>"},{"location":"optimizations_and_benchmark/","title":"Optimizations and benchmark","text":"<p>apischema is (a lot) faster than its known alternatives, thanks to advanced optimizations.    </p> <p> </p> <p>Note</p> <p>Chart is truncated to a relative performance of 20x slower. Benchmark results are detailed in the results table.</p>"},{"location":"optimizations_and_benchmark/#precomputed-deserialization-methods","title":"Precomputed (de)serialization methods","text":"<p>apischema precomputes (de)serialization methods depending on the (de)serialized type (and other parameters); type annotations processing is done in the precomputation. Methods are then cached using <code>functools.lru_cache</code>, so <code>deserialize</code> and <code>serialize</code> don't recompute them every time.</p> <p>Note</p> <p>The cache is automatically reset when global settings are modified, because it impacts the generated methods.</p> <p>However, if <code>lru_cache</code> is fast, using the methods directly is faster, so apischema provides <code>apischema.deserialization_method</code> and <code>apischema.serialization_method</code>. These functions share the same parameters than <code>deserialize</code>/<code>serialize</code>, except the data/object parameter to (de)serialize. Using the computed methods directly can increase performances by 10%.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema import deserialization_method, serialization_method\n\n\n@dataclass\nclass Foo:\n    bar: int\n\n\ndeserialize_foo = deserialization_method(Foo)\nserialize_foo = serialization_method(Foo)\n\nassert deserialize_foo({\"bar\": 0}) == Foo(0)\nassert serialize_foo(Foo(0)) == {\"bar\": 0}\n</code></pre> <p>Warning</p> <p>Methods computed before settings modification will not be updated and use the old settings. Be careful to set your settings first.</p>"},{"location":"optimizations_and_benchmark/#avoid-unnecessary-copies","title":"Avoid unnecessary copies","text":"<p>As an example, when a list of integers is deserialized, <code>json.load</code> already return a list of integers. The loaded data can thus be \"reused\", and the deserialization just become a validation step. The same principle applies to serialization.</p> <p>It's controlled by the settings <code>apischema.settings.deserialization.no_copy</code>/<code>apischema.settings.serialization.no_copy</code>, or <code>no_copy</code> parameter of <code>deserialize</code>/<code>serialize</code> methods. Default behavior is to avoid these unnecessary copies, i.e. <code>no_copy=False</code>.</p> <pre><code>from timeit import timeit\n\nfrom apischema import deserialize\n\nints = list(range(100))\n\nassert deserialize(list[int], ints, no_copy=True) is ints  # default\nassert deserialize(list[int], ints, no_copy=False) is not ints\n\nprint(timeit(\"deserialize(list[int], ints, no_copy=True)\", globals=globals()))\n# 8.596703557006549\nprint(timeit(\"deserialize(list[int], ints, no_copy=False)\", globals=globals()))\n# 9.365363762015477\n</code></pre>"},{"location":"optimizations_and_benchmark/#serialization-passthrough","title":"Serialization passthrough","text":"<p>JSON serialization libraries expect primitive data types (<code>dict</code>/<code>list</code>/<code>str</code>/etc.). A non-negligible part of objects to be serialized are primitive.</p> <p>When type checking is disabled (this is default), objects annotated with primitive types doesn't need to be transformed or checked; apischema can simply \"pass through\" them, and it will result into an identity serialization method, just returning its argument.</p> <p>Container types like <code>list</code> or <code>dict</code> are passed through only when the contained types are passed through too (and when <code>no_copy=True</code>)</p> <pre><code>from apischema import serialize\n\nints = list(range(5))\nassert serialize(list[int], ints) is ints\n</code></pre> <p>Note</p> <p><code>Enum</code> subclasses which also inherit <code>str</code>/<code>int</code> are also passed through</p>"},{"location":"optimizations_and_benchmark/#passthrough-options","title":"Passthrough options","text":"<p>Some JSON serialization libraries natively support types like <code>UUID</code> or <code>datetime</code>, sometimes with a faster implementation than the apischema one \u2014 orjson, written in Rust, is a good example.</p> <p>To take advantage of that, apischema provides <code>apischema.PassThroughOptions</code> class to specify which type should be passed through, whether they are supported natively by JSON libraries (or handled in a <code>default</code> fallback). </p> <p><code>apischema.serialization_default</code> can be used as <code>default</code> fallback in combination to <code>PassThroughOptions</code>. It has to be instantiated with the same kwargs parameters (<code>aliaser</code>, etc.) than <code>serialization_method</code>.</p> <pre><code>from collections.abc import Collection\nfrom uuid import UUID, uuid4\n\nfrom apischema import PassThroughOptions, serialization_method\n\nuuids_method = serialization_method(\n    Collection[UUID], pass_through=PassThroughOptions(collections=True, types={UUID})\n)\nuuids = [uuid4() for _ in range(5)]\nassert uuids_method(uuids) is uuids\n</code></pre> <p>Important</p> <p>Passthrough optimization is a lot diminished with <code>check_type=False</code>.</p> <p><code>PassThroughOptions</code> has the following parameters:</p>"},{"location":"optimizations_and_benchmark/#any-pass-through-any","title":"<code>any</code> \u2014 pass through <code>Any</code>","text":""},{"location":"optimizations_and_benchmark/#collections-pass-through-collections","title":"<code>collections</code> \u2014 pass through collections","text":"<p>Standard collections <code>list</code>, <code>tuple</code> and <code>dict</code> are natively handled by JSON libraries, but <code>set</code>, for example, isn't. Moreover, standard abstract collections like <code>Collection</code> or <code>Mapping</code>, which are used a lot, are not guaranteed to have their runtime type supported (having a <code>set</code> annotated with <code>Collection</code> for instance). </p> <p>But, most of the time, collections runtime types are <code>list</code>/<code>dict</code>, so others can be handled in <code>default</code> fallback.</p> <p>Note</p> <p>Set-like type will not be passed through.</p>"},{"location":"optimizations_and_benchmark/#dataclasses-pass-through-dataclasses","title":"<code>dataclasses</code> - pass through dataclasses","text":"<p>Some JSON libraries, like orjson, support dataclasses natively.  However, because apischema has a lot of specific features (aliasing, flatten fields, conditional skipping, fields ordering, etc.), only dataclasses with none of these features, and only passed through fields, will be passed through too.</p>"},{"location":"optimizations_and_benchmark/#enums-pass-through-enumenum-subclasses","title":"<code>enums</code> \u2014 pass through <code>enum.Enum</code> subclasses","text":""},{"location":"optimizations_and_benchmark/#tuple-pass-through-tuple","title":"<code>tuple</code> \u2014 pass through <code>tuple</code>","text":"<p>Even if <code>tuple</code> is often supported by JSON serializers, if this options is not enabled, tuples will be serialized as lists. It also allows easier test writing for example.</p> <p>Note</p> <p><code>collections=True</code> implies <code>tuple=True</code>;</p>"},{"location":"optimizations_and_benchmark/#types-pass-through-arbitrary-types","title":"<code>types</code> \u2014 pass through arbitrary types","text":"<p>Either a collection of types, or a predicate to determine if type has to be passed through.</p>"},{"location":"optimizations_and_benchmark/#binary-compilation-using-cython","title":"Binary compilation using Cython","text":"<p>apischema use Cython in order to compile critical parts of the code, i.e. the (de)serialization methods.</p> <p>However, apischema remains a pure Python library \u2014 it can work without binary modules. Cython source files (<code>.pyx</code>) are in fact generated from Python modules. It allows notably keeping the code simple, by adding switch-case optimization to replace dynamic dispatch, avoiding big chains of <code>elif</code> in Python code.</p> <p>Note</p> <p>Compilation is disabled when using PyPy, because it's even faster with the bare Python code. That's another interest of generating <code>.pyx</code> files: keeping Python source for PyPy.</p>"},{"location":"optimizations_and_benchmark/#override-dataclass-constructors","title":"Override dataclass constructors","text":"<p>Warning</p> <p>This feature is still experimental and disabled by default. Test carefully its impact on your code before enable it in production.</p> <p>Dataclass constructors calls is the slowest part of the deserialization, about 50% of its runtime! They are indeed pure Python functions and cannot be compiled.</p> <p>In case of \"normal\" dataclass (no <code>__slots__</code>, <code>__post_init__</code>, or <code>__init__</code>/<code>__new__</code>/<code>__setattr__</code> overriding), apischema can override the constructor with a compilable code. </p> <p>This feature can be toggled on/off globally using <code>apischema.settings.deserialization.override_dataclass_constructors</code></p>"},{"location":"optimizations_and_benchmark/#discriminator","title":"Discriminator","text":"<p>OpenAPI discriminator allows making union deserialization time more homogeneous.</p> <pre><code>from dataclasses import dataclass\nfrom timeit import timeit\nfrom typing import Annotated\n\nfrom apischema import deserialization_method, discriminator\n\n\n@dataclass\nclass Cat:\n    love_dog: bool = False\n\n\n@dataclass\nclass Dog:\n    love_cat: bool = False\n\n\ndeserialize_union = deserialization_method(Cat | Dog)\ndeserialize_discriminated = deserialization_method(\n    Annotated[Cat | Dog, discriminator(\"type\")]\n)\n##### Without discrimininator\nprint(timeit('deserialize_union({\"love_dog\": False})', globals=globals()))\n# Cat: 0.760085788\nprint(timeit('deserialize_union({\"love_cat\": False})', globals=globals()))\n# Dog: 3.078876515 \u2248 x4\n##### With discriminator\nprint(timeit('deserialize_discriminated({\"type\": \"Cat\"})', globals=globals()))\n# Cat: 1.244204702\nprint(timeit('deserialize_discriminated({\"type\": \"Dog\"})', globals=globals()))\n# Dog: 1.234058598 \u2248 same\n</code></pre> <p>Note</p> <p>As you can notice in the example, discriminator brings its own additional cost, but it's completely worth it. </p>"},{"location":"optimizations_and_benchmark/#benchmark","title":"Benchmark","text":"<p>Benchmark code is located benchmark directory or apischema repository. Performances are measured on two datasets: a simple, a more complex one.</p> <p>Benchmark is run by Github Actions workflow on <code>ubuntu-latest</code> with Python 3.10.</p> <p>Results are given relatively to the fastest library, i.e. apischema; simple and complex results are detailed in the table, displayed result is the mean of both.</p>"},{"location":"optimizations_and_benchmark/#relative-execution-time-lower-is-better","title":"Relative execution time (lower is better)","text":"library version deserialization serialization pydantic 2.4.2 / / <p>Note</p> <p>Benchmark use binary optimization, but even running as a pure Python library, apischema still performs better than almost all of the competitors.</p>"},{"location":"validation/","title":"Validation","text":"<p>Validation is an important part of deserialization. By default, apischema validates types of data according to typing annotations, and <code>schema</code> constraints. But custom validators can also be add for a more precise validation.</p>"},{"location":"validation/#deserialization-and-validation-error","title":"Deserialization and validation error","text":"<p><code>ValidationError</code> is raised when validation fails. This exception contains all the information about the ill-formed part of the data. It can be formatted/serialized using its <code>errors</code> property. </p> <pre><code>from dataclasses import dataclass, field\nfrom typing import NewType\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, schema\n\nTag = NewType(\"Tag\", str)\nschema(min_len=3, pattern=r\"^\\w*$\", examples=[\"available\", \"EMEA\"])(Tag)\n\n\n@dataclass\nclass Resource:\n    id: int\n    tags: list[Tag] = field(\n        default_factory=list,\n        metadata=schema(\n            description=\"regroup multiple resources\", max_items=3, unique=True\n        ),\n    )\n\n\nwith pytest.raises(ValidationError) as err:  # pytest check exception is raised\n    deserialize(\n        Resource, {\"id\": 42, \"tags\": [\"tag\", \"duplicate\", \"duplicate\", \"bad&amp;\", \"_\"]}\n    )\nassert err.value.errors == [\n    {\"loc\": [\"tags\"], \"err\": \"item count greater than 3 (maxItems)\"},\n    {\"loc\": [\"tags\"], \"err\": \"duplicate items (uniqueItems)\"},\n    {\"loc\": [\"tags\", 3], \"err\": \"not matching pattern ^\\\\w*$ (pattern)\"},\n    {\"loc\": [\"tags\", 4], \"err\": \"string length lower than 3 (minLength)\"},\n]\n</code></pre> <p>As shown in the example, apischema will not stop at the first error met but tries to validate all parts of the data.</p> <p>Note</p> <p><code>ValidationError</code> can also be serialized using <code>apischema.serialize</code> (this will use <code>errors</code> internally).</p>"},{"location":"validation/#constraint-errors-customization","title":"Constraint errors customization","text":"<p>Constraints are validated at deserialization, with apischema providing default error messages. Messages can be customized by setting the corresponding attribute of <code>apischema.settings.errors</code>. They can be either a string which will be formatted with the constraint value (using <code>str.format</code>), e.g. <code>less than {} (minimum)</code>, or a function with 2 parameters: the constraint value and the invalid data. </p> <pre><code>import pytest\n\nfrom apischema import ValidationError, deserialize, schema, settings\n\nsettings.errors.max_items = (\n    lambda constraint, data: f\"too-many-items: {len(data)} &gt; {constraint}\"\n)\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(list[int], [0, 1, 2, 3], schema=schema(max_items=3))\nassert err.value.errors == [{\"loc\": [], \"err\": \"too-many-items: 4 &gt; 3\"}]\n</code></pre> <p>Note</p> <p>Default error messages doesn't include the invalid data for security reason (data could for example be a password too short).</p> <p>Note</p> <p>Other error message can be customized, for example <code>missing property</code> for missing required properties, etc.</p>"},{"location":"validation/#dataclass-validators","title":"Dataclass validators","text":"<p>Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized.</p> <p>Note</p> <p>Previously to v0.17, validators could raise arbitrary exceptions (except AssertionError of course); see FAQ for the reason of this change.</p> <pre><code>from dataclasses import dataclass\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, validator\n\n\n@dataclass\nclass PasswordForm:\n    password: str\n    confirmation: str\n\n    @validator\n    def password_match(self):\n        # DO NOT use assert\n        if self.password != self.confirmation:\n            raise ValidationError(\"password doesn't match its confirmation\")\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(PasswordForm, {\"password\": \"p455w0rd\", \"confirmation\": \"...\"})\nassert err.value.errors == [\n    {\"loc\": [], \"err\": \"password doesn't match its confirmation\"}\n]\n</code></pre> <p>Warning</p> <p>DO NOT use <code>assert</code> statement to validate external data, ever. In fact, this statement is made to be disabled when executed in optimized mode (see documentation), so validation would be disabled too. This warning doesn't concern only apischema; <code>assert</code> is only for internal assertion in debug/development environment. That's why apischema will not catch <code>AssertionError</code> as a validation error but reraises it, making <code>deserialize</code> fail. </p> <p>Note</p> <p>Validators are always executed in order of declaration.</p>"},{"location":"validation/#automatic-dependency-management","title":"Automatic dependency management","text":"<p>It makes no sense to execute a validator using a field that is ill-formed. Hopefully, apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok.</p> <pre><code>from dataclasses import dataclass\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, validator\n\n\n@dataclass\nclass PasswordForm:\n    password: str\n    confirmation: str\n\n    @validator\n    def password_match(self):\n        if self.password != self.confirmation:\n            raise ValueError(\"password doesn't match its confirmation\")\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(PasswordForm, {\"password\": \"p455w0rd\"})\nassert err.value.errors == [\n    # validator is not executed because confirmation is missing\n    {\"loc\": [\"confirmation\"], \"err\": \"missing property\"}\n]\n</code></pre> <p>Note</p> <p>Despite the fact that validator uses the <code>self</code> argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field.</p>"},{"location":"validation/#raise-more-than-one-error-with-yield","title":"Raise more than one error with <code>yield</code>","text":"<p>Validation of a list field can require raising several exceptions, one for each bad element. With <code>raise</code>, this is not possible, because you can raise only once.</p> <p>However, apischema provides a way of raising as many errors as needed by using <code>yield</code>. Moreover, with this syntax, it is possible to add a \"path\" (see below) to the error to precise its location in the validated data. This path will be added to the <code>loc</code> key of the error.</p> <pre><code>from dataclasses import dataclass\nfrom ipaddress import IPv4Address, IPv4Network\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, validator\nfrom apischema.objects import get_alias\n\n\n@dataclass\nclass SubnetIps:\n    subnet: IPv4Network\n    ips: list[IPv4Address]\n\n    @validator\n    def check_ips_in_subnet(self):\n        for index, ip in enumerate(self.ips):\n            if ip not in self.subnet:\n                # yield &lt;error path&gt;, &lt;error message&gt;\n                yield (get_alias(self).ips, index), \"ip not in subnet\"\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(\n        SubnetIps,\n        {\"subnet\": \"126.42.18.0/24\", \"ips\": [\"126.42.18.1\", \"126.42.19.0\", \"0.0.0.0\"]},\n    )\nassert err.value.errors == [\n    {\"loc\": [\"ips\", 1], \"err\": \"ip not in subnet\"},\n    {\"loc\": [\"ips\", 2], \"err\": \"ip not in subnet\"},\n]\n</code></pre>"},{"location":"validation/#error-path","title":"Error path","text":"<p>In the example, the validator yields a tuple of an \"error path\" and the error message. Error path can be:</p> <ul> <li>a field alias (obtained with <code>apischema.objects.get_alias</code>);</li> <li>an integer, for list indices;</li> <li>a raw string, for dict key (or field);</li> <li>an <code>apischema.objects.AliasedStr</code>, a string subclass which will be aliased by deserialization aliaser;</li> <li>an iterable, e.g. a tuple, of this 4 components.</li> </ul> <p><code>yield</code> can also be used with only an error message.</p> <p>Note</p> <p>For dataclass field error path, it's advised to use <code>apischema.objects.get_alias</code> instead of raw string, because it will take into account potential aliasing and it will be better handled by IDE (refactoring, cross-referencing, etc.)</p>"},{"location":"validation/#discard","title":"Discard","text":"<p>If one of your validators fails because a field is corrupted, maybe you don't want subsequent validators to be executed. <code>validator</code> decorator provides a <code>discard</code> parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed.</p> <pre><code>from dataclasses import dataclass, field\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, validator\nfrom apischema.objects import get_alias, get_field\n\n\n@dataclass\nclass BoundedValues:\n    # field must be assign to be used, even with empty `field()`\n    bounds: tuple[int, int] = field()\n    values: list[int]\n\n    # validator(\"bounds\") would also work, but it's not handled by IDE refactoring, etc.\n    @validator(discard=bounds)\n    def bounds_are_sorted(self):\n        min_bound, max_bound = self.bounds\n        if min_bound &gt; max_bound:\n            yield get_alias(self).bounds, \"bounds are not sorted\"\n\n    @validator\n    def values_dont_exceed_bounds(self):\n        min_bound, max_bound = self.bounds\n        for index, value in enumerate(self.values):\n            if not min_bound &lt;= value &lt;= max_bound:\n                yield (get_alias(self).values, index), \"value exceeds bounds\"\n\n\n# Outside class, fields can still be accessed in a \"static\" way, to avoid use raw string\n@validator(discard=get_field(BoundedValues).bounds)\ndef bounds_are_sorted_equivalent(bounded: BoundedValues):\n    min_bound, max_bound = bounded.bounds\n    if min_bound &gt; max_bound:\n        yield get_alias(bounded).bounds, \"bounds are not sorted\"\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(BoundedValues, {\"bounds\": [10, 0], \"values\": [-1, 2, 4]})\nassert err.value.errors == [\n    {\"loc\": [\"bounds\"], \"err\": \"bounds are not sorted\"}\n    # Without discard, there would have been an other error\n    # {\"loc\": [\"values\", 1], \"err\": \"value exceeds bounds\"}\n]\n</code></pre> <p>You can notice in this example that apischema tries to avoid using raw strings to identify fields. In every function of the library using fields identifier (<code>apischema.validator</code>, <code>apischema.dependent_required</code>, <code>apischema.fields.set_fields</code>, etc.), you have always three ways to pass them: - using field object, preferred in dataclass definition; - using <code>apischema.objects.get_field</code>, to be used outside of class definition; it works with <code>NamedTuple</code> too \u2014 the object returned is the apischema internal field representation, common to <code>dataclass</code>, <code>NamedTuple</code> and <code>TypedDict</code>; - using raw strings, thus not handled by static tools like refactoring, but it works;</p>"},{"location":"validation/#field-validators","title":"Field validators","text":""},{"location":"validation/#at-field-level","title":"At field level","text":"<p>Fields are validated according to their types and schema. But it's also possible to add validators to fields.</p> <pre><code>from dataclasses import dataclass, field\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize\nfrom apischema.metadata import validators\n\n\ndef check_no_duplicate_digits(n: int):\n    if len(str(n)) != len(set(str(n))):\n        raise ValidationError(\"number has duplicate digits\")\n\n\n@dataclass\nclass Foo:\n    bar: str = field(metadata=validators(check_no_duplicate_digits))\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(Foo, {\"bar\": \"11\"})\nassert err.value.errors == [{\"loc\": [\"bar\"], \"err\": \"number has duplicate digits\"}]\n</code></pre> <p>When validation fails for a field, it is discarded and cannot be used in class validators, as is the case when field schema validation fails.</p> <p>Note</p> <p><code>field_validator</code> allows reusing the the same validator for several fields. However in this case, using a custom type (for example a <code>NewType</code>) with validators (see next section) could often be a better solution.</p>"},{"location":"validation/#using-other-fields","title":"Using other fields","text":"<p>A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However, there is a shortcut for this use case:</p> <pre><code>from dataclasses import dataclass, field\nfrom enum import Enum\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, validator\nfrom apischema.objects import get_alias, get_field\n\n\nclass Parity(Enum):\n    EVEN = \"even\"\n    ODD = \"odd\"\n\n\n@dataclass\nclass NumberWithParity:\n    parity: Parity\n    number: int = field()\n\n    @validator(number)\n    def check_parity(self):\n        if (self.parity == Parity.EVEN) != (self.number % 2 == 0):\n            yield \"number doesn't respect parity\"\n\n    # A field validator is equivalent to a discard argument and all error paths prefixed\n    # with the field alias\n    @validator(discard=number)\n    def check_parity_equivalent(self):\n        if (self.parity == Parity.EVEN) != (self.number % 2 == 0):\n            yield get_alias(self).number, \"number doesn't respect parity\"\n\n\n@validator(get_field(NumberWithParity).number)\ndef check_parity_other_equivalent(number2: NumberWithParity):\n    if (number2.parity == Parity.EVEN) != (number2.number % 2 == 0):\n        yield \"number doesn't respect parity\"\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(NumberWithParity, {\"parity\": \"even\", \"number\": 1})\nassert err.value.errors == [{\"loc\": [\"number\"], \"err\": \"number doesn't respect parity\"}]\n</code></pre>"},{"location":"validation/#validators-inheritance","title":"Validators inheritance","text":"<p>Validators are inherited just like other class fields.</p> <pre><code>from dataclasses import dataclass\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, validator\n\n\n@dataclass\nclass PasswordForm:\n    password: str\n    confirmation: str\n\n    @validator\n    def password_match(self):\n        if self.password != self.confirmation:\n            raise ValidationError(\"password doesn't match its confirmation\")\n\n\n@dataclass\nclass CompleteForm(PasswordForm):\n    username: str\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(\n        CompleteForm,\n        {\"username\": \"wyfo\", \"password\": \"p455w0rd\", \"confirmation\": \"...\"},\n    )\nassert err.value.errors == [\n    {\"loc\": [], \"err\": \"password doesn't match its confirmation\"}\n]\n</code></pre>"},{"location":"validation/#validator-with-initvar","title":"Validator with <code>InitVar</code>","text":"<p>Dataclasses <code>InitVar</code> are accessible in validators by using parameters the same way <code>__post_init__</code> does. Only the needed fields have to be put in parameters, they are then added to validator dependencies.</p> <pre><code>from dataclasses import InitVar, dataclass, field\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, validator\nfrom apischema.metadata import init_var\n\n\n@dataclass\nclass Foo:\n    bar: InitVar[int] = field(metadata=init_var(int))\n\n    @validator(bar)\n    def validate(self, bar: int):\n        if bar &lt; 0:\n            yield \"negative\"\n\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(Foo, {\"bar\": -1})\nassert err.value.errors == [{\"loc\": [\"bar\"], \"err\": \"negative\"}]\n</code></pre>"},{"location":"validation/#validators-are-not-run-on-default-values","title":"Validators are not run on default values","text":"<p>If all validator dependencies are initialized with their default values, they are not run.</p> <pre><code>from dataclasses import dataclass, field\n\nfrom apischema import deserialize, validator\n\nvalidator_run = False\n\n\n@dataclass\nclass Foo:\n    bar: int = field(default=0)\n\n    @validator(bar)\n    def password_match(self):\n        global validator_run\n        validator_run = True\n        if self.bar &lt; 0:\n            raise ValueError(\"negative\")\n\n\ndeserialize(Foo, {})\nassert not validator_run\n</code></pre>"},{"location":"validation/#validators-for-every-type","title":"Validators for every type","text":"<p>Validators can also be declared as regular functions, in which case annotation of the first param is used to associate it to the validated type (you can also use the <code>owner</code> parameter); this allows adding a validator to every type.</p> <p>Last but not least, validators can be embedded directly into <code>Annotated</code> arguments using <code>validators</code> metadata.</p> <pre><code>from typing import Annotated, NewType\n\nimport pytest\n\nfrom apischema import ValidationError, deserialize, validator\nfrom apischema.metadata import validators\n\nPalindrome = NewType(\"Palindrome\", str)\n\n\n@validator  # could also use @validator(owner=Palindrome)\ndef check_palindrome(s: Palindrome):\n    for i in range(len(s) // 2):\n        if s[i] != s[-1 - i]:\n            raise ValidationError(\"Not a palindrome\")\n\n\nassert deserialize(Palindrome, \"tacocat\") == \"tacocat\"\nwith pytest.raises(ValidationError) as err:\n    deserialize(Palindrome, \"palindrome\")\nassert err.value.errors == [{\"loc\": [], \"err\": \"Not a palindrome\"}]\n\n# Using Annotated\nwith pytest.raises(ValidationError) as err:\n    deserialize(Annotated[str, validators(check_palindrome)], \"palindrom\")\nassert err.value.errors == [{\"loc\": [], \"err\": \"Not a palindrome\"}]\n</code></pre>"},{"location":"validation/#faq","title":"FAQ","text":""},{"location":"validation/#how-are-validator-dependencies-computed","title":"How are validator dependencies computed?","text":"<p><code>ast.NodeVisitor</code> and the Python black magic begins...</p>"},{"location":"validation/#why-only-validate-at-deserialization-and-not-at-instantiation","title":"Why only validate at deserialization and not at instantiation?","text":"<p>apischema uses type annotations, so every objects used can already be statically type-checked (with Mypy/Pycharm/etc.) at instantiation but also at modification.</p>"},{"location":"validation/#why-use-validators-for-dataclasses-instead-of-doing-validation-in-__post_init__","title":"Why use validators for dataclasses instead of doing validation in <code>__post_init__</code>?","text":"<p>Actually, validation can completely be done in <code>__post_init__</code>, there is no problem with that. However, validators offers one thing that cannot be achieved with <code>__post_init__</code>: they are run before <code>__init__</code>, so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation.</p>"},{"location":"validation/#why-validators-cannot-raise-arbitrary-exception","title":"Why validators cannot raise arbitrary exception?","text":"<p>Allowing arbitrary exception is in fact a security issue, because unwanted exception could be raised, and their message displayed in validation error. It could either contain sensitive data, or give information about the implementation which could be used to hack it.</p> <p>By the way, it's possible to define a decorator to convert precise exceptions to <code>ValidationError</code>: <pre><code>from collections.abc import Callable\nfrom functools import wraps\nfrom typing import TypeVar\nfrom apischema import ValidationError\n\nFunc = TypeVar(\"Func\", bound=Callable)\ndef catch(*exceptions) -&gt; Callable[[Func], Func]:\n    def decorator(func: Func) -&gt; Func:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as err:\n                raise ValidationError(str(err)) if isinstance(err, exceptions) else err\n        return wrapper\n    return decorator\n</code></pre></p>"},{"location":"examples/attrs_support/","title":"Attrs support","text":"<pre><code>from typing import Sequence\n\nimport attrs\n\nfrom apischema import deserialize, serialize, settings\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.objects import ObjectField\n\nprev_default_object_fields = settings.default_object_fields\n\n\ndef attrs_fields(cls: type) -&gt; Sequence[ObjectField] | None:\n    if hasattr(cls, \"__attrs_attrs__\"):\n        return [\n            ObjectField(\n                a.name, a.type, required=a.default == attrs.NOTHING, default=a.default\n            )\n            for a in getattr(cls, \"__attrs_attrs__\")\n        ]\n    else:\n        return prev_default_object_fields(cls)\n\n\nsettings.default_object_fields = attrs_fields\n\n\n@attrs.define\nclass Foo:\n    bar: int\n\n\nassert deserialize(Foo, {\"bar\": 0}) == Foo(0)\nassert serialize(Foo, Foo(0)) == {\"bar\": 0}\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {\"type\": \"integer\"}},\n    \"required\": [\"bar\"],\n    \"additionalProperties\": False,\n}\n</code></pre>"},{"location":"examples/inherited_deserializer/","title":"Inherited deserializer","text":"<pre><code>from collections.abc import Iterator\nfrom dataclasses import dataclass\nfrom typing import TypeVar\n\nfrom apischema import deserialize, deserializer\nfrom apischema.conversions import Conversion\n\nFoo_ = TypeVar(\"Foo_\", bound=\"Foo\")\n\n\n# Use a dataclass in order to be easily testable with ==\n@dataclass\nclass Foo:\n    value: int\n\n    @classmethod\n    def deserialize(cls: type[Foo_], value: int) -&gt; Foo_:\n        return cls(value)\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        # Register subclasses' conversion in __init_subclass__\n        deserializer(Conversion(cls.deserialize, target=cls))\n\n\n# Register main conversion after the class definition\ndeserializer(Conversion(Foo.deserialize, target=Foo))\n\n\nclass Bar(Foo):\n    pass\n\n\nassert deserialize(Foo, 0) == Foo(0)\nassert deserialize(Bar, 0) == Bar(0)\n\n\n# For external types (defines in imported library)\n\n\n@dataclass\nclass ForeignType:\n    value: int\n\n\nclass ForeignSubtype(ForeignType):\n    pass\n\n\nT = TypeVar(\"T\")\n\n\n# Recursive implementation of type.__subclasses__\ndef rec_subclasses(cls: type[T]) -&gt; Iterator[type[T]]:\n    for sub_cls in cls.__subclasses__():\n        yield sub_cls\n        yield from rec_subclasses(sub_cls)\n\n\n# Register deserializers for all subclasses\nfor cls in (ForeignType, *rec_subclasses(ForeignType)):\n    # cls=cls is an lambda idiom to capture variable by value inside loop\n    deserializer(Conversion(lambda value, cls=cls: cls(value), source=int, target=cls))\n\nassert deserialize(ForeignType, 0) == ForeignType(0)\nassert deserialize(ForeignSubtype, 0) == ForeignSubtype(0)\n</code></pre>"},{"location":"examples/pydantic_support/","title":"Pydantic support","text":"<p>It takes only 30 lines of code to support <code>pydantic.BaseModel</code> and all of its subclasses. You could add these lines to your project using pydantic and start to benefit from apischema features.</p> <p>This example deliberately doesn't use <code>set_object_fields</code> but instead the conversions feature in order to roughly include pydantic \"as is\": it will reuse pydantic coercion, error messages, JSON schema, etc. This makes a full retro-compatible support.</p> <p>As a result, lot of apischema features like GraphQL schema generation or <code>NewType</code> validation cannot be supported using this method \u2014 but they could be by using <code>set_object_fields</code> instead. </p> <pre><code>import inspect\nfrom collections.abc import Mapping\nfrom typing import Any\n\nimport pydantic\nimport pytest\n\nfrom apischema import (\n    ValidationError,\n    deserialize,\n    schema,\n    serialize,\n    serializer,\n    settings,\n)\nfrom apischema.conversions import AnyConversion, Conversion\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.schemas import Schema\n\n# ---------- Pydantic support code starts here ----------\nprev_deserialization = settings.deserialization.default_conversion\n\n\ndef default_deserialization(tp: Any) -&gt; AnyConversion | None:\n    if inspect.isclass(tp) and issubclass(tp, pydantic.BaseModel):\n\n        def deserialize_pydantic(data):\n            try:\n                return tp.parse_obj(data)\n            except pydantic.ValidationError as error:\n                raise ValidationError.from_errors(\n                    [{\"loc\": err[\"loc\"], \"err\": err[\"msg\"]} for err in error.errors()]\n                )\n\n        return Conversion(\n            deserialize_pydantic,\n            source=tp.__annotations__.get(\"__root__\", Mapping[str, Any]),\n            target=tp,\n        )\n    else:\n        return prev_deserialization(tp)\n\n\nsettings.deserialization.default_conversion = default_deserialization\n\nprev_schema = settings.base_schema.type\n\n\ndef default_schema(tp: Any) -&gt; Schema | None:\n    if inspect.isclass(tp) and issubclass(tp, pydantic.BaseModel):\n        return schema(extra=tp.schema(), override=True)\n    else:\n        return prev_schema(tp)\n\n\nsettings.base_schema.type = default_schema\n\n\n# No need to use settings.serialization because serializer is inherited\n@serializer\ndef serialize_pydantic(obj: pydantic.BaseModel) -&gt; Any:\n    # There is currently no way to retrieve `serialize` parameters inside converters,\n    # so exclude_unset is set to True as it's the default apischema setting\n    return getattr(obj, \"__root__\", obj.dict(exclude_unset=True))\n\n\n# ---------- Pydantic support code ends here ----------\n\n\nclass Foo(pydantic.BaseModel):\n    bar: int\n\n\nassert deserialize(Foo, {\"bar\": 0}) == Foo(bar=0)\nassert serialize(Foo, Foo(bar=0)) == {\"bar\": 0}\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"title\": \"Foo\",  # pydantic title\n    \"type\": \"object\",\n    \"properties\": {\"bar\": {\"title\": \"Bar\", \"type\": \"integer\"}},\n    \"required\": [\"bar\"],\n}\nwith pytest.raises(ValidationError) as err:\n    deserialize(Foo, {\"bar\": \"not an int\"})\nassert err.value.errors == [\n    {\"loc\": [\"bar\"], \"err\": \"value is not a valid integer\"}  # pydantic error message\n]\n</code></pre>"},{"location":"examples/recoverable_fields/","title":"Recoverable fields","text":"<p>Inspired by https://github.com/samuelcolvin/pydantic/issues/800</p> <pre><code>from typing import Any, Dict, Generic, TypeVar\n\nimport pytest\n\nfrom apischema import deserialize, deserializer, schema, serialize, serializer\nfrom apischema.json_schema import deserialization_schema, serialization_schema\n\n\n# Add a dummy placeholder comment in order to not have an empty schema\n# (because Union member with empty schema would \"contaminate\" whole Union schema)\n@schema(extra={\"$comment\": \"recoverable\"})\nclass RecoverableRaw(Exception):\n    def __init__(self, raw: Any):\n        self.raw = raw\n\n\ndeserializer(RecoverableRaw)\n\nT = TypeVar(\"T\")\n\n\ndef remove_recoverable_schema(json_schema: Dict[str, Any]):\n    if \"anyOf\" in json_schema:  # deserialization schema\n        value_schema, recoverable_comment = json_schema.pop(\"anyOf\")\n        assert recoverable_comment == {\"$comment\": \"recoverable\"}\n        json_schema.update(value_schema)\n\n\n@schema(extra=remove_recoverable_schema)\nclass Recoverable(Generic[T]):\n    def __init__(self, value: T | RecoverableRaw):\n        self._value = value\n\n    @property\n    def value(self) -&gt; T:\n        if isinstance(self._value, RecoverableRaw):\n            raise self._value\n        return self._value\n\n    @value.setter\n    def value(self, value: T):\n        self._value = value\n\n\ndeserializer(Recoverable)\nserializer(Recoverable.value)\n\nassert deserialize(Recoverable[int], 0).value == 0\nwith pytest.raises(RecoverableRaw) as err:\n    _ = deserialize(Recoverable[int], \"bad\").value\nassert err.value.raw == \"bad\"\n\nassert serialize(Recoverable[int], Recoverable(0)) == 0\nwith pytest.raises(RecoverableRaw) as err:\n    serialize(Recoverable[int], Recoverable(RecoverableRaw(\"bad\")))\nassert err.value.raw == \"bad\"\n\nassert (\n    deserialization_schema(Recoverable[int])\n    == serialization_schema(Recoverable[int])\n    == {\"$schema\": \"http://json-schema.org/draft/2020-12/schema#\", \"type\": \"integer\"}\n)\n</code></pre>"},{"location":"examples/sqlalchemy_support/","title":"SQLAlchemy support","text":"<p>This example shows simple support for SQLAlchemy.</p> <pre><code>from collections.abc import Collection\nfrom inspect import getmembers\nfrom itertools import starmap\nfrom typing import Any\n\nfrom graphql import print_schema\nfrom sqlalchemy import Column, Integer, String\nfrom sqlalchemy.ext.declarative import as_declarative\n\nfrom apischema import Undefined, deserialize, serialize\nfrom apischema.graphql import graphql_schema\nfrom apischema.json_schema import deserialization_schema\nfrom apischema.objects import ObjectField, set_object_fields\n\n\ndef column_field(name: str, column: Column) -&gt; ObjectField:\n    required = False\n    default: Any = ...\n    if column.default is not None:\n        default = column.default\n    elif column.server_default is not None:\n        default = Undefined\n    elif column.nullable:\n        default = None\n    else:\n        required = True\n    col_type = column.type.python_type\n    if column.nullable:\n        col_type = col_type | None\n    return ObjectField(column.name or name, col_type, required, default=default)\n\n\n# Very basic SQLAlchemy support\n@as_declarative()\nclass Base:\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        columns = getmembers(cls, lambda m: isinstance(m, Column))\n        if not columns:\n            return\n        set_object_fields(cls, starmap(column_field, columns))\n\n\nclass Foo(Base):\n    __tablename__ = \"foo\"\n    bar = Column(Integer, primary_key=True)\n    baz = Column(String)\n\n\nfoo = deserialize(Foo, {\"bar\": 0})\nassert isinstance(foo, Foo)\nassert foo.bar == 0\nassert serialize(Foo, foo) == {\"bar\": 0, \"baz\": None}\nassert deserialization_schema(Foo) == {\n    \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"bar\": {\"type\": \"integer\"},\n        \"baz\": {\"type\": [\"string\", \"null\"], \"default\": None},\n    },\n    \"required\": [\"bar\"],\n    \"additionalProperties\": False,\n}\n\n\ndef foos() -&gt; Collection[Foo] | None:\n    ...\n\n\nschema = graphql_schema(query=[foos])\nschema_str = \"\"\"\\\ntype Query {\n  foos: [Foo!]\n}\n\ntype Foo {\n  bar: Int!\n  baz: String\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"examples/subclass_tagged_union/","title":"Class as tagged union of its subclasses","text":"<p>From https://github.com/wyfo/apischema/discussions/56</p> <p>Tagged unions are useful when it comes to GraphQL input (or even output).</p> <pre><code>from collections import defaultdict\nfrom collections.abc import AsyncIterable, Callable, Iterator\nfrom dataclasses import dataclass, field\nfrom types import new_class\nfrom typing import Annotated, Any, TypeVar, get_type_hints\n\nimport graphql\n\nfrom apischema import deserializer, schema, serializer, type_name\nfrom apischema.conversions import Conversion\nfrom apischema.graphql import graphql_schema\nfrom apischema.metadata import conversion\nfrom apischema.objects import object_deserialization\nfrom apischema.tagged_unions import Tagged, TaggedUnion, get_tagged\nfrom apischema.utils import to_pascal_case\n\n_alternative_constructors: dict[type, list[Callable]] = defaultdict(list)\nFunc = TypeVar(\"Func\", bound=Callable)\n\n\ndef alternative_constructor(func: Func) -&gt; Func:\n    _alternative_constructors[get_type_hints(func)[\"return\"]].append(func)\n    return func\n\n\ndef rec_subclasses(cls: type) -&gt; Iterator[type]:\n    \"\"\"Recursive implementation of type.__subclasses__\"\"\"\n    for sub_cls in cls.__subclasses__():\n        yield sub_cls\n        yield from rec_subclasses(sub_cls)\n\n\nCls = TypeVar(\"Cls\", bound=type)\n\n\ndef as_tagged_union(cls: Cls) -&gt; Cls:\n    def serialization() -&gt; Conversion:\n        annotations = {sub.__name__: Tagged[sub] for sub in rec_subclasses(cls)}\n        namespace = {\"__annotations__\": annotations}\n        tagged_union = new_class(\n            cls.__name__, (TaggedUnion,), exec_body=lambda ns: ns.update(namespace)\n        )\n        return Conversion(\n            lambda obj: tagged_union(**{obj.__class__.__name__: obj}),\n            source=cls,\n            target=tagged_union,\n            # Conversion must not be inherited because it would lead to\n            # infinite recursion otherwise\n            inherited=False,\n        )\n\n    def deserialization() -&gt; Conversion:\n        annotations: dict[str, Any] = {}\n        namespace: dict[str, Any] = {\"__annotations__\": annotations}\n        for sub in rec_subclasses(cls):\n            annotations[sub.__name__] = Tagged[sub]\n            # Add tagged fields for all its alternative constructors\n            for constructor in _alternative_constructors.get(sub, ()):\n                # Build the alias of the field\n                alias = to_pascal_case(constructor.__name__)\n                # object_deserialization uses get_type_hints, but the constructor\n                # return type is stringified and the class not defined yet,\n                # so it must be assigned manually\n                constructor.__annotations__[\"return\"] = sub\n                # Use object_deserialization to wrap constructor as deserializer\n                deserialization = object_deserialization(constructor, type_name(alias))\n                # Add constructor tagged field with its conversion\n                annotations[alias] = Tagged[sub]\n                namespace[alias] = Tagged(conversion(deserialization=deserialization))\n        # Create the deserialization tagged union class\n        tagged_union = new_class(\n            cls.__name__, (TaggedUnion,), exec_body=lambda ns: ns.update(namespace)\n        )\n        return Conversion(\n            lambda obj: get_tagged(obj)[1], source=tagged_union, target=cls\n        )\n\n    deserializer(lazy=deserialization, target=cls)\n    serializer(lazy=serialization, source=cls)\n    return cls\n\n\n@as_tagged_union\nclass Drawing:\n    def points(self) -&gt; AsyncIterable[float]:\n        raise NotImplementedError\n\n\n@dataclass\nclass Line(Drawing):\n    start: float\n    stop: float\n    step: float = field(default=1, metadata=schema(exc_min=0))\n\n    async def points(self) -&gt; AsyncIterable[float]:\n        point = self.start\n        while point &lt;= self.stop:\n            yield point\n            point += self.step\n\n\n@alternative_constructor\ndef sized_line(\n    start: float, stop: float, size: Annotated[float, schema(min=1)]\n) -&gt; \"Line\":\n    return Line(start=start, stop=stop, step=(stop - start) / (size - 1))\n\n\n@dataclass\nclass Concat(Drawing):\n    left: Drawing\n    right: Drawing\n\n    async def points(self) -&gt; AsyncIterable[float]:\n        async for point in self.left.points():\n            yield point\n        async for point in self.right.points():\n            yield point\n\n\ndef echo(drawing: Drawing = None) -&gt; Drawing | None:\n    return drawing\n\n\ndrawing_schema = graphql_schema(query=[echo])\nassert (\n    graphql.utilities.print_schema(drawing_schema)\n    == \"\"\"\\\ntype Query {\n  echo(drawing: DrawingInput): Drawing\n}\n\ntype Drawing {\n  Line: Line\n  Concat: Concat\n}\n\ntype Line {\n  start: Float!\n  stop: Float!\n  step: Float!\n}\n\ntype Concat {\n  left: Drawing!\n  right: Drawing!\n}\n\ninput DrawingInput {\n  Line: LineInput\n  SizedLine: SizedLineInput\n  Concat: ConcatInput\n}\n\ninput LineInput {\n  start: Float!\n  stop: Float!\n  step: Float! = 1\n}\n\ninput SizedLineInput {\n  start: Float!\n  stop: Float!\n  size: Float!\n}\n\ninput ConcatInput {\n  left: DrawingInput!\n  right: DrawingInput!\n}\"\"\"\n)\n\nquery = \"\"\"\\\n{\necho(drawing: {\n    Concat: {\n        left: {\n            SizedLine: {\n                start: 0, stop: 12, size: 3\n            },\n        },\n        right: {\n            Line: {\n                start: 12, stop: 13\n            },\n        }\n    }\n}) {\n    Concat {\n        left {\n            Line {\n                start stop step\n            }\n        }\n        right {\n            Line {\n                start stop step\n            }\n        }\n    }\n}\n}\"\"\"\n\nassert graphql.graphql_sync(drawing_schema, query).data == {\n    \"echo\": {\n        \"Concat\": {\n            \"left\": {\"Line\": {\"start\": 0.0, \"stop\": 12.0, \"step\": 6.0}},\n            \"right\": {\"Line\": {\"start\": 12.0, \"stop\": 13.0, \"step\": 1.0}},\n        }\n    }\n}\n</code></pre>"},{"location":"examples/subclass_union/","title":"Class as union of its subclasses","text":"<p>Inspired by https://github.com/samuelcolvin/pydantic/issues/2036</p> <p>A class can easily be deserialized as a union of its subclasses using deserializers. Indeed, when more than one deserializer is registered, it results in a union.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Any, Union\n\nfrom apischema import deserialize, deserializer, identity, serializer\nfrom apischema.conversions import Conversion\nfrom apischema.json_schema import deserialization_schema, serialization_schema\n\n\nclass Base:\n    _union: Any = None\n\n    # You can use __init_subclass__ to register new subclass automatically\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        # Deserializers stack directly as a Union\n        deserializer(Conversion(identity, source=cls, target=Base))\n        # Only Base serializer must be registered (and updated for each subclass) as\n        # a Union, and not be inherited\n        Base._union = cls if Base._union is None else Union[Base._union, cls]\n        serializer(\n            Conversion(identity, source=Base, target=Base._union, inherited=False)\n        )\n\n\n@dataclass\nclass Foo(Base):\n    foo: int\n\n\n@dataclass\nclass Bar(Base):\n    bar: str\n\n\nassert (\n    deserialization_schema(Base)\n    == serialization_schema(Base)\n    == {\n        \"anyOf\": [\n            {\n                \"type\": \"object\",\n                \"properties\": {\"foo\": {\"type\": \"integer\"}},\n                \"required\": [\"foo\"],\n                \"additionalProperties\": False,\n            },\n            {\n                \"type\": \"object\",\n                \"properties\": {\"bar\": {\"type\": \"string\"}},\n                \"required\": [\"bar\"],\n                \"additionalProperties\": False,\n            },\n        ],\n        \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n    }\n)\nassert deserialize(Base, {\"foo\": 0}) == Foo(0)\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/","title":"Data model and resolvers","text":"<p>Almost everything in the Data model section remains valid in GraphQL integration, with a few differences.</p>"},{"location":"graphql/data_model_and_resolvers/#graphql-specific-data-model","title":"GraphQL specific data model","text":""},{"location":"graphql/data_model_and_resolvers/#enum","title":"<code>Enum</code>","text":"<p><code>Enum</code> members are represented in the schema using their name instead of their value. This is more consistent with the way GraphQL represents enumerations.</p>"},{"location":"graphql/data_model_and_resolvers/#typeddict","title":"<code>TypedDict</code>","text":"<p><code>TypedDict</code> is not supported as an output type. (see FAQ)</p>"},{"location":"graphql/data_model_and_resolvers/#union","title":"<code>Union</code>","text":"<p>Unions are only supported between output object type, which means <code>dataclass</code> and <code>NamedTuple</code> (and conversions/dataclass model).</p> <p>There are 2 exceptions which can be always be used in <code>Union</code>:</p> <ul> <li><code>None</code>/<code>Optional</code>: Types are non-null (marked with an exclamation mark <code>!</code> in GraphQL schema) by default; <code>Optional</code> types however results in normal GraphQL types (without <code>!</code>).</li> <li><code>apischema.UndefinedType</code>: it is simply ignored. It is useful in resolvers, see following section</li> </ul>"},{"location":"graphql/data_model_and_resolvers/#non-null","title":"Non-null","text":"<p>Types are assumed to be non-null by default, as in Python typing. Nullable types are obtained using <code>typing.Optional</code> (or <code>typing.Union</code> with a <code>None</code> argument).</p> <p>Note</p> <p>There is one exception, when resolver parameter default value is not serializable (and thus cannot be included in the schema), the parameter type is then set as nullable to make the parameter non-required. For example parameters not <code>Optional</code> but with <code>Undefined</code> default value will be marked as nullable. This is only for the schema, the default value is still used at execution.</p>"},{"location":"graphql/data_model_and_resolvers/#undefined","title":"Undefined","text":"<p>In output, <code>Undefined</code> is converted to <code>None</code>; so in the schema, <code>Union[T, UndefinedType]</code> will be nullable.</p> <p>In input, fields become nullable when <code>Undefined</code> is their default value.</p>"},{"location":"graphql/data_model_and_resolvers/#interfaces","title":"Interfaces","text":"<p>Interfaces are simply classes marked with <code>apischema.graphql.interface</code> decorator. An object type implements an interface when its class inherits from an interface-marked class, or when it has flattened fields of interface-marked dataclass.</p> <pre><code>from dataclasses import dataclass\n\nfrom graphql import print_schema\n\nfrom apischema.graphql import graphql_schema, interface\n\n\n@interface\n@dataclass\nclass Bar:\n    bar: int\n\n\n@dataclass\nclass Foo(Bar):\n    baz: str\n\n\ndef foo() -&gt; Foo | None:\n    ...\n\n\nschema = graphql_schema(query=[foo])\nschema_str = \"\"\"\\\ntype Query {\n  foo: Foo\n}\n\ntype Foo implements Bar {\n  bar: Int!\n  baz: String!\n}\n\ninterface Bar {\n  bar: Int!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#resolvers","title":"Resolvers","text":"<p>All <code>dataclass</code>/<code>NamedTuple</code> fields (excepted skipped) are resolved with their alias in the GraphQL schema.</p> <p>Custom resolvers can also be added by marking methods with <code>apischema.graphql.resolver</code> decorator \u2014 resolvers share a common interface with <code>apischema.serialized</code>, with a few differences.</p> <p>Methods can be synchronous or asynchronous (defined with <code>async def</code> or annotated with an <code>typing.Awaitable</code> return type).</p> <p>Resolvers parameters are included in the schema with their type, and their default value.</p> <pre><code>from dataclasses import dataclass\n\nfrom graphql import print_schema\n\nfrom apischema.graphql import graphql_schema, resolver\n\n\n@dataclass\nclass Bar:\n    baz: int\n\n\n@dataclass\nclass Foo:\n    @resolver\n    async def bar(self, arg: int = 0) -&gt; Bar:\n        ...\n\n\nasync def foo() -&gt; Foo | None:\n    ...\n\n\nschema = graphql_schema(query=[foo])\nschema_str = \"\"\"\\\ntype Query {\n  foo: Foo\n}\n\ntype Foo {\n  bar(arg: Int! = 0): Bar!\n}\n\ntype Bar {\n  baz: Int!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#graphqlresolveinfo-parameter","title":"<code>GraphQLResolveInfo</code> parameter","text":"<p>Resolvers can have an additional parameter of type <code>graphql.GraphQLResolveInfo</code> (or <code>Optional[graphql.GraphQLResolveInfo]</code>), which is automatically injected when the resolver is executed in the context of a GraphQL request. This parameter contains the info about the current GraphQL request being executed.</p>"},{"location":"graphql/data_model_and_resolvers/#undefined-parameter-default-null-vs-undefined","title":"Undefined parameter default \u2014 <code>null</code> vs. <code>undefined</code>","text":"<p><code>Undefined</code> can be used as default value of resolver parameters. It can be to distinguish a <code>null</code> input from an absent/<code>undefined</code> input. In fact, <code>null</code> value will result in a <code>None</code> argument where no value will use the default value, <code>Undefined</code> so.</p> <pre><code>from graphql import graphql_sync\n\nfrom apischema import Undefined, UndefinedType\nfrom apischema.graphql import graphql_schema\n\n\ndef arg_is_absent(arg: int | UndefinedType | None = Undefined) -&gt; bool:\n    return arg is Undefined\n\n\nschema = graphql_schema(query=[arg_is_absent])\nassert graphql_sync(schema, \"{argIsAbsent(arg: null)}\").data == {\"argIsAbsent\": False}\nassert graphql_sync(schema, \"{argIsAbsent}\").data == {\"argIsAbsent\": True}\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#error-handling","title":"Error handling","text":"<p>Errors occurring in resolvers can be caught in a dedicated error handler registered with <code>error_handler</code> parameter. This function takes in parameters the exception, the object, the info and the kwargs of the failing resolver; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization.</p> <p>The resulting serialization type will be a <code>Union</code> of the normal type and the error handling type; if the error handler always raises, use <code>typing.NoReturn</code> annotation.</p> <p><code>error_handler=None</code> correspond to a default handler which only return <code>None</code> \u2014 exception is thus discarded and the resolver type becomes <code>Optional</code>.</p> <p>The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code.</p> <p>Error handler can be synchronous or asynchronous.</p> <pre><code>from dataclasses import dataclass\nfrom logging import getLogger\nfrom typing import Any\n\nimport graphql\nfrom graphql.utilities import print_schema\n\nfrom apischema.graphql import graphql_schema, resolver\n\nlogger = getLogger(__name__)\n\n\ndef log_error(\n    error: Exception, obj: Any, info: graphql.GraphQLResolveInfo, **kwargs\n) -&gt; None:\n    logger.error(\n        \"Resolve error in %s\", \".\".join(map(str, info.path.as_list())), exc_info=error\n    )\n    return None\n\n\n@dataclass\nclass Foo:\n    @resolver(error_handler=log_error)\n    def bar(self) -&gt; int:\n        raise RuntimeError(\"Bar error\")\n\n    @resolver\n    def baz(self) -&gt; int:\n        raise RuntimeError(\"Baz error\")\n\n\ndef foo(info: graphql.GraphQLResolveInfo) -&gt; Foo:\n    return Foo()\n\n\nschema = graphql_schema(query=[foo])\n# Notice that bar is Int while baz is Int!\nschema_str = \"\"\"\\\ntype Query {\n  foo: Foo!\n}\n\ntype Foo {\n  bar: Int\n  baz: Int!\n}\"\"\"\nassert print_schema(schema) == schema_str\n# Logs \"Resolve error in foo.bar\", no error raised\nassert graphql.graphql_sync(schema, \"{foo{bar}}\").data == {\"foo\": {\"bar\": None}}\n# Error is raised\nassert graphql.graphql_sync(schema, \"{foo{baz}}\").errors[0].message == \"Baz error\"\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#parameters-metadata","title":"Parameters metadata","text":"<p>Resolvers parameters can have metadata like dataclass fields. They can be passed using <code>typing.Annotated</code>.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Annotated\n\nfrom graphql.utilities import print_schema\n\nfrom apischema import alias, schema\nfrom apischema.graphql import graphql_schema, resolver\n\n\n@dataclass\nclass Foo:\n    @resolver\n    def bar(\n        self, param: Annotated[int, alias(\"arg\") | schema(description=\"argument\")]\n    ) -&gt; int:\n        return param\n\n\ndef foo() -&gt; Foo:\n    return Foo()\n\n\nschema_ = graphql_schema(query=[foo])\n# Notice that bar is Int while baz is Int!\nschema_str = '''\\\ntype Query {\n  foo: Foo!\n}\n\ntype Foo {\n  bar(\n    \"\"\"argument\"\"\"\n    arg: Int!\n  ): Int!\n}'''\nassert print_schema(schema_) == schema_str\n</code></pre> <p>Note</p> <p>Metadata can also be passed with <code>parameters_metadata</code> parameter; it takes a mapping of parameter names as key and mapped metadata as value.</p>"},{"location":"graphql/data_model_and_resolvers/#parameters-base-schema","title":"Parameters base schema","text":"<p>Following the example of type/field/method base schema, resolver parameters also support a base schema definition</p> <pre><code>import inspect\nfrom dataclasses import dataclass\nfrom typing import Any, Callable\n\nimport docstring_parser\nfrom graphql.utilities import print_schema\n\nfrom apischema import schema, settings\nfrom apischema.graphql import graphql_schema, resolver\nfrom apischema.schemas import Schema\n\n\n@dataclass\nclass Foo:\n    @resolver\n    def bar(self, arg: str) -&gt; int:\n        \"\"\"bar method\n\n        :param arg: arg parameter\n        \"\"\"\n        ...\n\n\ndef method_base_schema(tp: Any, method: Callable, alias: str) -&gt; Schema | None:\n    return schema(description=docstring_parser.parse(method.__doc__).short_description)\n\n\ndef parameter_base_schema(\n    method: Callable, parameter: inspect.Parameter, alias: str\n) -&gt; Schema | None:\n    for doc_param in docstring_parser.parse(method.__doc__).params:\n        if doc_param.arg_name == parameter.name:\n            return schema(description=doc_param.description)\n    return None\n\n\nsettings.base_schema.method = method_base_schema\nsettings.base_schema.parameter = parameter_base_schema\n\n\ndef foo() -&gt; Foo:\n    ...\n\n\nschema_ = graphql_schema(query=[foo])\nschema_str = '''\\\ntype Query {\n  foo: Foo!\n}\n\ntype Foo {\n  \"\"\"bar method\"\"\"\n  bar(\n    \"\"\"arg parameter\"\"\"\n    arg: String!\n  ): Int!\n}'''\nassert print_schema(schema_) == schema_str\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#scalars","title":"Scalars","text":"<p><code>NewType</code> or non-object types annotated with <code>type_name</code>  will be translated in the GraphQL schema by a <code>scalar</code>. By the way, <code>Any</code> will automatically be translated to a <code>JSON</code> scalar, as it is deserialized from and serialized to JSON.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Any\nfrom uuid import UUID\n\nfrom graphql.utilities import print_schema\n\nfrom apischema.graphql import graphql_schema\n\n\n@dataclass\nclass Foo:\n    id: UUID\n    content: Any\n\n\ndef foo() -&gt; Foo | None:\n    ...\n\n\nschema = graphql_schema(query=[foo])\nschema_str = \"\"\"\\\ntype Query {\n  foo: Foo\n}\n\ntype Foo {\n  id: UUID!\n  content: JSON\n}\n\nscalar UUID\n\nscalar JSON\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#id-type","title":"ID type","text":"<p>GraphQL <code>ID</code> has no precise specification and is defined according API needs; it can be a UUID or/and ObjectId, etc.</p> <p><code>apischema.graphql_schema</code> has a parameter <code>id_types</code> which can be used to define which types will be marked as <code>ID</code> in the generated schema. Parameter value can be either a collection of types (each type will then be mapped to <code>ID</code> scalar), or a predicate returning if the given type must be marked as <code>ID</code>.</p> <pre><code>from dataclasses import dataclass\nfrom uuid import UUID\n\nfrom graphql import print_schema\n\nfrom apischema.graphql import graphql_schema\n\n\n@dataclass\nclass Foo:\n    bar: UUID\n\n\ndef foo() -&gt; Foo | None:\n    ...\n\n\n# id_types={UUID} is equivalent to id_types=lambda t: t in {UUID}\nschema = graphql_schema(query=[foo], id_types={UUID})\nschema_str = \"\"\"\\\ntype Query {\n  foo: Foo\n}\n\ntype Foo {\n  bar: ID!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre> <p>Note</p> <p><code>ID</code> type could also be identified using <code>typing.Annotated</code> and a predicate looking into annotations.</p> <p>apischema also provides a simple <code>ID</code> type with <code>apischema.graphql.ID</code>. It is just defined as a <code>NewType</code> of string, so you can use it when you want to manipulate raw <code>ID</code> strings in your resolvers.</p>"},{"location":"graphql/data_model_and_resolvers/#id-encoding","title":"ID encoding","text":"<p><code>ID</code> encoding can directly be controlled the <code>id_encoding</code> parameters of <code>graphql_schema</code>. A current practice is to use base64 encoding for <code>ID</code>.</p> <pre><code>from base64 import b64decode, b64encode\nfrom dataclasses import dataclass\nfrom uuid import UUID\n\nfrom graphql import graphql_sync\n\nfrom apischema.graphql import graphql_schema\n\n\n@dataclass\nclass Foo:\n    id: UUID\n\n\ndef foo() -&gt; Foo | None:\n    return Foo(UUID(\"58c88e87-5769-4723-8974-f9ec5007a38b\"))\n\n\nschema = graphql_schema(\n    query=[foo],\n    id_types={UUID},\n    id_encoding=(\n        lambda s: b64decode(s).decode(),\n        lambda s: b64encode(s.encode()).decode(),\n    ),\n)\n\nassert graphql_sync(schema, \"{foo{id}}\").data == {\n    \"foo\": {\"id\": \"NThjODhlODctNTc2OS00NzIzLTg5NzQtZjllYzUwMDdhMzhi\"}\n}\n</code></pre> <p>Note</p> <p>You can also use <code>relay.base64_encoding</code> (see next section)</p> <p>Note</p> <p><code>ID</code> serialization (respectively deserialization) is applied after apischema conversions (respectively before apischema conversion): in the example, uuid is already converted into string before being passed to <code>id_serializer</code>.</p> <p>If you use base64 encodeing and an ID type which is converted by apischema to a base64 str, you will get a double encoded base64 string</p>"},{"location":"graphql/data_model_and_resolvers/#tagged-unions","title":"Tagged unions","text":"<p>Important</p> <p>This feature has a provisional status, as the concerned GraphQL RFC is not finalized.</p> <p>apischema provides a <code>apischema.tagged_unions.TaggedUnion</code> base class which helps to implement the tagged union pattern. It's fields must be typed using <code>apischema.tagged_unions.Tagged</code> generic type.</p> <pre><code>from dataclasses import dataclass\n\nimport pytest\n\nfrom apischema import Undefined, ValidationError, alias, deserialize, schema, serialize\nfrom apischema.tagged_unions import Tagged, TaggedUnion, get_tagged\n\n\n@dataclass\nclass Bar:\n    field: str\n\n\nclass Foo(TaggedUnion):\n    bar: Tagged[Bar]\n    # Tagged can have metadata like a dataclass fields\n    i: Tagged[int] = Tagged(alias(\"baz\") | schema(min=0))\n\n\n# Instantiate using class fields\ntagged_bar = Foo.bar(Bar(\"value\"))\n# you can also use default constructor, but it's not typed-checked\nassert tagged_bar == Foo(bar=Bar(\"value\"))\n\n# All fields that are not tagged are Undefined\nassert tagged_bar.bar is not Undefined and tagged_bar.i is Undefined\n# get_tagged allows to retrieve the tag and it's value\n# (but the value is not typed-checked)\nassert get_tagged(tagged_bar) == (\"bar\", Bar(\"value\"))\n\n# (De)serialization works as expected\nassert deserialize(Foo, {\"bar\": {\"field\": \"value\"}}) == tagged_bar\nassert serialize(Foo, tagged_bar) == {\"bar\": {\"field\": \"value\"}}\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(Foo, {\"unknown\": 42})\nassert err.value.errors == [{\"loc\": [\"unknown\"], \"err\": \"unexpected property\"}]\n\nwith pytest.raises(ValidationError) as err:\n    deserialize(Foo, {\"bar\": {\"field\": \"value\"}, \"baz\": 0})\nassert err.value.errors == [\n    {\"loc\": [], \"err\": \"property count greater than 1 (maxProperties)\"}\n]\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#json-schema","title":"JSON schema","text":"<p>Tagged unions JSON schema uses <code>minProperties: 1</code> and <code>maxProperties: 1</code>.</p> <pre><code>from dataclasses import dataclass\n\nfrom apischema.json_schema import deserialization_schema, serialization_schema\nfrom apischema.tagged_unions import Tagged, TaggedUnion\n\n\n@dataclass\nclass Bar:\n    field: str\n\n\nclass Foo(TaggedUnion):\n    bar: Tagged[Bar]\n    baz: Tagged[int]\n\n\nassert (\n    deserialization_schema(Foo)\n    == serialization_schema(Foo)\n    == {\n        \"$schema\": \"http://json-schema.org/draft/2020-12/schema#\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\n                \"type\": \"object\",\n                \"properties\": {\"field\": {\"type\": \"string\"}},\n                \"required\": [\"field\"],\n                \"additionalProperties\": False,\n            },\n            \"baz\": {\"type\": \"integer\"},\n        },\n        \"additionalProperties\": False,\n        \"minProperties\": 1,\n        \"maxProperties\": 1,\n    }\n)\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#graphql-schema","title":"GraphQL schema","text":"<p>As tagged unions are not (yet?) part of the GraphQL spec, they are just implemented as normal (input) object type with nullable fields. An error is raised if several tags are passed in input.</p> <pre><code>from dataclasses import dataclass\n\nfrom graphql import graphql_sync\nfrom graphql.utilities import print_schema\n\nfrom apischema.graphql import graphql_schema\nfrom apischema.tagged_unions import Tagged, TaggedUnion\n\n\n@dataclass\nclass Bar:\n    field: str\n\n\nclass Foo(TaggedUnion):\n    bar: Tagged[Bar]\n    baz: Tagged[int]\n\n\ndef query(foo: Foo) -&gt; Foo:\n    return foo\n\n\nschema = graphql_schema(query=[query])\nschema_str = \"\"\"\\\ntype Query {\n  query(foo: FooInput!): Foo!\n}\n\ntype Foo {\n  bar: Bar\n  baz: Int\n}\n\ntype Bar {\n  field: String!\n}\n\ninput FooInput {\n  bar: BarInput\n  baz: Int\n}\n\ninput BarInput {\n  field: String!\n}\"\"\"\nassert print_schema(schema) == schema_str\n\nquery_str = \"\"\"\n{\n    query(foo: {bar: {field: \"value\"}}) {\n        bar {\n            field\n        }\n        baz\n    }\n}\"\"\"\nassert graphql_sync(schema, query_str).data == {\n    \"query\": {\"bar\": {\"field\": \"value\"}, \"baz\": None}\n}\n</code></pre>"},{"location":"graphql/data_model_and_resolvers/#faq","title":"FAQ","text":""},{"location":"graphql/data_model_and_resolvers/#why-typeddict-is-not-supported-as-an-output-type","title":"Why <code>TypedDict</code> is not supported as an output type?","text":"<p>At first, <code>TypedDict</code> subclasses are not real classes, so they cannot be used to check types at runtime. Runtime check is however requried to disambiguate unions/interfaces. A hack could be done to solve this issue, but there is another one which cannot be hacked: <code>TypedDict</code> inheritance hierarchy is lost at runtime, so they don't play nicely with the interface concept. </p>"},{"location":"graphql/overview/","title":"GraphQL Overview","text":"<p>apischema supports GraphQL through the graphql-core library.</p> <p>You can install this dependency directly with apischema using the following extra requirement: <pre><code>pip install apischema[graphql]\n</code></pre></p> <p>GraphQL supports consists of generating a GraphQL schema <code>graphql.GraphQLSchema</code> from your data model and endpoints (queries/mutations/subscribtions), in a similar way than the JSON schema generation. This schema can then be used through graphql-core library to query/mutate/subscribe.</p> <pre><code>from dataclasses import dataclass\nfrom datetime import date, datetime\nfrom typing import Collection\nfrom uuid import UUID, uuid4\n\nfrom graphql import graphql_sync, print_schema\n\nfrom apischema.graphql import graphql_schema, resolver\n\n\n@dataclass\nclass User:\n    id: UUID\n    username: str\n    birthday: date | None = None\n\n    @resolver\n    def posts(self) -&gt; Collection[\"Post\"]:\n        return [post for post in POSTS if post.author.id == self.id]\n\n\n@dataclass\nclass Post:\n    id: UUID\n    author: User\n    date: datetime\n    content: str\n\n\nUSERS = [User(uuid4(), \"foo\"), User(uuid4(), \"bar\")]\nPOSTS = [Post(uuid4(), USERS[0], datetime.now(), \"Hello world!\")]\n\n\ndef users() -&gt; Collection[User]:\n    return USERS\n\n\ndef posts() -&gt; Collection[Post]:\n    return POSTS\n\n\ndef user(username: str) -&gt; User | None:\n    for user in users():\n        if user.username == username:\n            return user\n    else:\n        return None\n\n\nschema = graphql_schema(query=[users, user, posts], id_types={UUID})\nschema_str = \"\"\"\\\ntype Query {\n  users: [User!]!\n  user(username: String!): User\n  posts: [Post!]!\n}\n\ntype User {\n  id: ID!\n  username: String!\n  birthday: Date\n  posts: [Post!]!\n}\n\nscalar Date\n\ntype Post {\n  id: ID!\n  author: User!\n  date: Datetime!\n  content: String!\n}\n\nscalar Datetime\"\"\"\nassert print_schema(schema) == schema_str\n\nquery = \"\"\"\n{\n  users {\n    username\n    posts {\n        content\n    }\n  }\n}\"\"\"\nassert graphql_sync(schema, query).data == {\n    \"users\": [\n        {\"username\": \"foo\", \"posts\": [{\"content\": \"Hello world!\"}]},\n        {\"username\": \"bar\", \"posts\": []},\n    ]\n}\n</code></pre> <p>GraphQL is fully integrated with the rest of apischema features, especially conversions, so it's easy to integrate ORM and other custom types in the generated schema; this concerns query results but also arguments.</p> <p>By the way, while GraphQL doesn't support constraints, apischema still offers you all the power of its validation feature. In fact, apischema deserialize and validate all the arguments passed to resolvers. </p>"},{"location":"graphql/overview/#faq","title":"FAQ","text":""},{"location":"graphql/overview/#is-it-possible-to-use-the-same-classes-to-do-both-graphql-and-rest-api","title":"Is it possible to use the same classes to do both GraphQL and REST-API?","text":"<p>Yes it is. GraphQL has some restrictions in comparison to JSON schema (see next section), but this taken in account, all of your code can be reused. In fact, GraphQL endpoints can also be used both by a GraphQL API and a more traditional REST or RPC API.</p>"},{"location":"graphql/relay/","title":"Relay","text":"<p>apischema provides some facilities to implement a GraphQL server following Relay GraphQL server specification. They are included in the module <code>apischema.graphql.relay</code>.</p> <p>Note</p> <p>These facilities are independent of each others \u2014 you could keep only the mutations part and use your own identification and connection system for example.</p>"},{"location":"graphql/relay/#global-object-identification","title":"(Global) Object Identification","text":"<p>apischema defines a generic <code>relay.Node[Id]</code> interface which can be used which can be used as base class of all identified resources. This class contains a unique generic field of type <code>Id</code>, which will be automatically converted into an <code>ID!</code> in the schema. The <code>Id</code> type chosen has to be serializable into a string-convertible value (it can register conversions if needed).</p> <p>Each node has to implement the <code>classmethod</code> <code>get_by_id(cls: type[T], id: Id, info: graphql.GraphQLResolveInfo=None) -&gt; T</code>.</p> <p>All nodes defined can be retrieved using <code>relay.nodes</code>, while the <code>node</code> query is defined as <code>relay.node</code>. <code>relay.nodes()</code> can be passed to <code>graphql_schema</code> <code>types</code> parameter in order to add them in the schema even if they don't appear in any resolvers.</p> <pre><code>from dataclasses import dataclass\nfrom uuid import UUID\n\nimport graphql\nfrom graphql.utilities import print_schema\n\nfrom apischema.graphql import graphql_schema, relay\n\n\n@dataclass\nclass Ship(relay.Node[UUID]):  # Let's use an UUID for Ship id\n    name: str\n\n    @classmethod\n    async def get_by_id(cls, id: UUID, info: graphql.GraphQLResolveInfo = None):\n        ...\n\n\n@dataclass\nclass Faction(relay.Node[int]):  # Nodes can have different id types\n    name: str\n\n    @classmethod\n    def get_by_id(cls, id: int, info: graphql.GraphQLResolveInfo = None) -&gt; \"Faction\":\n        ...\n\n\nschema = graphql_schema(query=[relay.node], types=relay.nodes())\nschema_str = \"\"\"\\\ntype Ship implements Node {\n  id: ID!\n  name: String!\n}\n\ninterface Node {\n  id: ID!\n}\n\ntype Faction implements Node {\n  id: ID!\n  name: String!\n}\n\ntype Query {\n  node(id: ID!): Node!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre> <p>Warning</p> <p>For now, even if its result is not used, <code>relay.nodes</code> must be called before generating the schema.</p>"},{"location":"graphql/relay/#global-id","title":"Global ID","text":"<p>apischema defines a <code>relay.GlobalId</code> type with the following signature :</p> <p><pre><code>@dataclass\nclass GlobalId(Generic[Node]):\n    id: str\n    node_class: type[Node]\n</code></pre> In fact, it is <code>GlobalId</code> type which is serialized and deserialized as an <code>ID!</code>, not the <code>Id</code> parameter of the <code>Node</code> class; apischema automatically add a field converter to make the conversion between the <code>Id</code> (for example an <code>UUID</code>) of a given node and the corresponding <code>GlobalId</code>.</p> <p>Node instance global id can be retrieved with <code>global_id</code> property.</p> <pre><code>from dataclasses import dataclass\n\nimport graphql\n\nfrom apischema import serialize\nfrom apischema.graphql import graphql_schema, relay\n\n\n@dataclass\nclass Faction(relay.Node[int]):\n    name: str\n\n    @classmethod\n    def get_by_id(cls, id: int, info: graphql.GraphQLResolveInfo = None) -&gt; \"Faction\":\n        return [Faction(0, \"Empire\"), Faction(1, \"Rebels\")][id]\n\n\nschema = graphql_schema(query=[relay.node], types=relay.nodes())\nsome_global_id = Faction.get_by_id(0).global_id  # Let's pick a global id ...\nassert some_global_id == relay.GlobalId(\"0\", Faction)\nquery = \"\"\"\nquery factionName($id: ID!) {\n    node(id: $id) {\n        ... on Faction {\n            name\n        }\n    }\n}\"\"\"\nassert graphql.graphql_sync(  # ... and use it in a query\n    schema, query, variable_values={\"id\": serialize(relay.GlobalId, some_global_id)}\n).data == {\"node\": {\"name\": \"Empire\"}}\n</code></pre>"},{"location":"graphql/relay/#id-encoding","title":"Id encoding","text":"<p>Relay specifications encourage the use of base64 encoding, so apischema defines a <code>relay.base64_encoding</code> that you can pass to <code>graphql_schema</code> <code>id_encoding</code> parameter.</p>"},{"location":"graphql/relay/#connections","title":"Connections","text":"<p>apischema provides a generic <code>relay.Connection[Node, Cursor, Edge]</code> type, which can be used directly without subclassing it; it's also possible to subclass it to add fields to a given connection (or to all the connection which will subclass the subclass). <code>relay.Edge[Node, Cursor]</code> can also be subclassed to add fields to the edges.</p> <p><code>Connection</code> dataclass has the following declaration: <pre><code>@dataclass\nclass Connection(Generic[Node, Cursor, Edge]):\n    edges: Optional[Sequence[Optional[Edge]]]\n    has_previous_page: bool = field(default=False, metadata=skip)\n    has_next_page: bool = field(default=False, metadata=skip)\n    start_cursor: Optional[Cursor] = field(default=None, metadata=skip)\n    end_cursor: Optional[Cursor] = field(default=None, metadata=skip)\n\n    @resolver\n    def page_info(self) -&gt; PageInfo[Cursor]:\n        ...\n</code></pre></p> <p>The <code>pageInfo</code> field is computed by a resolver; it uses the cursors of the first and the last edge when they are not provided.</p> <p>Here is an example of <code>Connection</code> use:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional, TypeVar\n\nimport graphql\nfrom graphql.utilities import print_schema\n\nfrom apischema.graphql import graphql_schema, relay, resolver\n\nCursor = int  # let's use an integer cursor in all our connection\nNode = TypeVar(\"Node\")\nConnection = relay.Connection[Node, Cursor, relay.Edge[Node, Cursor]]\n# Connection can now be used just like Connection[Ship] or Connection[Faction | None]\n\n\n@dataclass\nclass Ship:\n    name: str\n\n\n@dataclass\nclass Faction:\n    @resolver\n    def ships(\n        self, first: int | None, after: Cursor | None\n    ) -&gt; Connection[Optional[Ship]] | None:\n        edges = [relay.Edge(Ship(\"X-Wing\"), 0), relay.Edge(Ship(\"B-Wing\"), 1)]\n        return Connection(edges, relay.PageInfo.from_edges(edges))\n\n\ndef faction() -&gt; Faction | None:\n    return Faction()\n\n\nschema = graphql_schema(query=[faction])\nschema_str = \"\"\"\\\ntype Query {\n  faction: Faction\n}\n\ntype Faction {\n  ships(first: Int, after: Int): ShipConnection\n}\n\ntype ShipConnection {\n  edges: [ShipEdge]\n  pageInfo: PageInfo!\n}\n\ntype ShipEdge {\n  node: Ship\n  cursor: Int!\n}\n\ntype Ship {\n  name: String!\n}\n\ntype PageInfo {\n  hasPreviousPage: Boolean!\n  hasNextPage: Boolean!\n  startCursor: Int\n  endCursor: Int\n}\"\"\"\nassert print_schema(schema) == schema_str\nquery = \"\"\"\n{\n    faction {\n        ships {\n            pageInfo {\n                endCursor\n                hasNextPage\n            }\n            edges {\n                cursor\n                node {\n                    name\n                }\n            }\n        }\n    }\n}\"\"\"\nassert graphql.graphql_sync(schema, query).data == {\n    \"faction\": {\n        \"ships\": {\n            \"pageInfo\": {\"endCursor\": 1, \"hasNextPage\": False},\n            \"edges\": [\n                {\"cursor\": 0, \"node\": {\"name\": \"X-Wing\"}},\n                {\"cursor\": 1, \"node\": {\"name\": \"B-Wing\"}},\n            ],\n        }\n    }\n}\n</code></pre>"},{"location":"graphql/relay/#custom-connectionsedges","title":"Custom connections/edges","text":"<p>Connections can be customized by simply subclassing <code>relay.Connection</code> class and adding the additional fields.</p> <p>For the edges, <code>relay.Edge</code> can be subclassed too, and the subclass has then to be passed as type argument to the generic connection.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Optional, TypeVar\n\nfrom graphql import print_schema\n\nfrom apischema.graphql import graphql_schema, relay, resolver\n\nCursor = int\nNode = TypeVar(\"Node\")\nEdge = TypeVar(\"Edge\", bound=relay.Edge)\n\n\n@dataclass\nclass MyConnection(relay.Connection[Node, Cursor, Edge]):\n    connection_field: bool\n\n\n@dataclass\nclass MyEdge(relay.Edge[Node, Cursor]):\n    edge_field: int | None\n\n\nConnection = MyConnection[Node, MyEdge[Node]]\n\n\n@dataclass\nclass Ship:\n    name: str\n\n\n@dataclass\nclass Faction:\n    @resolver\n    def ships(\n        self, first: int | None, after: Cursor | None\n    ) -&gt; Connection[Optional[Ship]] | None:\n        ...\n\n\ndef faction() -&gt; Faction | None:\n    return Faction()\n\n\nschema = graphql_schema(query=[faction])\nschema_str = \"\"\"\\\ntype Query {\n  faction: Faction\n}\n\ntype Faction {\n  ships(first: Int, after: Int): ShipConnection\n}\n\ntype ShipConnection {\n  edges: [ShipEdge]\n  pageInfo: PageInfo!\n  connectionField: Boolean!\n}\n\ntype ShipEdge {\n  node: Ship\n  cursor: Int!\n  edgeField: Int\n}\n\ntype Ship {\n  name: String!\n}\n\ntype PageInfo {\n  hasPreviousPage: Boolean!\n  hasNextPage: Boolean!\n  startCursor: Int\n  endCursor: Int\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/relay/#mutations","title":"Mutations","text":"<p>Relay compliant mutations can be declared with a dataclass subclassing the <code>relay.Mutation</code> class; its fields will be put in the payload type of the mutation.</p> <p>This class must implement a <code>classmethod</code>/<code>staticmethod</code> name <code>mutate</code>; it can be synchronous or asynchronous. The arguments of the method will correspond to the input type fields.</p> <p>The mutation will be named after the name of the mutation class.</p> <p>All the mutations declared can be retrieved with <code>relay.mutations</code>, in order to be passed to <code>graphql_schema</code>.</p> <pre><code>from dataclasses import dataclass\n\nfrom graphql.utilities import print_schema\n\nfrom apischema.graphql import graphql_schema, relay\n\n\n@dataclass\nclass Ship:\n    ...\n\n\n@dataclass\nclass Faction:\n    ...\n\n\n@dataclass\nclass IntroduceShip(relay.Mutation):\n    ship: Ship\n    faction: Faction\n\n    @staticmethod\n    def mutate(faction_id: str, ship_name: str) -&gt; \"IntroduceShip\":\n        ...\n\n\ndef hello() -&gt; str:\n    return \"world\"\n\n\nschema = graphql_schema(query=[hello], mutation=relay.mutations())\nschema_str = \"\"\"\\\ntype Query {\n  hello: String!\n}\n\ntype Mutation {\n  introduceShip(input: IntroduceShipInput!): IntroduceShipPayload!\n}\n\ntype IntroduceShipPayload {\n  ship: Ship!\n  faction: Faction!\n  clientMutationId: String\n}\n\ntype Ship\n\ntype Faction\n\ninput IntroduceShipInput {\n  factionId: String!\n  shipName: String!\n  clientMutationId: String\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/relay/#clientmutationid","title":"ClientMutationId","text":"<p>As you can see in the previous example, the field named <code>clientMutationId</code> is automatically added to the input and the payload types. </p> <p>The forward of the mutation id from the input to the payload is automatically handled. It's value can be accessed by declaring a parameter of type <code>relay.ClientMutationId</code> \u2014 even if the parameter is not named <code>client_mutation_id</code>, it will be renamed internally.</p> <p>This feature is controlled by a <code>Mutation</code> class variable <code>_client_mutation_id</code>, with 3 possible values:</p> <ul> <li><code>None</code> (automatic, the default): <code>clientMutationId</code> field will be nullable unless it's declared as a required parameter (without default value) in the <code>mutate</code> method.</li> <li><code>False</code>: their will be no <code>clientMutationId</code> field added (having a dedicated parameter will raise an error)</li> <li><code>True</code>: <code>clientMutationId</code> is added and forced to be non-null.</li> </ul> <pre><code>from dataclasses import dataclass\n\nfrom graphql.utilities import print_schema\n\nfrom apischema.graphql import graphql_schema, relay\n\n\n@dataclass\nclass Ship:\n    ...\n\n\n@dataclass\nclass Faction:\n    ...\n\n\n@dataclass\nclass IntroduceShip(relay.Mutation):\n    ship: Ship\n    faction: Faction\n\n    @staticmethod\n    def mutate(\n        # mut_id is required because no default value\n        faction_id: str,\n        ship_name: str,\n        mut_id: relay.ClientMutationId,\n    ) -&gt; \"IntroduceShip\":\n        ...\n\n\ndef hello() -&gt; str:\n    return \"world\"\n\n\nschema = graphql_schema(query=[hello], mutation=relay.mutations())\n# clientMutationId field becomes non nullable in introduceShip types\nschema_str = \"\"\"\\\ntype Query {\n  hello: String!\n}\n\ntype Mutation {\n  introduceShip(input: IntroduceShipInput!): IntroduceShipPayload!\n}\n\ntype IntroduceShipPayload {\n  ship: Ship!\n  faction: Faction!\n  clientMutationId: String!\n}\n\ntype Ship\n\ntype Faction\n\ninput IntroduceShipInput {\n  factionId: String!\n  shipName: String!\n  clientMutationId: String!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/relay/#error-handling-and-other-resolver-arguments","title":"Error handling and other resolver arguments","text":"<p>Relay mutation are operations, so they can be configured with the same parameters. As they are declared as classes, parameters will be passed as class variables, prefixed by <code>_</code> (<code>error_handler</code> becomes <code>_error_handler</code>)</p> <p>Note</p> <p>Because parameters are class variables, you can reuse them by setting their value in a base class; for example, to share a same <code>error_handler</code> in a group of mutations.</p>"},{"location":"graphql/schema/","title":"GraphQL schema","text":"<p>GraphQL schema is generated by passing all the operations (query/mutation/subscription) functions to <code>apischema.graphql.graphql_schema</code>. </p> <p>Functions parameters and return types are then processed by apischema to generate the <code>Query</code>/<code>Mutation</code>/<code>Subscription</code> types with their resolvers/subscribers, which are then passed to <code>graphql.GraphQLSchema</code>.</p> <p>In fact, <code>graphql_schema</code> is just a wrapper around <code>graphql.GraphQLSchema</code> (same parameters plus a few extras); it just uses apischema abstraction to build <code>GraphQL</code> object types directly from your code. </p>"},{"location":"graphql/schema/#operations-metadata","title":"Operations metadata","text":"<p>GraphQL operations can be passed to <code>graphql_schema</code> either using simple functions or wrapping it into <code>apischema.graphql.Query</code>/<code>apischema.graphql.Mutation</code>/<code>apischema.graphql.Subscription</code>. These wrappers have the same parameters as <code>apischema.graphql.resolver</code>: <code>alias</code>, <code>conversions</code>, <code>error_handler</code>, <code>order</code> and <code>schema</code> (<code>Subscription</code> has an additional parameter).</p> <pre><code>from dataclasses import dataclass\n\nfrom graphql import print_schema\n\nfrom apischema.graphql import Query, graphql_schema, resolver\n\n\n@dataclass\nclass Foo:\n    @resolver\n    async def bar(self, arg: int = 0) -&gt; str:\n        ...\n\n\nasync def get_foo() -&gt; Foo:\n    ...\n\n\nschema = graphql_schema(query=[Query(get_foo, alias=\"foo\", error_handler=None)])\nschema_str = \"\"\"\\\ntype Query {\n  foo: Foo\n}\n\ntype Foo {\n  bar(arg: Int! = 0): String!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/schema/#camelcase","title":"camelCase","text":"<p>GraphQL use camelCase as a convention for resolvers; apischema follows this convention by automatically convert all resolver names (and their parameters) to camelCase. <code>graphql_schema</code> has an <code>aliaser</code> parameter if you want to use another case.</p>"},{"location":"graphql/schema/#type-names","title":"Type names","text":"<p>Schema types are named the same way they are in generated JSON schema: type name is used by default, and it can be overridden using <code>apischema.type_name</code></p> <pre><code>from dataclasses import dataclass\n\nfrom graphql import print_schema\n\nfrom apischema import type_name\nfrom apischema.graphql import graphql_schema\n\n\n@type_name(\"Foo\")\n@dataclass\nclass FooFoo:\n    bar: int\n\n\ndef foo() -&gt; FooFoo | None:\n    ...\n\n\nschema = graphql_schema(query=[foo])\nschema_str = \"\"\"\\\ntype Query {\n  foo: Foo\n}\n\ntype Foo {\n  bar: Int!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre> <p>Note</p> <p>Type names can be distinguished between JSON schema and GraphQL schema using <code>type_name</code> named parameter. Indeed, <code>type_name(\"foo\")</code> is equivalent to <code>type_name(json_schema=\"foo\", graphql=\"foo\")</code>.</p> <p>However, in GraphQL schema, unions must be named, so <code>typing.Union</code> used should be annotated with <code>apischema.type_name</code>. <code>graphql_schema</code> also provides a <code>union_ref</code> parameter which can be passed as a function to generate a type name from the union argument. Default <code>union_ref</code> is <code>\"Or\".join</code> meaning <code>typing.Union[Foo, Bar]</code> will result in <code>union FooOrBar = Foo | Bar</code></p> <pre><code>from dataclasses import dataclass\n\nfrom graphql import print_schema\n\nfrom apischema.graphql import graphql_schema\n\n\n@dataclass\nclass Foo:\n    foo: int\n\n\n@dataclass\nclass Bar:\n    bar: int\n\n\ndef foo_or_bar() -&gt; Foo | Bar:\n    ...\n\n\n# union_ref default value is made explicit here\nschema = graphql_schema(query=[foo_or_bar], union_name=\"Or\".join)\nschema_str = \"\"\"\\\ntype Query {\n  fooOrBar: FooOrBar!\n}\n\nunion FooOrBar = Foo | Bar\n\ntype Foo {\n  foo: Int!\n}\n\ntype Bar {\n  bar: Int!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/schema/#enum-metadata","title":"<code>Enum</code> metadata","text":"<p>Contrary to dataclasses, <code>Enum</code> doesn't provide a way to set metadata for its members, especially description, but also deprecation reason. They can however be passed using <code>enum_schemas</code> parameter of <code>graphql_schema</code>. </p> <pre><code>from enum import Enum\n\nfrom graphql import graphql_sync\nfrom graphql.utilities import print_schema\n\nfrom apischema import schema\nfrom apischema.graphql import graphql_schema\n\n\nclass MyEnum(Enum):\n    FOO = \"FOO\"\n    BAR = \"BAR\"\n\n\ndef echo(enum: MyEnum) -&gt; MyEnum:\n    return enum\n\n\nschema_ = graphql_schema(\n    query=[echo], enum_schemas={MyEnum.FOO: schema(description=\"foo\")}\n)\nschema_str = '''\\\ntype Query {\n  echo(enum: MyEnum!): MyEnum!\n}\n\nenum MyEnum {\n  \"\"\"foo\"\"\"\n  FOO\n  BAR\n}'''\nassert print_schema(schema_) == schema_str\nassert graphql_sync(schema_, \"{echo(enum: FOO)}\").data == {\"echo\": \"FOO\"}\n</code></pre>"},{"location":"graphql/schema/#additional-types","title":"Additional types","text":"<p>apischema will only include in the schema the types annotating resolvers. However, it is possible to add other types by using the <code>types</code> parameter of <code>graphql_schema</code>. This is especially useful to add interface implementations where only interface is used in resolver types. </p> <pre><code>from dataclasses import dataclass\n\nfrom graphql import print_schema\n\nfrom apischema.graphql import graphql_schema, interface\n\n\n@interface\n@dataclass\nclass Bar:\n    bar: int\n\n\n@dataclass\nclass Foo(Bar):\n    baz: str\n\n\ndef bar() -&gt; Bar:\n    ...\n\n\nschema = graphql_schema(query=[bar], types=[Foo])\n# type Foo would have not been present if Foo was not put in types\nschema_str = \"\"\"\\\ntype Foo implements Bar {\n  bar: Int!\n  baz: String!\n}\n\ninterface Bar {\n  bar: Int!\n}\n\ntype Query {\n  bar: Bar!\n}\"\"\"\nassert print_schema(schema) == schema_str\n</code></pre>"},{"location":"graphql/schema/#subscriptions","title":"Subscriptions","text":"<p>Subscriptions are particular operations which must return an <code>AsyncIterable</code>; this event generator can come with a dedicated resolver to post-process the event.</p>"},{"location":"graphql/schema/#event-generator-only","title":"Event generator only","text":"<pre><code>import asyncio\nfrom typing import AsyncIterable\n\nimport graphql\nfrom graphql import print_schema\n\nfrom apischema.graphql import graphql_schema\n\n\ndef hello() -&gt; str:\n    return \"world\"\n\n\nasync def events() -&gt; AsyncIterable[str]:\n    yield \"bonjour\"\n    yield \"au revoir\"\n\n\nschema = graphql_schema(query=[hello], subscription=[events])\nschema_str = \"\"\"\\\ntype Query {\n  hello: String!\n}\n\ntype Subscription {\n  events: String!\n}\"\"\"\nassert print_schema(schema) == schema_str\n\n\nasync def test():\n    subscription = await graphql.subscribe(\n        schema, graphql.parse(\"subscription {events}\")\n    )\n    assert [event.data async for event in subscription] == [\n        {\"events\": \"bonjour\"},\n        {\"events\": \"au revoir\"},\n    ]\n\n\nasyncio.run(test())\n</code></pre> <p>Note</p> <p>Because there is no post-processing of generated event in a dedicated resolver, <code>error_handler</code> cannot be called, but it will still modify the type of the event. </p>"},{"location":"graphql/schema/#event-generator-resolver","title":"Event generator + resolver","text":"<p>A resolver can be added by using the <code>resolver</code> parameter of <code>Subscription</code>.  In this case, apischema will map subscription name, parameters and return type on the resolver instead of the event generator. It allows using the same event generator with several resolvers to create different subscriptions.</p> <p>The first resolver argument will be the event yielded by the event generator.</p> <pre><code>import asyncio\nfrom dataclasses import dataclass\nfrom typing import AsyncIterable\n\nimport graphql\nfrom graphql import print_schema\n\nfrom apischema.graphql import Subscription, graphql_schema\n\n\ndef hello() -&gt; str:\n    return \"world\"\n\n\nasync def events() -&gt; AsyncIterable[str]:\n    yield \"bonjour\"\n    yield \"au revoir\"\n\n\n@dataclass\nclass Message:\n    body: str\n\n\n# Message can also be used directly as a function\nschema = graphql_schema(\n    query=[hello],\n    subscription=[Subscription(events, alias=\"messageReceived\", resolver=Message)],\n)\nschema_str = \"\"\"\\\ntype Query {\n  hello: String!\n}\n\ntype Subscription {\n  messageReceived: Message!\n}\n\ntype Message {\n  body: String!\n}\"\"\"\nassert print_schema(schema) == schema_str\n\n\nasync def test():\n    subscription = await graphql.subscribe(\n        schema, graphql.parse(\"subscription {messageReceived {body}}\")\n    )\n    assert [event.data async for event in subscription] == [\n        {\"messageReceived\": {\"body\": \"bonjour\"}},\n        {\"messageReceived\": {\"body\": \"au revoir\"}},\n    ]\n\n\nasyncio.run(test())\n</code></pre>"}]}